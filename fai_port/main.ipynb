{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "\n",
    "def predict_batch(self, item, rm_type_tfms=None, with_input=False):\n",
    "    dl = self.dls.test_dl(item, rm_type_tfms=rm_type_tfms, num_workers=0)\n",
    "    inp,preds,_,dec_preds = self.get_preds(dl=dl, with_input=True, with_decoded=True)\n",
    "    i = getattr(self.dls, 'n_inp', -1)\n",
    "    inp = (inp,) if i==1 else tuplify(inp)\n",
    "    dec = self.dls.decode_batch(inp + tuplify(dec_preds))\n",
    "    dec_inp,dec_targ = map(detuplify, [dec[:i],dec[i:]])\n",
    "    res = dec_targ,dec_preds,preds\n",
    "    if with_input: res = (dec_inp,) + res\n",
    "    return res\n",
    "Learner.predict_batch = predict_batch\n",
    "from fastai.callback.tensorboard import TensorBoardCallback\n",
    "\n",
    "from sklearn import metrics, model_selection, preprocessing\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def asl_name_fn(x):\n",
    "    return Path(x).parent.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"experiment_name\": \"test_asl_starter\",\n",
    "    \"ds_path\": Path(\"/mnt/e/Datasets/asl/asl_alphabet_train/asl_alphabet_train\"),\n",
    "    \"ds_name\": \"asl\",\n",
    "    \"name_fn\": asl_name_fn,\n",
    "    \"image_size\": 224,\n",
    "    \"batch_size\": 64,\n",
    "    \"epoch_steps\": [1, 2],\n",
    "    \"enable_proxy_attention\": True,\n",
    "    # \"change_subset_attention\": tune.loguniform(0.1, 0.8),\n",
    "    \"change_subset_attention\": .5,\n",
    "    \"validation_split\": 0.3,\n",
    "    # \"shuffle_dataset\": tune.choice([True, False]),\n",
    "    \"shuffle_dataset\": True,\n",
    "    \"num_gpu\": 1,\n",
    "    \"transfer_imagenet\": False,\n",
    "    \"subset_images\": 8000,\n",
    "    # \"proxy_threshold\": tune.loguniform(0.008, 0.01),\n",
    "    \"proxy_threshold\":  0.005,\n",
    "    # \"pixel_replacement_method\": tune.choice([\"mean\", \"max\", \"min\", \"black\", \"white\"]),\n",
    "    \"pixel_replacement_method\": \"half\",\n",
    "    \"model\": \"resnet18\",\n",
    "    # \"proxy_steps\": tune.choice([[1, \"p\", 1], [3, \"p\", 1], [1, 1], [3,1]]),\n",
    "    # \"proxy_steps\": tune.choice([[\"p\", 1],[1, 1], [\"p\",1], [1, \"p\",1], [1,1,1]]),\n",
    "    \"proxy_steps\": [\"p\",1],\n",
    "    \"global_run_count\" : 0,\n",
    "    \"gradient_method\" : \"saliency\", #guidedgradcam\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"subset_images\"] != None:\n",
    "    def get_image_files(path, recurse=True, folders=None):\n",
    "        \"Get image files in `path` recursively, only in `folders`, if specified.\"\n",
    "        return get_files(path, extensions=image_extensions, recurse=recurse, folders=folders)[:config[\"subset_images\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dblock = DataBlock(blocks    = (ImageBlock, CategoryBlock),\n",
    "                   get_items = get_image_files,\n",
    "                   get_y     = asl_name_fn,\n",
    "                   splitter  = RandomSplitter(),\n",
    "                   item_tfms = Resize(config[\"image_size\"]),\n",
    "                   batch_tfms = [*aug_transforms(), Normalize.from_stats(*imagenet_stats)]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = dblock.dataloaders(config[\"ds_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3 classes\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {dls.c} classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dls.show_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_map = {\n",
    "    \"resnet18\" : resnet18\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs = [\n",
    "    TensorBoardCallback(projector=True)\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eragon/miniconda3/envs/pytorcher/envs/fai/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/eragon/miniconda3/envs/pytorcher/envs/fai/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "learn = vision_learner(dls, model_map[config[\"model\"]], metrics=accuracy, pretrained=False, cbs = cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.490132</td>\n",
       "      <td>0.059780</td>\n",
       "      <td>0.982500</td>\n",
       "      <td>01:21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.fit_one_cycle(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_im = get_image_files(config[\"ds_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fastai.data.core.TfmdDL at 0x7ff74e6bc9d0>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = learn.get_preds(dl = dls.valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {i: l for i, l in enumerate(dls.vocab)}\n",
    "rev_label_map = {l: i for i, l in enumerate(dls.vocab)}\n",
    "\n",
    "config[\"label_map\"]= label_map\n",
    "config[\"rev_label_map\"]= rev_label_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_labels = [config[\"rev_label_map\"][asl_name_fn(x)] for x in dls.valid_ds.items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 2,  ..., 0, 2, 1])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_indices = (torch.Tensor(original_labels) != preds[1]).nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], size=(0, 1), dtype=torch.int64)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# preds = learn.predict_batch(train[\"image_id\"][:10].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorBase([20,  5, 13, 27,  4,  0,  1, 10, 18, 26])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.callback.captum import CaptumInterpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "captum=CaptumInterpretation(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#6400) [(PILImage mode=RGB size=200x200, TensorCategory(1)),(PILImage mode=RGB size=200x200, TensorCategory(1)),(PILImage mode=RGB size=200x200, TensorCategory(0)),(PILImage mode=RGB size=200x200, TensorCategory(2)),(PILImage mode=RGB size=200x200, TensorCategory(0)),(PILImage mode=RGB size=200x200, TensorCategory(2)),(PILImage mode=RGB size=200x200, TensorCategory(0)),(PILImage mode=RGB size=200x200, TensorCategory(2)),(PILImage mode=RGB size=200x200, TensorCategory(0)),(PILImage mode=RGB size=200x200, TensorCategory(0))...]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls.train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_im = get_image_files(config[\"ds_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grid_sampler(): expected grid to have size 1 in last dimension, but got grid with sizes [3, 224, 224, 2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m captum\u001b[39m.\u001b[39;49mvisualize(test_im[\u001b[39m0\u001b[39;49m])\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorcher/envs/fai/lib/python3.10/site-packages/fastai/callback/captum.py:55\u001b[0m, in \u001b[0;36mCaptumInterpretation.visualize\u001b[0;34m(self, inp, metric, n_steps, baseline_type, nt_type, strides, sliding_window_shapes)\u001b[0m\n\u001b[1;32m     53\u001b[0m tls \u001b[39m=\u001b[39m L([TfmdLists(inp, t) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m L(ifnone(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdls\u001b[39m.\u001b[39mtfms,[\u001b[39mNone\u001b[39;00m]))])\n\u001b[1;32m     54\u001b[0m inp_data\u001b[39m=\u001b[39m\u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m(tls[\u001b[39m0\u001b[39m],tls[\u001b[39m1\u001b[39m])))[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> 55\u001b[0m enc_data,dec_data\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_enc_dec_data(inp_data)\n\u001b[1;32m     56\u001b[0m attributions\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_attributions(enc_data,metric,n_steps,nt_type,baseline_type,strides,sliding_window_shapes)\n\u001b[1;32m     57\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_viz(attributions,dec_data,metric)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorcher/envs/fai/lib/python3.10/site-packages/fastai/callback/captum.py:73\u001b[0m, in \u001b[0;36mCaptumInterpretation._get_enc_dec_data\u001b[0;34m(self, inp_data)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_enc_dec_data\u001b[39m(\u001b[39mself\u001b[39m,inp_data):\n\u001b[1;32m     72\u001b[0m     dec_data\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdls\u001b[39m.\u001b[39mafter_item(inp_data)\n\u001b[0;32m---> 73\u001b[0m     enc_data\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdls\u001b[39m.\u001b[39;49mafter_batch(to_device(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdls\u001b[39m.\u001b[39;49mbefore_batch(dec_data),\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdls\u001b[39m.\u001b[39;49mdevice))\n\u001b[1;32m     74\u001b[0m     \u001b[39mreturn\u001b[39;00m(enc_data,dec_data)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorcher/envs/fai/lib/python3.10/site-packages/fastcore/transform.py:208\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, o)\u001b[0m\n\u001b[0;32m--> 208\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, o): \u001b[39mreturn\u001b[39;00m compose_tfms(o, tfms\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfs, split_idx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msplit_idx)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorcher/envs/fai/lib/python3.10/site-packages/fastcore/transform.py:158\u001b[0m, in \u001b[0;36mcompose_tfms\u001b[0;34m(x, tfms, is_enc, reverse, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m tfms:\n\u001b[1;32m    157\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_enc: f \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mdecode\n\u001b[0;32m--> 158\u001b[0m     x \u001b[39m=\u001b[39m f(x, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    159\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorcher/envs/fai/lib/python3.10/site-packages/fastai/vision/augment.py:49\u001b[0m, in \u001b[0;36mRandTransform.__call__\u001b[0;34m(self, b, split_idx, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \n\u001b[1;32m     44\u001b[0m     b, \n\u001b[1;32m     45\u001b[0m     split_idx:\u001b[39mint\u001b[39m\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m# Index of the train/valid dataset\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m     47\u001b[0m ):\n\u001b[1;32m     48\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbefore_call(b, split_idx\u001b[39m=\u001b[39msplit_idx)\n\u001b[0;32m---> 49\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(b, split_idx\u001b[39m=\u001b[39;49msplit_idx, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo \u001b[39melse\u001b[39;00m b\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorcher/envs/fai/lib/python3.10/site-packages/fastcore/transform.py:81\u001b[0m, in \u001b[0;36mTransform.__call__\u001b[0;34m(self, x, **kwargs)\u001b[0m\n\u001b[0;32m---> 81\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs): \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m'\u001b[39;49m\u001b[39mencodes\u001b[39;49m\u001b[39m'\u001b[39;49m, x, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorcher/envs/fai/lib/python3.10/site-packages/fastcore/transform.py:91\u001b[0m, in \u001b[0;36mTransform._call\u001b[0;34m(self, fn, x, split_idx, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\u001b[39mself\u001b[39m, fn, x, split_idx\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     90\u001b[0m     \u001b[39mif\u001b[39;00m split_idx\u001b[39m!=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit_idx \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit_idx \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m: \u001b[39mreturn\u001b[39;00m x\n\u001b[0;32m---> 91\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_call(\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, fn), x, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorcher/envs/fai/lib/python3.10/site-packages/fastcore/transform.py:98\u001b[0m, in \u001b[0;36mTransform._do_call\u001b[0;34m(self, f, x, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m     ret \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mreturns(x) \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(f,\u001b[39m'\u001b[39m\u001b[39mreturns\u001b[39m\u001b[39m'\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     \u001b[39mreturn\u001b[39;00m retain_type(f(x, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs), x, ret)\n\u001b[0;32m---> 98\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_call(f, x_, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs) \u001b[39mfor\u001b[39;49;00m x_ \u001b[39min\u001b[39;49;00m x)\n\u001b[1;32m     99\u001b[0m \u001b[39mreturn\u001b[39;00m retain_type(res, x)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorcher/envs/fai/lib/python3.10/site-packages/fastcore/transform.py:98\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     96\u001b[0m     ret \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mreturns(x) \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(f,\u001b[39m'\u001b[39m\u001b[39mreturns\u001b[39m\u001b[39m'\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     97\u001b[0m     \u001b[39mreturn\u001b[39;00m retain_type(f(x, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs), x, ret)\n\u001b[0;32m---> 98\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_call(f, x_, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs) \u001b[39mfor\u001b[39;00m x_ \u001b[39min\u001b[39;00m x)\n\u001b[1;32m     99\u001b[0m \u001b[39mreturn\u001b[39;00m retain_type(res, x)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorcher/envs/fai/lib/python3.10/site-packages/fastcore/transform.py:97\u001b[0m, in \u001b[0;36mTransform._do_call\u001b[0;34m(self, f, x, **kwargs)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[39mif\u001b[39;00m f \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m: \u001b[39mreturn\u001b[39;00m x\n\u001b[1;32m     96\u001b[0m     ret \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mreturns(x) \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(f,\u001b[39m'\u001b[39m\u001b[39mreturns\u001b[39m\u001b[39m'\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m     \u001b[39mreturn\u001b[39;00m retain_type(f(x, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs), x, ret)\n\u001b[1;32m     98\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_call(f, x_, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39mfor\u001b[39;00m x_ \u001b[39min\u001b[39;00m x)\n\u001b[1;32m     99\u001b[0m \u001b[39mreturn\u001b[39;00m retain_type(res, x)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorcher/envs/fai/lib/python3.10/site-packages/fastcore/dispatch.py:120\u001b[0m, in \u001b[0;36mTypeDispatch.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minst \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m: f \u001b[39m=\u001b[39m MethodType(f, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minst)\n\u001b[1;32m    119\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mowner \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m: f \u001b[39m=\u001b[39m MethodType(f, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mowner)\n\u001b[0;32m--> 120\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorcher/envs/fai/lib/python3.10/site-packages/fastai/vision/augment.py:501\u001b[0m, in \u001b[0;36mAffineCoordTfm.encodes\u001b[0;34m(self, x)\u001b[0m\n\u001b[0;32m--> 501\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencodes\u001b[39m(\u001b[39mself\u001b[39m, x:TensorImage): \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encode(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorcher/envs/fai/lib/python3.10/site-packages/fastai/vision/augment.py:499\u001b[0m, in \u001b[0;36mAffineCoordTfm._encode\u001b[0;34m(self, x, mode, reverse)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_encode\u001b[39m(\u001b[39mself\u001b[39m, x, mode, reverse\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    498\u001b[0m     coord_func \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoord_fs)\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit_idx \u001b[39melse\u001b[39;00m partial(compose_tfms, tfms\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoord_fs, reverse\u001b[39m=\u001b[39mreverse)\n\u001b[0;32m--> 499\u001b[0m     \u001b[39mreturn\u001b[39;00m x\u001b[39m.\u001b[39;49maffine_coord(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmat, coord_func, sz\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize, mode\u001b[39m=\u001b[39;49mmode, pad_mode\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpad_mode, align_corners\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49malign_corners)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorcher/envs/fai/lib/python3.10/site-packages/fastai/vision/augment.py:391\u001b[0m, in \u001b[0;36maffine_coord\u001b[0;34m(x, mat, coord_tfm, sz, mode, pad_mode, align_corners)\u001b[0m\n\u001b[1;32m    389\u001b[0m coords \u001b[39m=\u001b[39m affine_grid(mat, x\u001b[39m.\u001b[39mshape[:\u001b[39m2\u001b[39m] \u001b[39m+\u001b[39m size, align_corners\u001b[39m=\u001b[39malign_corners)\n\u001b[1;32m    390\u001b[0m \u001b[39mif\u001b[39;00m coord_tfm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m: coords \u001b[39m=\u001b[39m coord_tfm(coords)\n\u001b[0;32m--> 391\u001b[0m \u001b[39mreturn\u001b[39;00m TensorImage(_grid_sample(x, coords, mode\u001b[39m=\u001b[39;49mmode, padding_mode\u001b[39m=\u001b[39;49mpad_mode, align_corners\u001b[39m=\u001b[39;49malign_corners))\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorcher/envs/fai/lib/python3.10/site-packages/fastai/vision/augment.py:364\u001b[0m, in \u001b[0;36m_grid_sample\u001b[0;34m(x, coords, mode, padding_mode, align_corners)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[39mif\u001b[39;00m d\u001b[39m>\u001b[39m\u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m d\u001b[39m>\u001b[39mz:\n\u001b[1;32m    363\u001b[0m         x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39minterpolate(x, scale_factor\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\u001b[39m/\u001b[39md, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39marea\u001b[39m\u001b[39m'\u001b[39m, recompute_scale_factor\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m--> 364\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mgrid_sample(x, coords, mode\u001b[39m=\u001b[39;49mmode, padding_mode\u001b[39m=\u001b[39;49mpadding_mode, align_corners\u001b[39m=\u001b[39;49malign_corners)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorcher/envs/fai/lib/python3.10/site-packages/torch/nn/functional.py:4197\u001b[0m, in \u001b[0;36mgrid_sample\u001b[0;34m(input, grid, mode, padding_mode, align_corners)\u001b[0m\n\u001b[1;32m   4097\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Given an :attr:`input` and a flow-field :attr:`grid`, computes the\u001b[39;00m\n\u001b[1;32m   4098\u001b[0m \u001b[39m``output`` using :attr:`input` values and pixel locations from :attr:`grid`.\u001b[39;00m\n\u001b[1;32m   4099\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4194\u001b[0m \u001b[39m.. _`OpenCV`: https://github.com/opencv/opencv/blob/f345ed564a06178670750bad59526cfa4033be55/modules/imgproc/src/resize.cpp#L908\u001b[39;00m\n\u001b[1;32m   4195\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4196\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_variadic(\u001b[39minput\u001b[39m, grid):\n\u001b[0;32m-> 4197\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   4198\u001b[0m         grid_sample, (\u001b[39minput\u001b[39;49m, grid), \u001b[39minput\u001b[39;49m, grid, mode\u001b[39m=\u001b[39;49mmode, padding_mode\u001b[39m=\u001b[39;49mpadding_mode, align_corners\u001b[39m=\u001b[39;49malign_corners\n\u001b[1;32m   4199\u001b[0m     )\n\u001b[1;32m   4200\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbilinear\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m mode \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnearest\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m mode \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbicubic\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   4201\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   4202\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mnn.functional.grid_sample(): expected mode to be \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4203\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbilinear\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mnearest\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mbicubic\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, but got: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(mode)\n\u001b[1;32m   4204\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorcher/envs/fai/lib/python3.10/site-packages/torch/overrides.py:1534\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1528\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mDefining your `__torch_function__ as a plain method is deprecated and \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1529\u001b[0m                   \u001b[39m\"\u001b[39m\u001b[39mwill be an error in future, please define it as a classmethod.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1530\u001b[0m                   \u001b[39mDeprecationWarning\u001b[39;00m)\n\u001b[1;32m   1532\u001b[0m \u001b[39m# Use `public_api` instead of `implementation` so __torch_function__\u001b[39;00m\n\u001b[1;32m   1533\u001b[0m \u001b[39m# implementations can do equality/identity comparisons.\u001b[39;00m\n\u001b[0;32m-> 1534\u001b[0m result \u001b[39m=\u001b[39m torch_func_method(public_api, types, args, kwargs)\n\u001b[1;32m   1536\u001b[0m \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[1;32m   1537\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorcher/envs/fai/lib/python3.10/site-packages/fastai/torch_core.py:372\u001b[0m, in \u001b[0;36mTensorBase.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mdebug \u001b[39mand\u001b[39;00m func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39m__str__\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m__repr__\u001b[39m\u001b[39m'\u001b[39m): \u001b[39mprint\u001b[39m(func, types, args, kwargs)\n\u001b[1;32m    371\u001b[0m \u001b[39mif\u001b[39;00m _torch_handled(args, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_opt, func): types \u001b[39m=\u001b[39m (torch\u001b[39m.\u001b[39mTensor,)\n\u001b[0;32m--> 372\u001b[0m res \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m__torch_function__(func, types, args, ifnone(kwargs, {}))\n\u001b[1;32m    373\u001b[0m dict_objs \u001b[39m=\u001b[39m _find_args(args) \u001b[39mif\u001b[39;00m args \u001b[39melse\u001b[39;00m _find_args(\u001b[39mlist\u001b[39m(kwargs\u001b[39m.\u001b[39mvalues()))\n\u001b[1;32m    374\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mtype\u001b[39m(res),TensorBase) \u001b[39mand\u001b[39;00m dict_objs: res\u001b[39m.\u001b[39mset_meta(dict_objs[\u001b[39m0\u001b[39m],as_copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorcher/envs/fai/lib/python3.10/site-packages/torch/_tensor.py:1279\u001b[0m, in \u001b[0;36mTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1276\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[1;32m   1278\u001b[0m \u001b[39mwith\u001b[39;00m _C\u001b[39m.\u001b[39mDisableTorchFunction():\n\u001b[0;32m-> 1279\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1280\u001b[0m     \u001b[39mif\u001b[39;00m func \u001b[39min\u001b[39;00m get_default_nowrap_functions():\n\u001b[1;32m   1281\u001b[0m         \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorcher/envs/fai/lib/python3.10/site-packages/torch/nn/functional.py:4235\u001b[0m, in \u001b[0;36mgrid_sample\u001b[0;34m(input, grid, mode, padding_mode, align_corners)\u001b[0m\n\u001b[1;32m   4227\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   4228\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mDefault grid_sample and affine_grid behavior has changed \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4229\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mto align_corners=False since 1.3.0. Please specify \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4230\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39malign_corners=True if the old behavior is desired. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4231\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSee the documentation of grid_sample for details.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4232\u001b[0m     )\n\u001b[1;32m   4233\u001b[0m     align_corners \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m-> 4235\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mgrid_sampler(\u001b[39minput\u001b[39;49m, grid, mode_enum, padding_mode_enum, align_corners)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grid_sampler(): expected grid to have size 1 in last dimension, but got grid with sizes [3, 224, 224, 2]"
     ]
    }
   ],
   "source": [
    "captum.visualize(test_im[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "28cc2143a3fb9bc063e0a77172084ec43a27587aa79180ef2164340558f0cb22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
