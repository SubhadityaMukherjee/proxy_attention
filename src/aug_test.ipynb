{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Imports\n",
    "from base64 import decode\n",
    "from config import ds_config\n",
    "from fastai.vision.all import *\n",
    "from fastai.callback.tensorboard import TensorBoardCallback\n",
    "import torchvision.transforms.functional as transformF\n",
    "\n",
    "# from fastai.callback.tracker import\n",
    "from fastai.vision.widgets import *\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "import argparse as ap\n",
    "import datetime\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "# from torch.multiprocessing import set_start_method\n",
    "# set_start_method('forkserver')\n",
    "\n",
    "# from torch.multiprocessing import Pool, Process, set_start_method\n",
    "# try:\n",
    "#      set_start_method('spawn')\n",
    "# except RuntimeError:\n",
    "#     pass\n",
    "\n",
    "from utils import *  # import utils at the end after fastai because the Hook function is a monkey-patch\n",
    "\n",
    "os.environ[\"TORCH_HOME\"] = \"/media/hdd/Datasets/\"\n",
    "os.environ[\"FASTAI_HOME\"] = \"/media/hdd/Datasets/\"\n",
    "\n",
    "# set_start_method('spawn')\n",
    "# Monkey patch batch prediction (fastai does not have this by default)\n",
    "Learner.predict_batch = predict_batch\n",
    "# %%\n",
    "# ags = ap.ArgumentParser(\"Additional Arguments for CLI\")\n",
    "# ags.add_argument(\n",
    "#     \"--config\", help=\"Name of config from dictionary\", default=\"fish_test_proxy\"\n",
    "# )\n",
    "# ags.add_argument(\"--name\", help=\"Name of the experiment\", required=True)\n",
    "# args = ags.parse_args()\n",
    "# ds_meta = ds_config[args.config]  # get info about dataset from the config file\n",
    "\n",
    "args = {\n",
    "    \"config\": \"fish_test_proxy\",\n",
    "    \"name\" : \"testing_ugh\"\n",
    "}\n",
    "from types import SimpleNamespace\n",
    "args = SimpleNamespace(**args)\n",
    "ds_meta = ds_config[args.config]  # get info about dataset from the config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] : File name = fish_testing_ugh_17102022_18:13:52\n",
      "[INFO] : Removed Old augmented files\n"
     ]
    }
   ],
   "source": [
    "path = Path(ds_meta[\"ds_path\"])\n",
    "fname_start = f'{ds_meta[\"ds_name\"]}_{args.name}_{datetime.now().strftime(\"%d%m%Y_%H:%M:%S\")}'  # unique_name\n",
    "print(f\"[INFO] : File name = {fname_start}\")\n",
    "\n",
    "# Check if directories all present\n",
    "create_if_not_exists(f\"tb_runs/{fname_start}\")\n",
    "create_if_not_exists(f\"csv_logs/{fname_start}\")\n",
    "# Remove previous files\n",
    "\n",
    "all_files = get_image_files(path)\n",
    "[Path.unlink(file) for file in all_files if \"augmented_\" in file.name]\n",
    "print(\"[INFO] : Removed Old augmented files\")\n",
    "\n",
    "# TODO : Add reset folder\n",
    "#%%\n",
    "batch_tfms = aug_transforms() if ds_meta[\"enable_default_augments\"] == True else None\n",
    "fields = DataBlock(\n",
    "    blocks=(ImageBlock, CategoryBlock),\n",
    "    get_items=get_image_files,\n",
    "    get_y=ds_meta[\"name_fn\"],\n",
    "    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n",
    "    item_tfms=RandomResizedCrop(ds_meta[\"image_size\"], min_scale=0.5),\n",
    "    batch_tfms=batch_tfms,\n",
    ")\n",
    "# Metrics\n",
    "metrics = [accuracy, error_rate]\n",
    "# Callbacks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Round 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eragon/micromamba/envs/pytorcher/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/eragon/micromamba/envs/pytorcher/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] : Cleared learner\n"
     ]
    }
   ],
   "source": [
    "dls = fields.dataloaders(path, bs=ds_meta[\"batch_size\"])\n",
    "cbs = [\n",
    "TensorBoardCallback(\n",
    "    log_dir=f\"tb_runs/{fname_start}\", projector=False, trace_model=False\n",
    "),\n",
    "CSVLogger(fname=f\"csv_logs/{fname_start}.csv\"),\n",
    "]\n",
    "\n",
    "learn = vision_learner(\n",
    "dls, ds_meta[\"network\"], cbs=cbs, metrics=metrics, pretrained=ds_meta[\"pretrained\"]\n",
    ").to_fp16()\n",
    "fname_training = f'{ds_meta[\"ds_name\"]}_{args.name}_{datetime.now().strftime(\"%d%m%Y_%H:%M:%S\")}'  # unique_name\n",
    "# learn.fine_tune(1)\n",
    "# learn.save(\"temp_model\")  # saving so can be reloaded\n",
    "learn.load(\"temp_model\")\n",
    "# clear_learner(learn, dls)\n",
    "print(\"[LOG] : Cleared learner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] : Starting Attention Loop\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Hook():\n",
    "    def __init__(self, m):\n",
    "        self.hook = m.register_forward_hook(self.hook_func)   \n",
    "    def hook_func(self, m, i, o): self.stored = o.detach().clone()\n",
    "    # Automatically register the hook when entering it\n",
    "    def __enter__(self, *args): return self\n",
    "    # Automatically remove the hook when exiting it\n",
    "    def __exit__(self, *args): self.hook.remove()\n",
    "\n",
    "class HookBwd():\n",
    "    def __init__(self, m):\n",
    "        self.hook = m.register_backward_hook(self.hook_func)   \n",
    "        # self.hook = m.register_full_backward_hook(self.hook_func)   \n",
    "    def hook_func(self, m, gi, go): self.stored = go[0].detach().clone()\n",
    "    def __enter__(self, *args): return self\n",
    "    def __exit__(self, *args): self.hook.remove()\n",
    "\n",
    "# dls.to('cpu')\n",
    "\n",
    "# Get the classes\n",
    "print(\"[INFO] : Starting Attention Loop\")\n",
    "vocab_dict = {\n",
    "learn.dls.vocab[x]: x for x in range(len(learn.dls.vocab))\n",
    "}  # Get class names\n",
    "# Get images, shuffle, pick a subset\n",
    "items = get_image_files_exclude_augment(ds_meta[\"ds_path\"])\n",
    "items = items.shuffle()\n",
    "subset = int(ds_meta[\"change_subset_attention\"] * len(items))\n",
    "items = items[:subset]\n",
    "# Get preds from the network for all the chosen images with \"num_workers\" threads\n",
    "bspred = learn.predict_batch(items, num_workers=10)\n",
    "# Get all the class names for the subset of images and convert them into the One hot encoded version that the network knows already\n",
    "item_names = list(\n",
    "map(lambda x: vocab_dict[x], list(map(ds_meta[\"name_fn\"], items)))\n",
    ")\n",
    "\n",
    "# Get the index of all the images that the network predicted wrong\n",
    "# TODO : Check for confidence\n",
    "index_wrongs = [\n",
    "x for x in range(subset) if bspred[2][x] != TensorBase(item_names)[x]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81/81 [00:00<00:00, 498881.97it/s]\n"
     ]
    }
   ],
   "source": [
    "ims = [items[x] for x in tqdm(index_wrongs, total = len(index_wrongs))]\n",
    "test_ds_new_ims = dls.test_dl(ims, bs = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = learn.get_preds(dl = test_ds_new_ims, with_decoded=True)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(lst, batch_size):\n",
    "    \"\"\"  Yields batch of specified size \"\"\"\n",
    "    return [lst[i : i + batch_size] for i in range(0, len(lst), batch_size)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/eragon/micromamba/envs/pytorcher/lib/python3.10/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(-4.8414, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# to_check = learn.model[-2][4][-1]\n",
    "# learn.eval()\n",
    "\n",
    "item_names_batched = generate_batch(item_names, test_ds_new_ims.bs)\n",
    "\n",
    "cam_maps = []\n",
    "for i, data in tqdm(enumerate(test_ds_new_ims)):\n",
    "\n",
    "    data = dls.train.decode(data)\n",
    "    y_values = item_names_batched[i]\n",
    "\n",
    "    with HookBwd(learn.model[-2][4][-1]) as hookg:  # for other layers\n",
    "        with Hook(learn.model[-2][4][-1]) as hook:\n",
    "            preds = learn.model.eval()(data[0].float().to(\"cuda\"))\n",
    "            # print(len(preds))\n",
    "            # acts = hook.stored\n",
    "            act = hook.stored\n",
    "        \n",
    "        # THIS is the issue I think.\n",
    "        for i_2 in range(len(preds)):\n",
    "            preds[i_2,y_values[i_2]-1].backward(retain_graph = True)\n",
    "        grad = hookg.stored\n",
    "        for i in grad:\n",
    "          print(i.sum())\n",
    "    for i_3 in range(len(preds)):\n",
    "        w = grad[i_3].mean(dim=[1, 2], keepdim=True)\n",
    "        cam_map = (w * act[i_3]).sum(0)\n",
    "        # print(w.sum())\n",
    "        cam_maps.append(cam_map)\n",
    "    break\n",
    "\n",
    "    #     preds[0, 3].backward()\n",
    "    #     grad = hookg.stored\n",
    "    # w = grad[0].mean(dim=[1, 2], keepdim=True)\n",
    "    # cam_map = (w * act[0]).sum(0)\n",
    "    # cam_maps.append(cam_map)\n",
    "\n",
    "    #     for i_2, output_x in tqdm(enumerate(preds), total = len(preds)):\n",
    "    #         preds[i_2, y_values[i_2]].backward(retain_graph = True)\n",
    "    #     grads = hookg.stored\n",
    "    #     # print(grads[0].shape, acts[0].shape)\n",
    "    # for i_3, output_x in tqdm(enumerate(grads), total = len(grads)):\n",
    "    #     w = grads[i_3].mean(dim=[1, 2], keepdim=True)\n",
    "    #     # w = grads[i_3]\n",
    "    #     cam_map = (w * acts[i_3]).sum(0)\n",
    "    #     cam_maps.append(cam_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 64, 56, 56])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGEAAABhCAYAAADGBs+jAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAABFElEQVR4nO3TsQ3CUBAFQWzRmkugSkpwb5xbcIL+CmbiC560um1m5sFS++oBiJAgQoAIASIEiBAgQoAIASIEPO8eHvvrmzt+0vl537rzCQEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIELANjOzesS/8wkBIgSIECBCgAgBIgSIECBCgAgBF56cC7tHAcfVAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGEAAABhCAYAAADGBs+jAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAABFElEQVR4nO3TsQ3CUBAFQWzRmkugSkpwb5xbcIL+CmbiC560um1m5sFS++oBiJAgQoAIASIEiBAgQoAIASIEPO8eHvvrmzt+0vl537rzCQEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIELANjOzesS/8wkBIgSIECBCgAgBIgSIECBCgAgBF56cC7tHAcfVAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGEAAABhCAYAAADGBs+jAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAABFElEQVR4nO3TsQ3CUBAFQWzRmkugSkpwb5xbcIL+CmbiC560um1m5sFS++oBiJAgQoAIASIEiBAgQoAIASIEPO8eHvvrmzt+0vl537rzCQEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIELANjOzesS/8wkBIgSIECBCgAgBIgSIECBCgAgBF56cC7tHAcfVAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGEAAABhCAYAAADGBs+jAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAABFElEQVR4nO3TsQ3CUBAFQWzRmkugSkpwb5xbcIL+CmbiC560um1m5sFS++oBiJAgQoAIASIEiBAgQoAIASIEPO8eHvvrmzt+0vl537rzCQEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIELANjOzesS/8wkBIgSIECBCgAgBIgSIECBCgAgBF56cC7tHAcfVAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGEAAABhCAYAAADGBs+jAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAABFElEQVR4nO3TsQ3CUBAFQWzRmkugSkpwb5xbcIL+CmbiC560um1m5sFS++oBiJAgQoAIASIEiBAgQoAIASIEPO8eHvvrmzt+0vl537rzCQEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIELANjOzesS/8wkBIgSIECBCgAgBIgSIECBCgAgBF56cC7tHAcfVAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGEAAABhCAYAAADGBs+jAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAABFElEQVR4nO3TsQ3CUBAFQWzRmkugSkpwb5xbcIL+CmbiC560um1m5sFS++oBiJAgQoAIASIEiBAgQoAIASIEPO8eHvvrmzt+0vl537rzCQEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIELANjOzesS/8wkBIgSIECBCgAgBIgSIECBCgAgBF56cC7tHAcfVAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGEAAABhCAYAAADGBs+jAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAABFElEQVR4nO3TsQ3CUBAFQWzRmkugSkpwb5xbcIL+CmbiC560um1m5sFS++oBiJAgQoAIASIEiBAgQoAIASIEPO8eHvvrmzt+0vl537rzCQEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIELANjOzesS/8wkBIgSIECBCgAgBIgSIECBCgAgBF56cC7tHAcfVAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGEAAABhCAYAAADGBs+jAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAABFElEQVR4nO3TsQ3CUBAFQWzRmkugSkpwb5xbcIL+CmbiC560um1m5sFS++oBiJAgQoAIASIEiBAgQoAIASIEPO8eHvvrmzt+0vl537rzCQEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIELANjOzesS/8wkBIgSIECBCgAgBIgSIECBCgAgBF56cC7tHAcfVAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGEAAABhCAYAAADGBs+jAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAABFElEQVR4nO3TsQ3CUBAFQWzRmkugSkpwb5xbcIL+CmbiC560um1m5sFS++oBiJAgQoAIASIEiBAgQoAIASIEPO8eHvvrmzt+0vl537rzCQEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIELANjOzesS/8wkBIgSIECBCgAgBIgSIECBCgAgBF56cC7tHAcfVAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGEAAABhCAYAAADGBs+jAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAABFElEQVR4nO3TsQ3CUBAFQWzRmkugSkpwb5xbcIL+CmbiC560um1m5sFS++oBiJAgQoAIASIEiBAgQoAIASIEPO8eHvvrmzt+0vl537rzCQEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIELANjOzesS/8wkBIgSIECBCgAgBIgSIECBCgAgBF56cC7tHAcfVAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGEAAABhCAYAAADGBs+jAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAABFElEQVR4nO3TsQ3CUBAFQWzRmkugSkpwb5xbcIL+CmbiC560um1m5sFS++oBiJAgQoAIASIEiBAgQoAIASIEPO8eHvvrmzt+0vl537rzCQEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIELANjOzesS/8wkBIgSIECBCgAgBIgSIECBCgAgBF56cC7tHAcfVAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGEAAABhCAYAAADGBs+jAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAABFElEQVR4nO3TsQ3CUBAFQWzRmkugSkpwb5xbcIL+CmbiC560um1m5sFS++oBiJAgQoAIASIEiBAgQoAIASIEPO8eHvvrmzt+0vl537rzCQEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIELANjOzesS/8wkBIgSIECBCgAgBIgSIECBCgAgBF56cC7tHAcfVAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGEAAABhCAYAAADGBs+jAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAABFElEQVR4nO3TsQ3CUBAFQWzRmkugSkpwb5xbcIL+CmbiC560um1m5sFS++oBiJAgQoAIASIEiBAgQoAIASIEPO8eHvvrmzt+0vl537rzCQEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIELANjOzesS/8wkBIgSIECBCgAgBIgSIECBCgAgBF56cC7tHAcfVAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGEAAABhCAYAAADGBs+jAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAABFElEQVR4nO3TsQ3CUBAFQWzRmkugSkpwb5xbcIL+CmbiC560um1m5sFS++oBiJAgQoAIASIEiBAgQoAIASIEPO8eHvvrmzt+0vl537rzCQEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIELANjOzesS/8wkBIgSIECBCgAgBIgSIECBCgAgBF56cC7tHAcfVAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGEAAABhCAYAAADGBs+jAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAABFElEQVR4nO3TsQ3CUBAFQWzRmkugSkpwb5xbcIL+CmbiC560um1m5sFS++oBiJAgQoAIASIEiBAgQoAIASIEPO8eHvvrmzt+0vl537rzCQEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIELANjOzesS/8wkBIgSIECBCgAgBIgSIECBCgAgBF56cC7tHAcfVAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "for i in range(len(cam_maps)):\n",
    "    show_image(cam_maps[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing\n"
     ]
    }
   ],
   "source": [
    "print(\"Resizing\")\n",
    "t_resized = [transformF.resize(torch.unsqueeze(cam_map, 0), ds_meta[\"image_size\"]) for cam_map in cam_maps]\n",
    "t_resized = [torch.cat([x, x, x], dim=0).detach().cpu() for x in t_resized]\n",
    "t_resized_batch = generate_batch(t_resized, test_ds_new_ims.bs)\n",
    "\n",
    "decoded_tensor_images = dls.train.decode(data)\n",
    "# decoded_tensor_images = [dls.train.decode(x)[0][0].float() for x in tqdm(test_ds_new_ims, total = len(test_ds_new_ims))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 224, 224])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_tensor_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 1\n"
     ]
    }
   ],
   "source": [
    "print(len(t_resized_batch), len(decoded_tensor_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for dimension 0 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/media/hdd/github/improving_robotics_datasets/src/aug_test.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/hdd/github/improving_robotics_datasets/src/aug_test.ipynb#ch0000034?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(t_resized_batch)):\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/hdd/github/improving_robotics_datasets/src/aug_test.ipynb#ch0000034?line=1'>2</a>\u001b[0m     \u001b[39mfor\u001b[39;00m index \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(t_resized_batch[batch])):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/hdd/github/improving_robotics_datasets/src/aug_test.ipynb#ch0000034?line=2'>3</a>\u001b[0m         \u001b[39mprint\u001b[39m(decoded_tensor_images[batch][index]\u001b[39m.\u001b[39mshape, t_resized_batch[batch][index]\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/hdd/github/improving_robotics_datasets/src/aug_test.ipynb#ch0000034?line=3'>4</a>\u001b[0m         decoded_tensor_images[batch][index][t_resized_batch[batch][index] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m\u001b[39m0.009\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n",
      "File \u001b[0;32m~/micromamba/envs/pytorcher/lib/python3.10/site-packages/fastai/torch_core.py:376\u001b[0m, in \u001b[0;36mTensorBase.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/eragon/micromamba/envs/pytorcher/lib/python3.10/site-packages/fastai/torch_core.py?line=373'>374</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mdebug \u001b[39mand\u001b[39;00m func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39m__str__\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m__repr__\u001b[39m\u001b[39m'\u001b[39m): \u001b[39mprint\u001b[39m(func, types, args, kwargs)\n\u001b[1;32m    <a href='file:///home/eragon/micromamba/envs/pytorcher/lib/python3.10/site-packages/fastai/torch_core.py?line=374'>375</a>\u001b[0m \u001b[39mif\u001b[39;00m _torch_handled(args, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_opt, func): types \u001b[39m=\u001b[39m (torch\u001b[39m.\u001b[39mTensor,)\n\u001b[0;32m--> <a href='file:///home/eragon/micromamba/envs/pytorcher/lib/python3.10/site-packages/fastai/torch_core.py?line=375'>376</a>\u001b[0m res \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m__torch_function__(func, types, args, ifnone(kwargs, {}))\n\u001b[1;32m    <a href='file:///home/eragon/micromamba/envs/pytorcher/lib/python3.10/site-packages/fastai/torch_core.py?line=376'>377</a>\u001b[0m dict_objs \u001b[39m=\u001b[39m _find_args(args) \u001b[39mif\u001b[39;00m args \u001b[39melse\u001b[39;00m _find_args(\u001b[39mlist\u001b[39m(kwargs\u001b[39m.\u001b[39mvalues()))\n\u001b[1;32m    <a href='file:///home/eragon/micromamba/envs/pytorcher/lib/python3.10/site-packages/fastai/torch_core.py?line=377'>378</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mtype\u001b[39m(res),TensorBase) \u001b[39mand\u001b[39;00m dict_objs: res\u001b[39m.\u001b[39mset_meta(dict_objs[\u001b[39m0\u001b[39m],as_copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/micromamba/envs/pytorcher/lib/python3.10/site-packages/torch/_tensor.py:1121\u001b[0m, in \u001b[0;36mTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/eragon/micromamba/envs/pytorcher/lib/python3.10/site-packages/torch/_tensor.py?line=1117'>1118</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[1;32m   <a href='file:///home/eragon/micromamba/envs/pytorcher/lib/python3.10/site-packages/torch/_tensor.py?line=1119'>1120</a>\u001b[0m \u001b[39mwith\u001b[39;00m _C\u001b[39m.\u001b[39mDisableTorchFunction():\n\u001b[0;32m-> <a href='file:///home/eragon/micromamba/envs/pytorcher/lib/python3.10/site-packages/torch/_tensor.py?line=1120'>1121</a>\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/eragon/micromamba/envs/pytorcher/lib/python3.10/site-packages/torch/_tensor.py?line=1121'>1122</a>\u001b[0m     \u001b[39mif\u001b[39;00m func \u001b[39min\u001b[39;00m get_default_nowrap_functions():\n\u001b[1;32m   <a href='file:///home/eragon/micromamba/envs/pytorcher/lib/python3.10/site-packages/torch/_tensor.py?line=1122'>1123</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mIndexError\u001b[0m: index 3 is out of bounds for dimension 0 with size 3"
     ]
    }
   ],
   "source": [
    "for batch in range(len(t_resized_batch)):\n",
    "    for index in range(len(t_resized_batch[batch])):\n",
    "        print(decoded_tensor_images[batch][index].shape, t_resized_batch[batch][index].shape)\n",
    "        decoded_tensor_images[batch][index][t_resized_batch[batch][index] >=0.009] = 0.0\n",
    "        # decoded_tensor_images[batch][index] = torch.einsum(\"ijk->jki\", decoded_tensor_images[batch][index])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69/69 [00:00<00:00, 519.24it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]/home/eragon/micromamba/envs/pytorcher/lib/python3.10/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([69, 18])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ims = [PILImage.create(items[x]) for x in tqdm(index_wrongs, total = len(index_wrongs))]\n",
    "im_names = [items[x] for x in index_wrongs]\n",
    "test_ds_new_ims = dls.test_dl(ims, shuffle = False)\n",
    "decoded = [dls.train.decode(x)[0].float() for x in tqdm(test_ds_new_ims, total = len(test_ds_new_ims))]\n",
    "eval_model =learn.model.eval() \n",
    "# eval_model_results = [eval_model(first(x).float().cuda()) for x in test_ds_new_ims]\n",
    "# eval_model_results = learn.predict_batch(ims)[0]\n",
    "# print(eval_model_results)\n",
    "# Hook\n",
    "cls = 1\n",
    "cam_maps = []\n",
    "\n",
    "print(\"Creating map\")\n",
    "for im in tqdm(decoded, total = len(decoded)):\n",
    "    with HookBwd(learn.model[-2][4][-1]) as hookg:  # for other layers\n",
    "        with Hook(learn.model[-2][4][-1]) as hook:\n",
    "            output = eval_model(im.cuda())\n",
    "            print(output.shape)\n",
    "            # output = learn.predict(im)\n",
    "            act = hook.stored\n",
    "        output[0, cls].backward()\n",
    "        grad = hookg.stored\n",
    "    w = grad[0].mean(dim=[1, 2], keepdim=True)\n",
    "    cam_map = (w * act[0]).sum(0)\n",
    "    cam_maps.append(cam_map)\n",
    "    break\n",
    "\n",
    "# print(\"Resizing\")\n",
    "# t_resized = [transformF.resize(torch.unsqueeze(cam_map, 0), ds_meta[\"image_size\"]) for cam_map in cam_maps]\n",
    "# t_resized = [torch.cat([x, x, x], dim=0).detach().cpu() for x in t_resized]\n",
    "# decoded_tensor_images = [dls.train.decode(x)[0][0].float() for x in tqdm(test_ds_new_ims, total = len(test_ds_new_ims))]\n",
    "\n",
    "# for ind in tqdm(range(len(t_resized))):\n",
    "#     decoded_tensor_images[ind][t_resized[ind] >=0.009] = 0.0\n",
    "#     decoded_tensor_images[ind] = torch.einsum(\"ijk->jki\", decoded_tensor_images[ind])\n",
    "\n",
    "# for ind in tqdm(range(len(decoded_tensor_images))):\n",
    "#     plt.imshow(decoded_tensor_images[ind])\n",
    "#     plt.axis(\"off\")\n",
    "#     ax=plt.gca()\n",
    "#     ax.get_xaxis().set_visible(False)\n",
    "#     plt.box(False)\n",
    "#     plt.savefig(rename_for_aug(im_names[ind]), transparent = True, bbox_inches='tight',pad_inches = 0)\n",
    "# clear_learner(learn, dls)\n",
    "# del bspred\n",
    "# del items\n",
    "# # del t_resized\n",
    "# gc.collect()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "db63ec5837ed7153ffcff7a5d42bf80536894c2cc03a9bb032bbe91b2078875e"
  },
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit ('pytorcher')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
