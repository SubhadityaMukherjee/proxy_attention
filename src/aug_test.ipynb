{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Imports\n",
    "from base64 import decode\n",
    "from config import ds_config\n",
    "from fastai.vision.all import *\n",
    "from fastai.callback.tensorboard import TensorBoardCallback\n",
    "import torchvision.transforms.functional as transformF\n",
    "\n",
    "# from fastai.callback.tracker import\n",
    "from fastai.vision.widgets import *\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "import argparse as ap\n",
    "import datetime\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "# from torch.multiprocessing import set_start_method\n",
    "# set_start_method('forkserver')\n",
    "\n",
    "# from torch.multiprocessing import Pool, Process, set_start_method\n",
    "# try:\n",
    "#      set_start_method('spawn')\n",
    "# except RuntimeError:\n",
    "#     pass\n",
    "\n",
    "from utils import *  # import utils at the end after fastai because the Hook function is a monkey-patch\n",
    "\n",
    "os.environ[\"TORCH_HOME\"] = \"/media/hdd/Datasets/\"\n",
    "os.environ[\"FASTAI_HOME\"] = \"/media/hdd/Datasets/\"\n",
    "\n",
    "# set_start_method('spawn')\n",
    "# Monkey patch batch prediction (fastai does not have this by default)\n",
    "Learner.predict_batch = predict_batch\n",
    "# %%\n",
    "# ags = ap.ArgumentParser(\"Additional Arguments for CLI\")\n",
    "# ags.add_argument(\n",
    "#     \"--config\", help=\"Name of config from dictionary\", default=\"fish_test_proxy\"\n",
    "# )\n",
    "# ags.add_argument(\"--name\", help=\"Name of the experiment\", required=True)\n",
    "# args = ags.parse_args()\n",
    "# ds_meta = ds_config[args.config]  # get info about dataset from the config file\n",
    "\n",
    "args = {\n",
    "    \"config\": \"fish_test_proxy\",\n",
    "    \"name\" : \"testing_ugh\"\n",
    "}\n",
    "from types import SimpleNamespace\n",
    "args = SimpleNamespace(**args)\n",
    "ds_meta = ds_config[args.config]  # get info about dataset from the config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] : File name = fish_testing_ugh_17102022_15:20:29\n",
      "[INFO] : Removed Old augmented files\n"
     ]
    }
   ],
   "source": [
    "path = Path(ds_meta[\"ds_path\"])\n",
    "fname_start = f'{ds_meta[\"ds_name\"]}_{args.name}_{datetime.now().strftime(\"%d%m%Y_%H:%M:%S\")}'  # unique_name\n",
    "print(f\"[INFO] : File name = {fname_start}\")\n",
    "\n",
    "# Check if directories all present\n",
    "create_if_not_exists(f\"tb_runs/{fname_start}\")\n",
    "create_if_not_exists(f\"csv_logs/{fname_start}\")\n",
    "# Remove previous files\n",
    "\n",
    "all_files = get_image_files(path)\n",
    "[Path.unlink(file) for file in all_files if \"augmented_\" in file.name]\n",
    "print(\"[INFO] : Removed Old augmented files\")\n",
    "\n",
    "# TODO : Add reset folder\n",
    "#%%\n",
    "batch_tfms = aug_transforms() if ds_meta[\"enable_default_augments\"] == True else None\n",
    "fields = DataBlock(\n",
    "    blocks=(ImageBlock, CategoryBlock),\n",
    "    get_items=get_image_files,\n",
    "    get_y=ds_meta[\"name_fn\"],\n",
    "    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n",
    "    item_tfms=RandomResizedCrop(ds_meta[\"image_size\"], min_scale=0.5),\n",
    "    batch_tfms=batch_tfms,\n",
    ")\n",
    "# Metrics\n",
    "metrics = [accuracy, error_rate]\n",
    "# Callbacks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Round 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eragon/micromamba/envs/pytorcher/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/eragon/micromamba/envs/pytorcher/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] : Cleared learner\n"
     ]
    }
   ],
   "source": [
    "dls = fields.dataloaders(path, bs=ds_meta[\"batch_size\"])\n",
    "cbs = [\n",
    "TensorBoardCallback(\n",
    "    log_dir=f\"tb_runs/{fname_start}\", projector=False, trace_model=False\n",
    "),\n",
    "CSVLogger(fname=f\"csv_logs/{fname_start}.csv\"),\n",
    "]\n",
    "\n",
    "learn = vision_learner(\n",
    "dls, ds_meta[\"network\"], cbs=cbs, metrics=metrics, pretrained=ds_meta[\"pretrained\"]\n",
    ").to_fp16()\n",
    "fname_training = f'{ds_meta[\"ds_name\"]}_{args.name}_{datetime.now().strftime(\"%d%m%Y_%H:%M:%S\")}'  # unique_name\n",
    "# learn.fine_tune(1)\n",
    "# learn.save(\"temp_model\")  # saving so can be reloaded\n",
    "learn.load(\"temp_model\")\n",
    "# clear_learner(learn, dls)\n",
    "print(\"[LOG] : Cleared learner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] : Starting Attention Loop\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Hook():\n",
    "    def __init__(self, m):\n",
    "        self.hook = m.register_forward_hook(self.hook_func)   \n",
    "    def hook_func(self, m, i, o): self.stored = o.detach().clone()\n",
    "    # Automatically register the hook when entering it\n",
    "    def __enter__(self, *args): return self\n",
    "    # Automatically remove the hook when exiting it\n",
    "    def __exit__(self, *args): self.hook.remove()\n",
    "\n",
    "class HookBwd():\n",
    "    def __init__(self, m):\n",
    "        self.hook = m.register_backward_hook(self.hook_func)   \n",
    "    def hook_func(self, m, gi, go): self.stored = go[0].detach().clone()\n",
    "    def __enter__(self, *args): return self\n",
    "    def __exit__(self, *args): self.hook.remove()\n",
    "\n",
    "# dls.to('cpu')\n",
    "\n",
    "# Get the classes\n",
    "print(\"[INFO] : Starting Attention Loop\")\n",
    "vocab_dict = {\n",
    "learn.dls.vocab[x]: x for x in range(len(learn.dls.vocab))\n",
    "}  # Get class names\n",
    "# Get images, shuffle, pick a subset\n",
    "items = get_image_files_exclude_augment(ds_meta[\"ds_path\"])\n",
    "items = items.shuffle()\n",
    "subset = int(ds_meta[\"change_subset_attention\"] * len(items))\n",
    "items = items[:subset]\n",
    "# Get preds from the network for all the chosen images with \"num_workers\" threads\n",
    "bspred = learn.predict_batch(items, num_workers=10)\n",
    "# Get all the class names for the subset of images and convert them into the One hot encoded version that the network knows already\n",
    "item_names = list(\n",
    "map(lambda x: vocab_dict[x], list(map(ds_meta[\"name_fn\"], items)))\n",
    ")\n",
    "\n",
    "# Get the index of all the images that the network predicted wrong\n",
    "# TODO : Check for confidence\n",
    "index_wrongs = [\n",
    "x for x in range(subset) if bspred[2][x] != TensorBase(item_names)[x]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 196178.86it/s]\n"
     ]
    }
   ],
   "source": [
    "ims = [items[x] for x in tqdm(index_wrongs, total = len(index_wrongs))]\n",
    "test_ds_new_ims = dls.test_dl(ims, bs = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = learn.get_preds(dl = test_ds_new_ims, with_decoded=True)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(lst, batch_size):\n",
    "    \"\"\"  Yields batch of specified size \"\"\"\n",
    "    return [lst[i : i + batch_size] for i in range(0, len(lst), batch_size)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/eragon/micromamba/envs/pytorcher/lib/python3.10/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "tensor(-9.8628, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# to_check = learn.model[-2][4][-1]\n",
    "# learn.eval()\n",
    "\n",
    "item_names_batched = generate_batch(item_names, test_ds_new_ims.bs)\n",
    "\n",
    "cam_maps = []\n",
    "for i, data in tqdm(enumerate(test_ds_new_ims)):\n",
    "\n",
    "    data = dls.train.decode(data)\n",
    "    y_values = item_names_batched[i]\n",
    "\n",
    "    with HookBwd(learn.model[-2][4][-1]) as hookg:  # for other layers\n",
    "        with Hook(learn.model[-2][4][-1]) as hook:\n",
    "            preds = learn.model.eval()(data[0].float().to(\"cuda\"))\n",
    "            # print(len(preds))\n",
    "            # acts = hook.stored\n",
    "            act = hook.stored\n",
    "        \n",
    "        # THIS is the issue I think.\n",
    "        for i_2 in range(len(preds)):\n",
    "            preds[i_2,y_values[i_2]-1].backward(retain_graph = True)\n",
    "        grad = hookg.stored\n",
    "        for i in grad:\n",
    "          print(i.sum())\n",
    "    for i_3 in range(len(preds)):\n",
    "        w = grad[i_3].mean(dim=[1, 2], keepdim=True)\n",
    "        cam_map = (w * act[i_3]).sum(0)\n",
    "        # print(w.sum())\n",
    "        cam_maps.append(cam_map)\n",
    "    break\n",
    "\n",
    "    #     preds[0, 3].backward()\n",
    "    #     grad = hookg.stored\n",
    "    # w = grad[0].mean(dim=[1, 2], keepdim=True)\n",
    "    # cam_map = (w * act[0]).sum(0)\n",
    "    # cam_maps.append(cam_map)\n",
    "\n",
    "    #     for i_2, output_x in tqdm(enumerate(preds), total = len(preds)):\n",
    "    #         preds[i_2, y_values[i_2]].backward(retain_graph = True)\n",
    "    #     grads = hookg.stored\n",
    "    #     # print(grads[0].shape, acts[0].shape)\n",
    "    # for i_3, output_x in tqdm(enumerate(grads), total = len(grads)):\n",
    "    #     w = grads[i_3].mean(dim=[1, 2], keepdim=True)\n",
    "    #     # w = grads[i_3]\n",
    "    #     cam_map = (w * acts[i_3]).sum(0)\n",
    "    #     cam_maps.append(cam_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 64, 56, 56])"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGEAAABhCAYAAADGBs+jAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAABFElEQVR4nO3TsQ3CUBAFQWzRmkugSkpwb5xbcIL+CmbiC560um1m5sFS++oBiJAgQoAIASIEiBAgQoAIASIEPO8eHvvrmzt+0vl537rzCQEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIELANjOzesS/8wkBIgSIECBCgAgBIgSIECBCgAgBF56cC7tHAcfVAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGEAAABhCAYAAADGBs+jAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAABFElEQVR4nO3TsQ3CUBAFQWzRmkugSkpwb5xbcIL+CmbiC560um1m5sFS++oBiJAgQoAIASIEiBAgQoAIASIEPO8eHvvrmzt+0vl537rzCQEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIELANjOzesS/8wkBIgSIECBCgAgBIgSIECBCgAgBF56cC7tHAcfVAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGEAAABhCAYAAADGBs+jAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAABFElEQVR4nO3TsQ3CUBAFQWzRmkugSkpwb5xbcIL+CmbiC560um1m5sFS++oBiJAgQoAIASIEiBAgQoAIASIEPO8eHvvrmzt+0vl537rzCQEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIELANjOzesS/8wkBIgSIECBCgAgBIgSIECBCgAgBF56cC7tHAcfVAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGEAAABhCAYAAADGBs+jAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAABFElEQVR4nO3TsQ3CUBAFQWzRmkugSkpwb5xbcIL+CmbiC560um1m5sFS++oBiJAgQoAIASIEiBAgQoAIASIEPO8eHvvrmzt+0vl537rzCQEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIELANjOzesS/8wkBIgSIECBCgAgBIgSIECBCgAgBF56cC7tHAcfVAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGEAAABhCAYAAADGBs+jAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAABFElEQVR4nO3TsQ3CUBAFQWzRmkugSkpwb5xbcIL+CmbiC560um1m5sFS++oBiJAgQoAIASIEiBAgQoAIASIEPO8eHvvrmzt+0vl537rzCQEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIELANjOzesS/8wkBIgSIECBCgAgBIgSIECBCgAgBF56cC7tHAcfVAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGEAAABhCAYAAADGBs+jAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAABFElEQVR4nO3TsQ3CUBAFQWzRmkugSkpwb5xbcIL+CmbiC560um1m5sFS++oBiJAgQoAIASIEiBAgQoAIASIEPO8eHvvrmzt+0vl537rzCQEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIELANjOzesS/8wkBIgSIECBCgAgBIgSIECBCgAgBF56cC7tHAcfVAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGEAAABhCAYAAADGBs+jAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAABFElEQVR4nO3TsQ3CUBAFQWzRmkugSkpwb5xbcIL+CmbiC560um1m5sFS++oBiJAgQoAIASIEiBAgQoAIASIEPO8eHvvrmzt+0vl537rzCQEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIELANjOzesS/8wkBIgSIECBCgAgBIgSIECBCgAgBF56cC7tHAcfVAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGEAAABhCAYAAADGBs+jAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAABFElEQVR4nO3TsQ3CUBAFQWzRmkugSkpwb5xbcIL+CmbiC560um1m5sFS++oBiJAgQoAIASIEiBAgQoAIASIEPO8eHvvrmzt+0vl537rzCQEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIELANjOzesS/8wkBIgSIECBCgAgBIgSIECBCgAgBF56cC7tHAcfVAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGEAAABhCAYAAADGBs+jAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAABFElEQVR4nO3TsQ3CUBAFQWzRmkugSkpwb5xbcIL+CmbiC560um1m5sFS++oBiJAgQoAIASIEiBAgQoAIASIEPO8eHvvrmzt+0vl537rzCQEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIELANjOzesS/8wkBIgSIECBCgAgBIgSIECBCgAgBF56cC7tHAcfVAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGEAAABhCAYAAADGBs+jAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAABFElEQVR4nO3TsQ3CUBAFQWzRmkugSkpwb5xbcIL+CmbiC560um1m5sFS++oBiJAgQoAIASIEiBAgQoAIASIEPO8eHvvrmzt+0vl537rzCQEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIELANjOzesS/8wkBIgSIECBCgAgBIgSIECBCgAgBF56cC7tHAcfVAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGEAAABhCAYAAADGBs+jAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAABFElEQVR4nO3TsQ3CUBAFQWzRmkugSkpwb5xbcIL+CmbiC560um1m5sFS++oBiJAgQoAIASIEiBAgQoAIASIEPO8eHvvrmzt+0vl537rzCQEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIELANjOzesS/8wkBIgSIECBCgAgBIgSIECBCgAgBF56cC7tHAcfVAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGEAAABhCAYAAADGBs+jAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAABFElEQVR4nO3TsQ3CUBAFQWzRmkugSkpwb5xbcIL+CmbiC560um1m5sFS++oBiJAgQoAIASIEiBAgQoAIASIEPO8eHvvrmzt+0vl537rzCQEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIELANjOzesS/8wkBIgSIECBCgAgBIgSIECBCgAgBF56cC7tHAcfVAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGEAAABhCAYAAADGBs+jAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAABFElEQVR4nO3TsQ3CUBAFQWzRmkugSkpwb5xbcIL+CmbiC560um1m5sFS++oBiJAgQoAIASIEiBAgQoAIASIEPO8eHvvrmzt+0vl537rzCQEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIELANjOzesS/8wkBIgSIECBCgAgBIgSIECBCgAgBF56cC7tHAcfVAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGEAAABhCAYAAADGBs+jAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAABFElEQVR4nO3TsQ3CUBAFQWzRmkugSkpwb5xbcIL+CmbiC560um1m5sFS++oBiJAgQoAIASIEiBAgQoAIASIEPO8eHvvrmzt+0vl537rzCQEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIELANjOzesS/8wkBIgSIECBCgAgBIgSIECBCgAgBF56cC7tHAcfVAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGEAAABhCAYAAADGBs+jAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAA9hAAAPYQGoP6dpAAABFElEQVR4nO3TsQ3CUBAFQWzRmkugSkpwb5xbcIL+CmbiC560um1m5sFS++oBiJAgQoAIASIEiBAgQoAIASIEPO8eHvvrmzt+0vl537rzCQEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIEKACAEiBIgQIELANjOzesS/8wkBIgSIECBCgAgBIgSIECBCgAgBF56cC7tHAcfVAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(len(cam_maps)):\n",
    "    show_image(cam_maps[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing\n"
     ]
    }
   ],
   "source": [
    "print(\"Resizing\")\n",
    "t_resized = [transformF.resize(torch.unsqueeze(cam_map, 0), ds_meta[\"image_size\"]) for cam_map in cam_maps]\n",
    "t_resized = [torch.cat([x, x, x], dim=0).detach().cpu() for x in t_resized]\n",
    "t_resized_batch = generate_batch(t_resized, test_ds_new_ims.bs)\n",
    "\n",
    "decoded_tensor_images = dls.train.decode(data)\n",
    "# decoded_tensor_images = [dls.train.decode(x)[0][0].float() for x in tqdm(test_ds_new_ims, total = len(test_ds_new_ims))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3, 224, 224])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_tensor_images[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 1\n"
     ]
    }
   ],
   "source": [
    "print(len(t_resized_batch), len(decoded_tensor_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224]) torch.Size([3, 224, 224])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for dimension 0 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/media/hdd/github/improving_robotics_datasets/src/aug_test.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/hdd/github/improving_robotics_datasets/src/aug_test.ipynb#ch0000034?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(t_resized_batch)):\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/hdd/github/improving_robotics_datasets/src/aug_test.ipynb#ch0000034?line=1'>2</a>\u001b[0m     \u001b[39mfor\u001b[39;00m index \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(t_resized_batch[batch])):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/hdd/github/improving_robotics_datasets/src/aug_test.ipynb#ch0000034?line=2'>3</a>\u001b[0m         \u001b[39mprint\u001b[39m(decoded_tensor_images[batch][index]\u001b[39m.\u001b[39mshape, t_resized_batch[batch][index]\u001b[39m.\u001b[39mshape)\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/hdd/github/improving_robotics_datasets/src/aug_test.ipynb#ch0000034?line=3'>4</a>\u001b[0m         decoded_tensor_images[batch][index][t_resized_batch[batch][index] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m\u001b[39m0.009\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n",
      "File \u001b[0;32m~/micromamba/envs/pytorcher/lib/python3.10/site-packages/fastai/torch_core.py:376\u001b[0m, in \u001b[0;36mTensorBase.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/eragon/micromamba/envs/pytorcher/lib/python3.10/site-packages/fastai/torch_core.py?line=373'>374</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mdebug \u001b[39mand\u001b[39;00m func\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39m__str__\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m__repr__\u001b[39m\u001b[39m'\u001b[39m): \u001b[39mprint\u001b[39m(func, types, args, kwargs)\n\u001b[1;32m    <a href='file:///home/eragon/micromamba/envs/pytorcher/lib/python3.10/site-packages/fastai/torch_core.py?line=374'>375</a>\u001b[0m \u001b[39mif\u001b[39;00m _torch_handled(args, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_opt, func): types \u001b[39m=\u001b[39m (torch\u001b[39m.\u001b[39mTensor,)\n\u001b[0;32m--> <a href='file:///home/eragon/micromamba/envs/pytorcher/lib/python3.10/site-packages/fastai/torch_core.py?line=375'>376</a>\u001b[0m res \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m__torch_function__(func, types, args, ifnone(kwargs, {}))\n\u001b[1;32m    <a href='file:///home/eragon/micromamba/envs/pytorcher/lib/python3.10/site-packages/fastai/torch_core.py?line=376'>377</a>\u001b[0m dict_objs \u001b[39m=\u001b[39m _find_args(args) \u001b[39mif\u001b[39;00m args \u001b[39melse\u001b[39;00m _find_args(\u001b[39mlist\u001b[39m(kwargs\u001b[39m.\u001b[39mvalues()))\n\u001b[1;32m    <a href='file:///home/eragon/micromamba/envs/pytorcher/lib/python3.10/site-packages/fastai/torch_core.py?line=377'>378</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mtype\u001b[39m(res),TensorBase) \u001b[39mand\u001b[39;00m dict_objs: res\u001b[39m.\u001b[39mset_meta(dict_objs[\u001b[39m0\u001b[39m],as_copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/micromamba/envs/pytorcher/lib/python3.10/site-packages/torch/_tensor.py:1121\u001b[0m, in \u001b[0;36mTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/eragon/micromamba/envs/pytorcher/lib/python3.10/site-packages/torch/_tensor.py?line=1117'>1118</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[1;32m   <a href='file:///home/eragon/micromamba/envs/pytorcher/lib/python3.10/site-packages/torch/_tensor.py?line=1119'>1120</a>\u001b[0m \u001b[39mwith\u001b[39;00m _C\u001b[39m.\u001b[39mDisableTorchFunction():\n\u001b[0;32m-> <a href='file:///home/eragon/micromamba/envs/pytorcher/lib/python3.10/site-packages/torch/_tensor.py?line=1120'>1121</a>\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/eragon/micromamba/envs/pytorcher/lib/python3.10/site-packages/torch/_tensor.py?line=1121'>1122</a>\u001b[0m     \u001b[39mif\u001b[39;00m func \u001b[39min\u001b[39;00m get_default_nowrap_functions():\n\u001b[1;32m   <a href='file:///home/eragon/micromamba/envs/pytorcher/lib/python3.10/site-packages/torch/_tensor.py?line=1122'>1123</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mIndexError\u001b[0m: index 3 is out of bounds for dimension 0 with size 3"
     ]
    }
   ],
   "source": [
    "for batch in range(len(t_resized_batch)):\n",
    "    for index in range(len(t_resized_batch[batch])):\n",
    "        print(decoded_tensor_images[batch][index].shape, t_resized_batch[batch][index].shape)\n",
    "        decoded_tensor_images[batch][index][t_resized_batch[batch][index] >=0.009] = 0.0\n",
    "        # decoded_tensor_images[batch][index] = torch.einsum(\"ijk->jki\", decoded_tensor_images[batch][index])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69/69 [00:00<00:00, 519.24it/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating map\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]/home/eragon/micromamba/envs/pytorcher/lib/python3.10/site-packages/torch/nn/modules/module.py:1053: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([69, 18])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ims = [PILImage.create(items[x]) for x in tqdm(index_wrongs, total = len(index_wrongs))]\n",
    "im_names = [items[x] for x in index_wrongs]\n",
    "test_ds_new_ims = dls.test_dl(ims, shuffle = False)\n",
    "decoded = [dls.train.decode(x)[0].float() for x in tqdm(test_ds_new_ims, total = len(test_ds_new_ims))]\n",
    "eval_model =learn.model.eval() \n",
    "# eval_model_results = [eval_model(first(x).float().cuda()) for x in test_ds_new_ims]\n",
    "# eval_model_results = learn.predict_batch(ims)[0]\n",
    "# print(eval_model_results)\n",
    "# Hook\n",
    "cls = 1\n",
    "cam_maps = []\n",
    "\n",
    "print(\"Creating map\")\n",
    "for im in tqdm(decoded, total = len(decoded)):\n",
    "    with HookBwd(learn.model[-2][4][-1]) as hookg:  # for other layers\n",
    "        with Hook(learn.model[-2][4][-1]) as hook:\n",
    "            output = eval_model(im.cuda())\n",
    "            print(output.shape)\n",
    "            # output = learn.predict(im)\n",
    "            act = hook.stored\n",
    "        output[0, cls].backward()\n",
    "        grad = hookg.stored\n",
    "    w = grad[0].mean(dim=[1, 2], keepdim=True)\n",
    "    cam_map = (w * act[0]).sum(0)\n",
    "    cam_maps.append(cam_map)\n",
    "    break\n",
    "\n",
    "# print(\"Resizing\")\n",
    "# t_resized = [transformF.resize(torch.unsqueeze(cam_map, 0), ds_meta[\"image_size\"]) for cam_map in cam_maps]\n",
    "# t_resized = [torch.cat([x, x, x], dim=0).detach().cpu() for x in t_resized]\n",
    "# decoded_tensor_images = [dls.train.decode(x)[0][0].float() for x in tqdm(test_ds_new_ims, total = len(test_ds_new_ims))]\n",
    "\n",
    "# for ind in tqdm(range(len(t_resized))):\n",
    "#     decoded_tensor_images[ind][t_resized[ind] >=0.009] = 0.0\n",
    "#     decoded_tensor_images[ind] = torch.einsum(\"ijk->jki\", decoded_tensor_images[ind])\n",
    "\n",
    "# for ind in tqdm(range(len(decoded_tensor_images))):\n",
    "#     plt.imshow(decoded_tensor_images[ind])\n",
    "#     plt.axis(\"off\")\n",
    "#     ax=plt.gca()\n",
    "#     ax.get_xaxis().set_visible(False)\n",
    "#     plt.box(False)\n",
    "#     plt.savefig(rename_for_aug(im_names[ind]), transparent = True, bbox_inches='tight',pad_inches = 0)\n",
    "# clear_learner(learn, dls)\n",
    "# del bspred\n",
    "# del items\n",
    "# # del t_resized\n",
    "# gc.collect()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "db63ec5837ed7153ffcff7a5d42bf80536894c2cc03a9bb032bbe91b2078875e"
  },
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit ('pytorcher')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
