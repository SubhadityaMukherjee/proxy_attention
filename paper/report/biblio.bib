
@article{eppel_classifying_nodate,
	title = {Classifying a specific image region using convolutional nets with an {ROI} mask as input},
	abstract = {Convolutional neural nets (CNN) are the leading computer vision method for classifying images. In some cases, it is desirable to classify only a specific region of the image that corresponds to a certain object. Hence, assuming that the region of the object in the image is known in advance and is given as a binary region of interest (ROI) mask, the goal is to classify the object in this region using a convolutional neural net. This goal is achieved using a standard image classification net with the addition of a side branch, which converts the ROI mask into an attention map. This map is then combined with the image classification net. This allows the net to focus the attention on the object region while still extracting contextual cues from the background. This approach was evaluated using the COCO object dataset and the OpenSurfaces materials dataset. In both cases, it gave superior results to methods that completely ignore the background region. In addition, it was found that combining the attention map at the first layer of the net gave better results than combining it at higher layers of the net. The advantages of this method are most apparent in the classification of small regions which demands a great deal of contextual information from the background.},
	language = {en},
	author = {Eppel, Sagi},
	keywords = {done},
	pages = {8},
}

@techreport{mitsuhara_embedding_2019,
	title = {Embedding {Human} {Knowledge} into {Deep} {Neural} {Network} via {Attention} {Map}},
	url = {http://arxiv.org/abs/1905.03540},
	abstract = {In this work, we aim to realize a method for embedding human knowledge into deep neural networks. While the conventional method to embed human knowledge has been applied for non-deep machine learning, it is challenging to apply it for deep learning models due to the enormous number of model parameters. To tackle this problem, we focus on the attention mechanism of an attention branch network (ABN). In this paper, we propose a fine-tuning method that utilizes a single-channel attention map which is manually edited by a human expert. Our fine-tuning method can train a network so that the output attention map corresponds to the edited ones. As a result, the fine-tuned network can output an attention map that takes into account human knowledge. Experimental results with ImageNet, CUB-200-2010, and IDRiD demonstrate that it is possible to obtain a clear attention map for a visual explanation and improve the classification performance. Our findings can be a novel framework for optimizing networks through human intuitive editing via a visual interface and suggest new possibilities for human-machine cooperation in addition to the improvement of visual explanations.},
	number = {arXiv:1905.03540},
	urldate = {2022-10-03},
	institution = {arXiv},
	author = {Mitsuhara, Masahiro and Fukui, Hiroshi and Sakashita, Yusuke and Ogata, Takanori and Hirakawa, Tsubasa and Yamashita, Takayoshi and Fujiyoshi, Hironobu},
	month = dec,
	year = {2019},
	note = {arXiv:1905.03540 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, done},
	annote = {Comment: 10 pages, 10 figures},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/2SVPKSTS/Mitsuhara et al. - 2019 - Embedding Human Knowledge into Deep Neural Network.pdf:application/pdf},
}

@techreport{lan_couplformerrethinking_2021,
	title = {Couplformer:{Rethinking} {Vision} {Transformer} with {Coupling} {Attention} {Map}},
	shorttitle = {Couplformer},
	url = {http://arxiv.org/abs/2112.05425},
	abstract = {With the development of the self-attention mechanism, the Transformer model has demonstrated its outstanding performance in the computer vision domain. However, the massive computation brought from the full attention mechanism became a heavy burden for memory consumption. Sequentially, the limitation of memory reduces the possibility of improving the Transformer model. To remedy this problem, we propose a novel memory economy attention mechanism named Couplformer, which decouples the attention map into two sub-matrices and generates the alignment scores from spatial information. A series of different scale image classification tasks are applied to evaluate the effectiveness of our model. The result of experiments shows that on the ImageNet-1k classification task, the Couplformer can significantly decrease 28\% memory consumption compared with regular Transformer while accessing sufficient accuracy requirements and outperforming 0.92\% on Top-1 accuracy while occupying the same memory footprint. As a result, the Couplformer can serve as an efficient backbone in visual tasks, and provide a novel perspective on the attention mechanism for researchers.},
	number = {arXiv:2112.05425},
	urldate = {2022-10-03},
	institution = {arXiv},
	author = {Lan, Hai and Wang, Xihao and Wei, Xian},
	month = dec,
	year = {2021},
	note = {arXiv:2112.05425 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 11 pages, 4 figures},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/FB5UAR58/Lan et al. - 2021 - CouplformerRethinking Vision Transformer with Cou.pdf:application/pdf},
}

@inproceedings{bello_attention_2019,
	address = {Seoul, Korea (South)},
	title = {Attention {Augmented} {Convolutional} {Networks}},
	isbn = {978-1-72814-803-8},
	url = {https://ieeexplore.ieee.org/document/9010285/},
	doi = {10.1109/ICCV.2019.00338},
	abstract = {Convolutional networks have been the paradigm of choice in many computer vision applications. The convolution operation however has a signiﬁcant weakness in that it only operates on a local neighborhood, thus missing global information. Self-attention, on the other hand, has emerged as a recent advance to capture long range interactions, but has mostly been applied to sequence modeling and generative modeling tasks. In this paper, we consider the use of self-attention for discriminative visual tasks as an alternative to convolutions. We introduce a novel two-dimensional relative self-attention mechanism that proves competitive in replacing convolutions as a stand-alone computational primitive for image classiﬁcation. We ﬁnd in control experiments that the best results are obtained when combining both convolutions and self-attention. We therefore propose to augment convolutional operators with this self-attention mechanism by concatenating convolutional feature maps with a set of feature maps produced via self-attention. Extensive experiments show that Attention Augmentation leads to consistent improvements in image classiﬁcation on ImageNet and object detection on COCO across many different models and scales, including ResNets and a stateof-the art mobile constrained network, while keeping the number of parameters similar. In particular, our method achieves a 1.3\% top-1 accuracy improvement on ImageNet classiﬁcation over a ResNet50 baseline and outperforms other attention mechanisms for images such as Squeezeand-Excitation [17]. It also achieves an improvement of 1.4 mAP in COCO Object Detection on top of a RetinaNet baseline.},
	language = {en},
	urldate = {2022-10-03},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Bello, Irwan and Zoph, Barret and Le, Quoc and Vaswani, Ashish and Shlens, Jonathon},
	month = oct,
	year = {2019},
	pages = {3285--3294},
	file = {Bello et al. - 2019 - Attention Augmented Convolutional Networks.pdf:/Users/eragon/Zotero/storage/WENTJ9EM/Bello et al. - 2019 - Attention Augmented Convolutional Networks.pdf:application/pdf},
}

@article{oyama_influence_2018,
	title = {Influence of image classification accuracy on saliency map estimation},
	volume = {3},
	issn = {2468-2322},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/trit.2018.1012},
	doi = {10.1049/trit.2018.1012},
	abstract = {Saliency map estimation in computer vision aims to estimate the locations where people gaze in images. Since people tend to look at objects in images, the parameters of the model pre-trained on ImageNet for image classification are useful for the saliency map estimation. However, there is no research on the relationship between the image classification accuracy and the performance of the saliency map estimation. In this study, it is shown that there is a strong correlation between image classification accuracy and saliency map estimation accuracy. The authors also investigated the effective architecture based on multi-scale images and the up-sampling layers to refine the saliency-map resolution. The model achieved the state-of-the-art accuracy on the PASCAL-S, OSIE, and MIT1003 datasets. In the MIT saliency benchmark, the model achieved the best performance in some metrics and competitive results in the other metrics.},
	language = {en},
	number = {3},
	urldate = {2022-10-03},
	journal = {CAAI Transactions on Intelligence Technology},
	author = {Oyama, Taiki and Yamanaka, Takao},
	year = {2018},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1049/trit.2018.1012},
	keywords = {computer vision, (B6135) Optical, (C5260B) Computer vision and image processing techniques, image and video signal processing, image classification, ImageNet, MIT1003, multiscale images, OSIE, PASCAL-S, saliency-map resolution, up-sampling layer},
	pages = {140--152},
	file = {Full Text PDF:/Users/eragon/Zotero/storage/HHL5L9G9/Oyama and Yamanaka - 2018 - Influence of image classification accuracy on sali.pdf:application/pdf},
}

@article{selvaraju_grad-cam_nodate,
	title = {Grad-{CAM}: {Visual} {Explanations} {From} {Deep} {Networks} via {Gradient}-{Based} {Localization}},
	abstract = {We propose a technique for producing ‘visual explanations’ for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent. Our approach – Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say logits for ‘dog’ or even a caption), ﬂowing into the ﬁnal convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, GradCAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in tasks with multi-modal inputs (e.g. visual question answering) or reinforcement learning, without architectural changes or re-training. We combine Grad-CAM with existing ﬁne-grained visualizations to create a high-resolution class-discriminative visualization, Guided Grad-CAM, and apply it to image classiﬁcation, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classiﬁcation models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) outperform previous methods on the ILSVRC-15 weakly-supervised localization task, (c) are more faithful to the underlying model, and (d) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visualizations show even non-attention based models can localize inputs. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that Grad-CAM helps untrained users successfully discern a ‘stronger’ deep network from a ‘weaker’ one even when both make identical predictions. Our code is available at https: //github.com/ramprs/grad-cam/ along with a demo on CloudCV [2]1 and video at youtu.be/COjUB9Izk6E.},
	language = {en},
	author = {Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
	pages = {9},
	file = {Selvaraju et al. - Grad-CAM Visual Explanations From Deep Networks v.pdf:/Users/eragon/Zotero/storage/EDVF5SE9/Selvaraju et al. - Grad-CAM Visual Explanations From Deep Networks v.pdf:application/pdf},
}

@inproceedings{chakraborty_generalizing_2022,
	address = {New Orleans, LA, USA},
	title = {Generalizing {Adversarial} {Explanations} with {Grad}-{CAM}},
	isbn = {978-1-66548-739-9},
	url = {https://ieeexplore.ieee.org/document/9857321/},
	doi = {10.1109/CVPRW56347.2022.00031},
	abstract = {Gradient-weighted Class Activation Mapping (GradCAM), is an example-based explanation method that provides a gradient activation heat map as an explanation for Convolution Neural Network (CNN) models. The drawback of this method is that it cannot be used to generalize CNN behaviour. In this paper, we present a novel method that extends Grad-CAM from example-based explanations to a method for explaining global model behaviour. This is achieved by introducing two new metrics, (i) Mean Observed Dissimilarity (MOD) and (ii) Variation in Dissimilarity (VID), for model generalization. These metrics are computed by comparing a Normalized Inverted Structural Similarity Index (NISSIM) metric of the Grad-CAM generated heatmap for samples from the original test set and samples from the adversarial test set. For our experiment, we study adversarial attacks on deep models such as VGG16, ResNet50, and ResNet101, and wide models such as InceptionNetv3 and XceptionNet using Fast Gradient Sign Method (FGSM). We then compute the metrics MOD and VID for the automatic face recognition (AFR) use case with the VGGFace2 dataset. We observe a consistent shift in the region highlighted in the Grad-CAM heatmap, reflecting its participation to the decision making, across all models under adversarial attacks. The proposed method can be used to understand adversarial attacks and explain the behaviour of black box CNN models for image analysis.},
	language = {en},
	urldate = {2022-10-03},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	publisher = {IEEE},
	author = {Chakraborty, Tanmay and Trehan, Utkarsh and Mallat, Khawla and Dugelay, Jean-Luc},
	month = jun,
	year = {2022},
	keywords = {read},
	pages = {186--192},
	file = {Chakraborty et al. - 2022 - Generalizing Adversarial Explanations with Grad-CA.pdf:/Users/eragon/Zotero/storage/6G98B28D/Chakraborty et al. - 2022 - Generalizing Adversarial Explanations with Grad-CA.pdf:application/pdf},
}

@inproceedings{chattopadhyay_grad-cam_2018,
	title = {Grad-{CAM}++: {Improved} {Visual} {Explanations} for {Deep} {Convolutional} {Networks}},
	shorttitle = {Grad-{CAM}++},
	url = {http://arxiv.org/abs/1710.11063},
	doi = {10.1109/WACV.2018.00097},
	abstract = {Over the last decade, Convolutional Neural Network (CNN) models have been highly successful in solving complex vision problems. However, these deep models are perceived as "black box" methods considering the lack of understanding of their internal functioning. There has been a significant recent interest in developing explainable deep learning models, and this paper is an effort in this direction. Building on a recently proposed method called Grad-CAM, we propose a generalized method called Grad-CAM++ that can provide better visual explanations of CNN model predictions, in terms of better object localization as well as explaining occurrences of multiple object instances in a single image, when compared to state-of-the-art. We provide a mathematical derivation for the proposed method, which uses a weighted combination of the positive partial derivatives of the last convolutional layer feature maps with respect to a specific class score as weights to generate a visual explanation for the corresponding class label. Our extensive experiments and evaluations, both subjective and objective, on standard datasets showed that Grad-CAM++ provides promising human-interpretable visual explanations for a given CNN architecture across multiple tasks including classification, image caption generation and 3D action recognition; as well as in new settings such as knowledge distillation.},
	urldate = {2022-10-03},
	booktitle = {2018 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	author = {Chattopadhyay, Aditya and Sarkar, Anirban and Howlader, Prantik and Balasubramanian, Vineeth N.},
	month = mar,
	year = {2018},
	note = {arXiv:1710.11063 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {839--847},
	annote = {Comment: 17 Pages, 15 Figures, 11 Tables. Accepted in the proceedings of IEEE Winter Conf. on Applications of Computer Vision (WACV2018). Extended version is under review at IEEE Transactions on Pattern Analysis and Machine Intelligence},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/V2RWR75J/Chattopadhyay et al. - 2018 - Grad-CAM++ Improved Visual Explanations for Deep .pdf:application/pdf},
}

@incollection{holzinger_overlap_2021,
	address = {Cham},
	title = {On the {Overlap} {Between} {Grad}-{CAM} {Saliency} {Maps} and {Explainable} {Visual} {Features} in {Skin} {Cancer} {Images}},
	volume = {12844},
	isbn = {978-3-030-84059-4 978-3-030-84060-0},
	url = {https://link.springer.com/10.1007/978-3-030-84060-0_16},
	abstract = {Dermatologists recognize melanomas by inspecting images in which they identify human-comprehensible visual features. In this paper, we investigate to what extent such features correspond to the saliency areas identiﬁed on CNNs trained for classiﬁcation. Our experiments, conducted on two neural architectures characterized by different depth and different resolution of the last convolutional layer, quantify to what extent thresholded Grad-CAM saliency maps can be used to identify visual features of skin cancer. We found that the best threshold value, i.e., the threshold at which we can measure the highest Jaccard index, varies signiﬁcantly among features; ranging from 0.3 to 0.7. In addition, we measured Jaccard indices as high as 0.143, which is almost 50\% of the performance of state-of-the-art architectures specialized in feature mask prediction at pixel-level, such as U-Net. Finally, a breakdown test between malignancy and classiﬁcation correctness shows that higher resolution saliency maps could help doctors in spotting wrong classiﬁcations.},
	language = {en},
	urldate = {2022-10-03},
	booktitle = {Machine {Learning} and {Knowledge} {Extraction}},
	publisher = {Springer International Publishing},
	author = {Nunnari, Fabrizio and Kadir, Md Abdul and Sonntag, Daniel},
	editor = {Holzinger, Andreas and Kieseberg, Peter and Tjoa, A Min and Weippl, Edgar},
	year = {2021},
	doi = {10.1007/978-3-030-84060-0_16},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {241--253},
	file = {Nunnari et al. - 2021 - On the Overlap Between Grad-CAM Saliency Maps and .pdf:/Users/eragon/Zotero/storage/33H2VKZ5/Nunnari et al. - 2021 - On the Overlap Between Grad-CAM Saliency Maps and .pdf:application/pdf},
}

@misc{howard2018fastai,
  title={fastai},
  author={Howard, Jeremy and others},
  year={2018},
  publisher={GitHub},
  howpublished={\url{https://github.com/fastai/fastai}},
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{bradski2000opencv,
  title={The openCV library.},
  author={Bradski, Gary},
  journal={Dr. Dobb's Journal: Software Tools for the Professional Programmer},
  volume={25},
  number={11},
  pages={120--123},
  year={2000},
  publisher={Miller Freeman Inc.}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}
@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}