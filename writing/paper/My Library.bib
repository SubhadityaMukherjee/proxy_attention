@online{abnarQuantifyingAttentionFlow2020,
  title = {Quantifying {{Attention Flow}} in {{Transformers}}},
  author = {Abnar, Samira and Zuidema, Willem},
  date = {2020-05-31},
  eprint = {2005.00928},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2005.00928},
  url = {http://arxiv.org/abs/2005.00928},
  urldate = {2023-06-13},
  abstract = {In the Transformer model, "self-attention" combines information from attended embeddings into the representation of the focal embedding in the next layer. Thus, across layers of the Transformer, information originating from different tokens gets increasingly mixed. This makes attention weights unreliable as explanations probes. In this paper, we consider the problem of quantifying this flow of information through self-attention. We propose two methods for approximating the attention to input tokens given attention weights, attention rollout and attention flow, as post hoc methods when we use attention weights as the relative relevance of the input tokens. We show that these methods give complementary views on the flow of information, and compared to raw attention, both yield higher correlations with importance scores of input tokens obtained using an ablation method and input gradients.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/eragon/Zotero/storage/NTT94JAS/Abnar and Zuidema - 2020 - Quantifying Attention Flow in Transformers.pdf}
}

@article{adadiPeekingBlackBoxSurvey2018,
  title = {Peeking {{Inside}} the {{Black-Box}}: {{A Survey}} on {{Explainable Artificial Intelligence}} ({{XAI}})},
  shorttitle = {Peeking {{Inside}} the {{Black-Box}}},
  author = {Adadi, Amina and Berrada, Mohammed},
  date = {2018},
  journaltitle = {IEEE Access},
  volume = {6},
  pages = {52138--52160},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2018.2870052},
  abstract = {At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.},
  eventtitle = {{{IEEE Access}}},
  keywords = {Biological system modeling,black-box models,Conferences,Explainable artificial intelligence,interpretable machine learning,Machine learning,Machine learning algorithms,Market research,Prediction algorithms},
  file = {/Users/eragon/Zotero/storage/985A55B6/Adadi and Berrada - 2018 - Peeking Inside the Black-Box A Survey on Explaina.pdf}
}

@online{adebayoSanityChecksSaliency2020,
  title = {Sanity {{Checks}} for {{Saliency Maps}}},
  author = {Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian and Hardt, Moritz and Kim, Been},
  date = {2020-11-06},
  eprint = {1810.03292},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1810.03292},
  url = {http://arxiv.org/abs/1810.03292},
  urldate = {2022-11-28},
  abstract = {Saliency methods have emerged as a popular tool to highlight features in an input deemed relevant for the prediction of a learned model. Several saliency methods have been proposed, often guided by visual appeal on image data. In this work, we propose an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide. We find that reliance, solely, on visual assessment can be misleading. Through extensive experiments we show that some existing saliency methods are independent both of the model and of the data generating process. Consequently, methods that fail the proposed tests are inadequate for tasks that are sensitive to either data or model, such as, finding outliers in the data, explaining the relationship between inputs and outputs that the model learned, and debugging the model. We interpret our findings through an analogy with edge detection in images, a technique that requires neither training data nor model. Theory in the case of a linear model and a single-layer convolutional neural network supports our experimental findings.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/eragon/Zotero/storage/DJDSS9U5/Adebayo et al. - 2020 - Sanity Checks for Saliency Maps.pdf}
}

@book{adelmanSensorySystemVision2013,
  title = {Sensory {{System I}}: {{Vision}} and {{Visual Systems}}},
  shorttitle = {Sensory {{System I}}},
  author = {ADELMAN},
  date = {2013-12-19},
  eprint = {vGn5BwAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {{Birkhäuser}},
  isbn = {978-1-4899-6647-6},
  langid = {english},
  pagetotal = {129},
  keywords = {Philosophy / General,Science / General,Social Science / General}
}

@online{aghelanUnderwaterImagesSuperResolution2022,
  title = {Underwater {{Images Super-Resolution Using Generative Adversarial Network-based Model}}},
  author = {Aghelan, Alireza},
  date = {2022-11-07},
  eprint = {2211.03550},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  doi = {10.48550/arXiv.2211.03550},
  url = {http://arxiv.org/abs/2211.03550},
  urldate = {2022-11-19},
  abstract = {Single image super-resolution (SISR) methods can enhance the resolution and quality of underwater images. Enhancing the resolution of underwater images leads to better performance of autonomous underwater vehicles. In this work, we fine-tune the Real-Enhanced Super-Resolution Generative Adversarial Network (Real-ESRGAN) model to increase the resolution of underwater images. In our proposed approach, the pre-trained generator and discriminator networks of the Real-ESRGAN model are fine-tuned using underwater image datasets. We used the USR-248 and UFO-120 datasets to fine-tune the Real-ESRGAN model. Our fine-tuned model produces images with better resolution and quality compared to the original model.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/eragon/Zotero/storage/K7B929UB/Aghelan - 2022 - Underwater Images Super-Resolution Using Generativ.pdf}
}

@article{aharonySocialFMRIInvestigating2011,
  title = {Social {{fMRI}}: {{Investigating}} and Shaping Social Mechanisms in the Real World},
  shorttitle = {Social {{fMRI}}},
  author = {Aharony, Nadav and Pan, Wei and Ip, Cory and Khayal, Inas and Pentland, Alex},
  date = {2011-12},
  journaltitle = {Pervasive and Mobile Computing},
  shortjournal = {Pervasive and Mobile Computing},
  volume = {7},
  number = {6},
  pages = {643--659},
  issn = {15741192},
  doi = {10.1016/j.pmcj.2011.09.004},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1574119211001246},
  urldate = {2022-06-15},
  abstract = {We introduce the Friends and Family study, a longitudinal living laboratory in a residential community. In this study, we employ a ubiquitous computing approach, Social Functional Mechanism-design and Relationship Imaging, or Social fMRI, that combines extremely rich data collection with the ability to conduct targeted experimental interventions with study populations. We present our mobile-phone-based social and behavioral sensing system, deployed in the wild for over 15 months. Finally, we present three investigations performed during the study, looking into the connection between individuals’ social behavior and their financial status, network effects in decision making, and a novel intervention aimed at increasing physical activity in the subject population. Results demonstrate the value of social factors for choice, motivation, and adherence, and enable quantifying the contribution of different incentive mechanisms.},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/GK5C7SYB/Aharony et al. - 2011 - Social fMRI Investigating and shaping social mech.pdf}
}

@online{alammarIllustratedTransformer,
  title = {The {{Illustrated Transformer}}},
  author = {Alammar, Jay},
  url = {https://jalammar.github.io/illustrated-transformer/},
  urldate = {2022-05-25},
  abstract = {Discussions: Hacker News (65 points, 4 comments), Reddit r/MachineLearning (29 points, 3 comments) Translations: Chinese (Simplified), French 1, French 2, Japanese, Korean, Russian, Spanish, Vietnamese Watch: MIT’s Deep Learning State of the Art lecture referencing this post In the previous post, we looked at Attention – a ubiquitous method in modern deep learning models. Attention is a concept that helped improve the performance of neural machine translation applications. In this post, we will look at The Transformer – a model that uses attention to boost the speed with which these models can be trained. The Transformer outperforms the Google Neural Machine Translation model in specific tasks. The biggest benefit, however, comes from how The Transformer lends itself to parallelization. It is in fact Google Cloud’s recommendation to use The Transformer as a reference model to use their Cloud TPU offering. So let’s try to break the model apart and look at how it functions. The Transformer was proposed in the paper Attention is All You Need. A TensorFlow implementation of it is available as a part of the Tensor2Tensor package. Harvard’s NLP group created a guide annotating the paper with PyTorch implementation. In this post, we will attempt to oversimplify things a bit and introduce the concepts one by one to hopefully make it easier to understand to people without in-depth knowledge of the subject matter. 2020 Update: I’ve created a “Narrated Transformer” video which is a gentler approach to the topic: A High-Level Look Let’s begin by looking at the model as a single black box. In a machine translation application, it would take a sentence in one language, and output its translation in another.},
  file = {/Users/eragon/Zotero/storage/WVBDCZFQ/illustrated-transformer.html}
}

@inproceedings{aliEvaluationLatentSpace2021,
  title = {Evaluation of {{Latent Space Learning With Procedurally-Generated Datasets}} of {{Shapes}}},
  author = {Ali, Sharjeel and family=Kaick, given=Oliver, prefix=van, useprefix=true},
  date = {2021},
  pages = {2086--2094},
  url = {https://openaccess.thecvf.com/content/ICCV2021W/DLGC/html/Ali_Evaluation_of_Latent_Space_Learning_With_Procedurally-Generated_Datasets_of_Shapes_ICCVW_2021_paper.html},
  urldate = {2022-09-06},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/SKFWLXEE/Ali and van Kaick - 2021 - Evaluation of Latent Space Learning With Procedura.pdf}
}

@online{anconaBetterUnderstandingGradientbased2018,
  title = {Towards Better Understanding of Gradient-Based Attribution Methods for {{Deep Neural Networks}}},
  author = {Ancona, Marco and Ceolini, Enea and Öztireli, Cengiz and Gross, Markus},
  date = {2018-03-07},
  eprint = {1711.06104},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1711.06104},
  url = {http://arxiv.org/abs/1711.06104},
  urldate = {2022-12-06},
  abstract = {Understanding the flow of information in Deep Neural Networks (DNNs) is a challenging problem that has gain increasing attention over the last few years. While several methods have been proposed to explain network predictions, there have been only a few attempts to compare them from a theoretical perspective. What is more, no exhaustive empirical comparison has been performed in the past. In this work, we analyze four gradient-based attribution methods and formally prove conditions of equivalence and approximation between them. By reformulating two of these methods, we construct a unified framework which enables a direct comparison, as well as an easier implementation. Finally, we propose a novel evaluation metric, called Sensitivity-n and test the gradient-based attribution methods alongside with a simple perturbation-based attribution method on several datasets in the domains of image and text classification, using various network architectures.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/eragon/Zotero/storage/N82WWPLB/Ancona et al. - 2018 - Towards better understanding of gradient-based att.pdf}
}

@article{andrienkoVisualAnalyticsHumanCentered2022,
  title = {Visual {{Analytics}} for {{Human-Centered Machine Learning}}},
  author = {Andrienko, Natalia and Andrienko, Gennady and Adilova, Linara and Wrobel, Stefan},
  date = {2022-01-01},
  journaltitle = {IEEE Computer Graphics and Applications},
  shortjournal = {IEEE Comput. Grap. Appl.},
  volume = {42},
  number = {1},
  pages = {123--133},
  issn = {0272-1716, 1558-1756},
  doi = {10.1109/MCG.2021.3130314},
  url = {https://ieeexplore.ieee.org/document/9693359/},
  urldate = {2023-02-13},
  abstract = {We introduce a new research area in Visual Analytics (VA) aiming to bridge existing gaps between methods of interactive Machine Learning (ML) and eXplainable Artificial Intelligence (XAI), on one side, and human minds, on the other side. The gaps are, first, a conceptual mismatch between ML/XAI outputs and human mental models and ways of reasoning, second, a mismatch between the information quantity and level of detail and human capabilities to perceive and understand. A grand challenge is to adapt ML and XAI to human goals, concepts, values, and ways of thinking. Complementing the current efforts in XAI towards solving this challenge, VA can contribute by exploiting the potential of visualization as an effective way of communicating information to humans and a strong trigger of human abstractive perception and thinking. We propose a cross-disciplinary research framework and formulate research directions for VA.},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/4D2793WK/Andrienko et al. - 2022 - Visual Analytics for Human-Centered Machine Learni.pdf}
}

@inproceedings{arriagaDifficultyEstimationAction2023,
  title = {Difficulty {{Estimation With Action Scores}} for {{Computer Vision Tasks}}},
  author = {Arriaga, Octavio and Palacio, Sebastian and Valdenegro-Toro, Matias},
  date = {2023},
  pages = {245--253},
  url = {https://openaccess.thecvf.com/content/CVPR2023W/LatinX/html/Arriaga_Difficulty_Estimation_With_Action_Scores_for_Computer_Vision_Tasks_CVPRW_2023_paper.html},
  urldate = {2023-06-17},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/9CET2DEL/Arriaga et al. - 2023 - Difficulty Estimation With Action Scores for Compu.pdf}
}

@online{arrietaExplainableArtificialIntelligence2019,
  title = {Explainable {{Artificial Intelligence}} ({{XAI}}): {{Concepts}}, {{Taxonomies}}, {{Opportunities}} and {{Challenges}} toward {{Responsible AI}}},
  shorttitle = {Explainable {{Artificial Intelligence}} ({{XAI}})},
  author = {Arrieta, Alejandro Barredo and Díaz-Rodríguez, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and García, Salvador and Gil-López, Sergio and Molina, Daniel and Benjamins, Richard and Chatila, Raja and Herrera, Francisco},
  date = {2019-12-26},
  eprint = {1910.10045},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1910.10045},
  url = {http://arxiv.org/abs/1910.10045},
  urldate = {2022-11-24},
  abstract = {In the last years, Artificial Intelligence (AI) has achieved a notable momentum that may deliver the best of expectations over many application sectors across the field. For this to occur, the entire community stands in front of the barrier of explainability, an inherent problem of AI techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI. Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is acknowledged as a crucial feature for the practical deployment of AI models. This overview examines the existing literature in the field of XAI, including a prospect toward what is yet to be reached. We summarize previous efforts to define explainability in Machine Learning, establishing a novel definition that covers prior conceptual propositions with a major focus on the audience for which explainability is sought. We then propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at Deep Learning methods for which a second taxonomy is built. This literature analysis serves as the background for a series of challenges faced by XAI, such as the crossroads between data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to XAI with a reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/eragon/Zotero/storage/24ZS9T3Q/Arrieta et al. - 2019 - Explainable Artificial Intelligence (XAI) Concept.pdf;/Users/eragon/Zotero/storage/DPZTMKX5/Arrieta et al. - 2019 - Explainable Artificial Intelligence (XAI) Concept.pdf}
}

@article{aylingPuttingAIEthics2022,
  title = {Putting {{AI}} Ethics to Work: Are the Tools Fit for Purpose?},
  shorttitle = {Putting {{AI}} Ethics to Work},
  author = {Ayling, Jacqui and Chapman, Adriane},
  date = {2022-08-01},
  journaltitle = {AI and Ethics},
  shortjournal = {AI Ethics},
  volume = {2},
  number = {3},
  pages = {405--429},
  issn = {2730-5961},
  doi = {10.1007/s43681-021-00084-x},
  url = {https://doi.org/10.1007/s43681-021-00084-x},
  urldate = {2023-01-17},
  abstract = {Bias, unfairness and lack of transparency and accountability in Artificial Intelligence (AI) systems, and the potential for the misuse of predictive models for decision-making have raised concerns about the ethical impact and unintended consequences of new technologies for society across every sector where data-driven innovation is taking place. This paper reviews the landscape of suggested ethical frameworks with a focus on those which go beyond high-level statements of principles and offer practical tools for application of these principles in the production and deployment of systems. This work provides an assessment of these practical frameworks with the lens of known best practices for impact assessment and audit of technology. We review other historical uses of risk assessments and audits and create a typology that allows us to compare current AI ethics tools to Best Practices found in previous methodologies from technology, environment, privacy, finance and engineering. We analyse current AI ethics tools and their support for diverse stakeholders and components of the AI development and deployment lifecycle as well as the types of tools used to facilitate use. From this, we identify gaps in current AI ethics tools in auditing and risk assessment that should be considered going forward.},
  langid = {english},
  keywords = {AI,Audit,Ethics,Impact assessment},
  file = {/Users/eragon/Zotero/storage/3C6HKMTW/Ayling and Chapman - 2022 - Putting AI ethics to work are the tools fit for p.pdf}
}

@online{babikerIntroductionDeepVisual2018,
  title = {An {{Introduction}} to {{Deep Visual Explanation}}},
  author = {Babiker, Housam Khalifa Bashier and Goebel, Randy},
  date = {2018-03-15},
  eprint = {1711.09482},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1711.09482},
  urldate = {2023-02-20},
  abstract = {The practical impact of deep learning on complex supervised learning problems has been significant, so much so that almost every Artificial Intelligence problem, or at least a portion thereof, has been somehow recast as a deep learning problem. The applications appeal is significant, but this appeal is increasingly challenged by what some call the challenge of explainability, or more generally the more traditional challenge of debuggability: if the outcomes of a deep learning process produce unexpected results (e.g., less than expected performance of a classifier), then there is little available in the way of theories or tools to help investigate the potential causes of such unexpected behavior, especially when this behavior could impact people's lives. We describe a preliminary framework to help address this issue, which we call "deep visual explanation" (DVE). "Deep," because it is the development and performance of deep neural network models that we want to understand. "Visual," because we believe that the most rapid insight into a complex multi-dimensional model is provided by appropriate visualization techniques, and "Explanation," because in the spectrum from instrumentation by inserting print statements to the abductive inference of explanatory hypotheses, we believe that the key to understanding deep learning relies on the identification and exposure of hypotheses about the performance behavior of a learned deep model. In the exposition of our preliminary framework, we use relatively straightforward image classification examples and a variety of choices on initial configuration of a deep model building scenario. By careful but not complicated instrumentation, we expose classification outcomes of deep models using visualization, and also show initial results for one potential application of interpretability.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/eragon/Zotero/storage/6FKGN7MN/Babiker and Goebel - 2018 - An Introduction to Deep Visual Explanation.pdf}
}

@online{baiAreTransformersMore2021,
  title = {Are {{Transformers More Robust Than CNNs}}?},
  author = {Bai, Yutong and Mei, Jieru and Yuille, Alan and Xie, Cihang},
  date = {2021-11-09},
  eprint = {2111.05464},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2111.05464},
  url = {http://arxiv.org/abs/2111.05464},
  urldate = {2023-05-07},
  abstract = {Transformer emerges as a powerful tool for visual recognition. In addition to demonstrating competitive performance on a broad range of visual benchmarks, recent works also argue that Transformers are much more robust than Convolutions Neural Networks (CNNs). Nonetheless, surprisingly, we find these conclusions are drawn from unfair experimental settings, where Transformers and CNNs are compared at different scales and are applied with distinct training frameworks. In this paper, we aim to provide the first fair \& in-depth comparisons between Transformers and CNNs, focusing on robustness evaluations. With our unified training setup, we first challenge the previous belief that Transformers outshine CNNs when measuring adversarial robustness. More surprisingly, we find CNNs can easily be as robust as Transformers on defending against adversarial attacks, if they properly adopt Transformers' training recipes. While regarding generalization on out-of-distribution samples, we show pre-training on (external) large-scale datasets is not a fundamental request for enabling Transformers to achieve better performance than CNNs. Moreover, our ablations suggest such stronger generalization is largely benefited by the Transformer's self-attention-like architectures per se, rather than by other training setups. We hope this work can help the community better understand and benchmark the robustness of Transformers and CNNs. The code and models are publicly available at https://github.com/ytongbai/ViTs-vs-CNNs.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/eragon/Zotero/storage/TLGEDGGF/Bai et al. - 2021 - Are Transformers More Robust Than CNNs.pdf}
}

@inproceedings{bakryUntanglingObjectviewManifold2014,
  title = {Untangling Object-View Manifold for Multiview Recognition and Pose Estimation},
  booktitle = {European Conference on Computer Vision},
  author = {Bakry, Amr and Elgammal, Ahmed},
  date = {2014},
  pages = {434--449},
  publisher = {{Springer}}
}

@article{banymuhammadEigenCAMVisualExplanations2021,
  title = {Eigen-{{CAM}}: {{Visual Explanations}} for {{Deep Convolutional Neural Networks}}},
  shorttitle = {Eigen-{{CAM}}},
  author = {Bany Muhammad, Mohammed and Yeasin, Mohammed},
  date = {2021-01-20},
  journaltitle = {SN Computer Science},
  shortjournal = {SN COMPUT. SCI.},
  volume = {2},
  number = {1},
  pages = {47},
  issn = {2661-8907},
  doi = {10.1007/s42979-021-00449-3},
  url = {https://doi.org/10.1007/s42979-021-00449-3},
  urldate = {2023-05-31},
  abstract = {The adoption of deep convolutional neural networks (CNN) is growing exponentially in wide varieties of applications due to exceptional performance that equals to or is better than classical machine learning as well as a human. However, such models are difficult to interpret, susceptible to overfit, and hard to decode failure. An increasing body of literature, such as class activation map (CAM), focused on understanding what representations or features a model learned from the data. This paper presents novel Eigen-CAM to enhance explanations of CNN predictions by visualizing principal components of learned representations from convolutional layers. The Eigen-CAM is intuitive, easy to use, computationally efficient, and does not require correct classification by the model. Eigen-CAM can work with all CNN models without the need to modify layers or retrain models. For the task of generating a visual explanation of CNN predictions, compared to state-of-the-art methods, Eigen-CAM is more consistent, class discriminative, and robust against classification errors made by dense layers. Empirical analyses and comparison with the best state-of-the-art methods show up to 12\% improvement in weakly-supervised object localization, an average of 13\% improvement in weakly-supervised segmentation, and at least 15\% improvement in generic object proposal.},
  langid = {english},
  keywords = {Class activation maps,Explainable AI,Salient features,Visual explanation of CNN,Weakly supervised localization},
  file = {/Users/eragon/Zotero/storage/4MAVEP3V/Bany Muhammad and Yeasin - 2021 - Eigen-CAM Visual Explanations for Deep Convolutio.pdf}
}

@online{bastingsElephantInterpretabilityRoom2020,
  title = {The Elephant in the Interpretability Room: {{Why}} Use Attention as Explanation When We Have Saliency Methods?},
  shorttitle = {The Elephant in the Interpretability Room},
  author = {Bastings, Jasmijn and Filippova, Katja},
  date = {2020-10-12},
  eprint = {2010.05607},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2010.05607},
  urldate = {2023-05-18},
  abstract = {There is a recent surge of interest in using attention as explanation of model predictions, with mixed evidence on whether attention can be used as such. While attention conveniently gives us one weight per input token and is easily extracted, it is often unclear toward what goal it is used as explanation. We find that often that goal, whether explicitly stated or not, is to find out what input tokens are the most relevant to a prediction, and that the implied user for the explanation is a model developer. For this goal and user, we argue that input saliency methods are better suited, and that there are no compelling reasons to use attention, despite the coincidence that it provides a weight for each input. With this position paper, we hope to shift some of the recent focus on attention to saliency methods, and for authors to clearly state the goal and user for their explanations.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/eragon/Zotero/storage/L9VP95TE/Bastings and Filippova - 2020 - The elephant in the interpretability room Why use.pdf}
}

@online{bauNetworkDissectionQuantifying2017,
  title = {Network {{Dissection}}: {{Quantifying Interpretability}} of {{Deep Visual Representations}}},
  shorttitle = {Network {{Dissection}}},
  author = {Bau, David and Zhou, Bolei and Khosla, Aditya and Oliva, Aude and Torralba, Antonio},
  date = {2017-04-19},
  eprint = {1704.05796},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1704.05796},
  url = {http://arxiv.org/abs/1704.05796},
  urldate = {2022-11-24},
  abstract = {We propose a general framework called Network Dissection for quantifying the interpretability of latent representations of CNNs by evaluating the alignment between individual hidden units and a set of semantic concepts. Given any CNN model, the proposed method draws on a broad data set of visual concepts to score the semantics of hidden units at each intermediate convolutional layer. The units with semantics are given labels across a range of objects, parts, scenes, textures, materials, and colors. We use the proposed method to test the hypothesis that interpretability of units is equivalent to random linear combinations of units, then we apply our method to compare the latent representations of various networks when trained to solve different supervised and self-supervised training tasks. We further analyze the effect of training iterations, compare networks trained with different initializations, examine the impact of network depth and width, and measure the effect of dropout and batch normalization on the interpretability of deep visual representations. We demonstrate that the proposed method can shed light on characteristics of CNN models and training methods that go beyond measurements of their discriminative power.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,I.2.10},
  file = {/Users/eragon/Zotero/storage/EDW6KRBC/Bau et al. - 2017 - Network Dissection Quantifying Interpretability o.pdf}
}

@inproceedings{baySurfSpeededRobust2006,
  title = {Surf: {{Speeded}} up Robust Features},
  booktitle = {European Conference on Computer Vision},
  author = {Bay, Herbert and Tuytelaars, Tinne and Van Gool, Luc},
  date = {2006},
  pages = {404--417},
  publisher = {{Springer}}
}

@inproceedings{belloAttentionAugmentedConvolutional2019,
  title = {Attention {{Augmented Convolutional Networks}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Bello, Irwan and Zoph, Barret and Le, Quoc and Vaswani, Ashish and Shlens, Jonathon},
  date = {2019-10},
  pages = {3285--3294},
  publisher = {{IEEE}},
  location = {{Seoul, Korea (South)}},
  doi = {10.1109/ICCV.2019.00338},
  url = {https://ieeexplore.ieee.org/document/9010285/},
  urldate = {2022-10-03},
  abstract = {Convolutional networks have been the paradigm of choice in many computer vision applications. The convolution operation however has a significant weakness in that it only operates on a local neighborhood, thus missing global information. Self-attention, on the other hand, has emerged as a recent advance to capture long range interactions, but has mostly been applied to sequence modeling and generative modeling tasks. In this paper, we consider the use of self-attention for discriminative visual tasks as an alternative to convolutions. We introduce a novel two-dimensional relative self-attention mechanism that proves competitive in replacing convolutions as a stand-alone computational primitive for image classification. We find in control experiments that the best results are obtained when combining both convolutions and self-attention. We therefore propose to augment convolutional operators with this self-attention mechanism by concatenating convolutional feature maps with a set of feature maps produced via self-attention. Extensive experiments show that Attention Augmentation leads to consistent improvements in image classification on ImageNet and object detection on COCO across many different models and scales, including ResNets and a stateof-the art mobile constrained network, while keeping the number of parameters similar. In particular, our method achieves a 1.3\% top-1 accuracy improvement on ImageNet classification over a ResNet50 baseline and outperforms other attention mechanisms for images such as Squeezeand-Excitation [17]. It also achieves an improvement of 1.4 mAP in COCO Object Detection on top of a RetinaNet baseline.},
  eventtitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-72814-803-8},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/WENTJ9EM/Bello et al. - 2019 - Attention Augmented Convolutional Networks.pdf}
}

@inproceedings{birhaneForgottenMarginsAI2022,
  title = {The {{Forgotten Margins}} of {{AI Ethics}}},
  booktitle = {2022 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Birhane, Abeba and Ruane, Elayne and Laurent, Thomas and S. Brown, Matthew and Flowers, Johnathan and Ventresque, Anthony and L. Dancy, Christopher},
  date = {2022-06-21},
  pages = {948--958},
  publisher = {{ACM}},
  location = {{Seoul Republic of Korea}},
  doi = {10.1145/3531146.3533157},
  url = {https://dl.acm.org/doi/10.1145/3531146.3533157},
  urldate = {2023-01-17},
  eventtitle = {{{FAccT}} '22: 2022 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  isbn = {978-1-4503-9352-2},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/BMVP5KFZ/Birhane et al. - 2022 - The Forgotten Margins of AI Ethics.pdf}
}

@inproceedings{birhaneLargeImageDatasets2021,
  title = {Large Image Datasets: {{A}} Pyrrhic Win for Computer Vision?},
  shorttitle = {Large Image Datasets},
  booktitle = {2021 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Birhane, Abeba and Prabhu, Vinay Uday},
  date = {2021-01},
  pages = {1536--1546},
  issn = {2642-9381},
  doi = {10.1109/WACV48630.2021.00158},
  abstract = {In this paper we investigate problematic practices and consequences of large scale vision datasets (LSVDs). We examine broad issues such as the question of consent and justice as well as specific concerns such as the inclusion of verifiably pornographic images in datasets. Taking the ImageNet-ILSVRC-2012 dataset as an example, we perform a cross-sectional model-based quantitative census covering factors such as age, gender, NSFW content scoring, class- wise accuracy, human-cardinality-analysis, and the semanticity of the image class information in order to statistically investigate the extent and subtleties of ethical transgressions. We then use the census to help hand-curate a look-up-table of images in the ImageNet-ILSVRC-2012 dataset that fall into the categories of verifiably pornographic: shot in a non-consensual setting (up-skirt), beach voyeuristic, and exposed private parts. We survey the landscape of harm and threats both the society at large and individuals face due to uncritical and ill-considered dataset curation practices. We then propose possible courses of correction and critique their pros and cons. We have duly open-sourced all of the code and the census meta-datasets generated in this endeavor for the computer vision community to build on. By unveiling the severity of the threats, our hope is to motivate the constitution of mandatory Institutional Review Boards (IRB) for large scale dataset curation.},
  eventtitle = {2021 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  keywords = {Computer vision,Conferences,Faces,IEEE Constitution},
  file = {/Users/eragon/Zotero/storage/NXB5CRPQ/Birhane and Prabhu - 2021 - Large image datasets A pyrrhic win for computer v.pdf}
}

@unpublished{bommasaniOpportunitiesRisksFoundation2021,
  title = {On the {{Opportunities}} and {{Risks}} of {{Foundation Models}}},
  author = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and family=Arx, given=Sydney, prefix=von, useprefix=true and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and Fei-Fei, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Koh, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and Ré, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tramèr, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
  date = {2021-08-18},
  eprint = {2108.07258},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2108.07258},
  url = {http://arxiv.org/abs/2108.07258},
  urldate = {2022-05-25},
  abstract = {AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/Users/eragon/Zotero/storage/WHXXTPUJ/Bommasani et al. - 2021 - On the Opportunities and Risks of Foundation Model.pdf;/Users/eragon/Zotero/storage/XHF446PP/2108.html}
}

@article{bonaventuraSurveyViewpointSelection2018,
  title = {A {{Survey}} of {{Viewpoint Selection Methods}} for {{Polygonal Models}}},
  author = {Bonaventura, Xavier and Feixas, Miquel and Sbert, Mateu and Chuang, Lewis and Wallraven, Christian},
  date = {2018-05-16},
  journaltitle = {Entropy},
  shortjournal = {Entropy},
  volume = {20},
  number = {5},
  pages = {370},
  issn = {1099-4300},
  doi = {10.3390/e20050370},
  url = {http://www.mdpi.com/1099-4300/20/5/370},
  urldate = {2022-05-14},
  abstract = {Viewpoint selection has been an emerging area in computer graphics for some years, and it is now getting maturity with applications in fields such as scene navigation, scientific visualization, object recognition, mesh simplification, and camera placement. In this survey, we review and compare twenty-two measures to select good views of a polygonal 3D model, classify them using an extension of the categories defined by Secord et al., and evaluate them against the Dutagaci et al. benchmark. Eleven of these measures have not been reviewed in previous surveys. Three out of the five short-listed best viewpoint measures are directly related to information. We also present in which fields the different viewpoint measures have been applied. Finally, we provide a publicly available framework where all the viewpoint selection measures are implemented and can be compared against each other.},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/LC7Y8LTY/Bonaventura et al. - 2018 - A Survey of Viewpoint Selection Methods for Polygo.pdf}
}

@article{borstStepbystepTutorialUsing2017,
  title = {A Step-by-Step Tutorial on Using the Cognitive Architecture {{ACT-R}} in Combination with {{fMRI}} Data},
  author = {Borst, Jelmer P. and Anderson, John R.},
  date = {2017-02},
  journaltitle = {Journal of Mathematical Psychology},
  shortjournal = {Journal of Mathematical Psychology},
  volume = {76},
  pages = {94--103},
  issn = {00222496},
  doi = {10.1016/j.jmp.2016.05.005},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0022249616300293},
  urldate = {2022-05-27},
  abstract = {The cognitive architecture ACT-R is at the same time a psychological theory and a modeling framework for constructing cognitive models that adhere to the principles of the theory. ACT-R can be used in combination with fMRI data in two different ways: (1) fMRI data can be used to evaluate and constrain models in ACT-R by means of predefined Region-of-Interest (ROI) analysis, and (2) predictions from ACT-R models can be used to locate neural correlates of model processes and representations by means of modelbased fMRI analysis. In this paper we provide a step-by-step tutorial on both approaches. Note that this tutorial neither teaches the ACT-R theory in any detail, nor fMRI analysis, but explains how ACT-R can be used in combination with fMRI data. To this end, we provide all data and computer code necessary to run the ACT-R model, carry out the analyses, and recreate the figures in the paper. As an example dataset we use a relatively simple algebra task. In the first section, we develop an ACT-R model of this task and fit it to behavioral data. In the second section, we apply a predefined ROI-analysis to evaluate the model using fMRI data. In the third section, we use model-based fMRI analysis to locate the following processes in the brain: retrieval of mathematical facts from memory, working memory updates, motor responses, and visually encoding the problems. After working through this tutorial, the reader will have learned what can be achieved with the two different analysis methods and how they are conducted; the example code can then be adapted to a new dataset.},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/87XGBJYN/Borst and Anderson - 2017 - A step-by-step tutorial on using the cognitive arc.pdf}
}

@article{brandoneChildrenDevelopingIntuitions2015,
  title = {Children's {{Developing Intuitions About}} the {{Truth Conditions}} and {{Implications}} of {{Novel Generics Versus Quantified Statements}}},
  author = {Brandone, Amanda C. and Gelman, Susan A. and Hedglen, Jenna},
  date = {2015-05},
  journaltitle = {Cognitive Science},
  shortjournal = {Cogn Sci},
  volume = {39},
  number = {4},
  pages = {711--738},
  issn = {03640213},
  doi = {10.1111/cogs.12176},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/cogs.12176},
  urldate = {2022-10-24},
  abstract = {Generic statements express generalizations about categories and present a unique semantic profile that is distinct from quantified statements. This paper reports two studies examining the development of children’s intuitions about the semantics of generics and how they differ from statements quantified by all, most, and some. Results reveal that, like adults, preschoolers (a) recognize that generics have flexible truth conditions and are capable of representing a wide range of prevalence levels; and (b) interpret novel generics as having near-universal prevalence implications. Results further show that by age 4, children are beginning to differentiate the meaning of generics and quantified statements; however, even 7- to 11-year-olds are not adultlike in their intuitions about the meaning of most-quantified statements. Overall, these studies suggest that by preschool, children interpret generics in much the same way that adults do; however, mastery of the semantics of quantified statements follows a more protracted course.},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/VLNKEHFJ/Brandone et al. - 2015 - Children's Developing Intuitions About the Truth C.pdf}
}

@unpublished{brockGenerativeDiscriminativeVoxel2016,
  title = {Generative and Discriminative Voxel Modeling with Convolutional Neural Networks},
  author = {Brock, Andrew and Lim, Theodore and Ritchie, James M and Weston, Nick},
  date = {2016},
  eprint = {1608.04236},
  eprinttype = {arxiv}
}

@online{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  date = {2020-07-22},
  eprint = {2005.14165},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2005.14165},
  url = {http://arxiv.org/abs/2005.14165},
  urldate = {2022-10-21},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/eragon/Zotero/storage/LZZ4A45Z/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf}
}

@online{brownUnionManifoldsHypothesis2022,
  title = {The {{Union}} of {{Manifolds Hypothesis}} and Its {{Implications}} for {{Deep Generative Modelling}}},
  author = {Brown, Bradley C. A. and Caterini, Anthony L. and Ross, Brendan Leigh and Cresswell, Jesse C. and Loaiza-Ganem, Gabriel},
  date = {2022-07-06},
  eprint = {2207.02862},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2207.02862},
  urldate = {2022-11-15},
  abstract = {Deep learning has had tremendous success at learning low-dimensional representations of high-dimensional data. This success would be impossible if there was no hidden low-dimensional structure in data of interest; this existence is posited by the manifold hypothesis, which states that the data lies on an unknown manifold of low intrinsic dimension. In this paper, we argue that this hypothesis does not properly capture the low-dimensional structure typically present in data. Assuming the data lies on a single manifold implies intrinsic dimension is identical across the entire data space, and does not allow for subregions of this space to have a different number of factors of variation. To address this deficiency, we put forth the union of manifolds hypothesis, which accommodates the existence of non-constant intrinsic dimensions. We empirically verify this hypothesis on commonly-used image datasets, finding that indeed, intrinsic dimension should be allowed to vary. We also show that classes with higher intrinsic dimensions are harder to classify, and how this insight can be used to improve classification accuracy. We then turn our attention to the impact of this hypothesis in the context of deep generative models (DGMs). Most current DGMs struggle to model datasets with several connected components and/or varying intrinsic dimensions. To tackle these shortcomings, we propose clustered DGMs, where we first cluster the data and then train a DGM on each cluster. We show that clustered DGMs can model multiple connected components with different intrinsic dimensions, and empirically outperform their non-clustered counterparts without increasing computational requirements.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/eragon/Zotero/storage/MJFIRSZ4/Brown et al. - 2022 - The Union of Manifolds Hypothesis and its Implicat.pdf}
}

@online{brundageTrustworthyAIDevelopment2020,
  title = {Toward {{Trustworthy AI Development}}: {{Mechanisms}} for {{Supporting Verifiable Claims}}},
  shorttitle = {Toward {{Trustworthy AI Development}}},
  author = {Brundage, Miles and Avin, Shahar and Wang, Jasmine and Belfield, Haydn and Krueger, Gretchen and Hadfield, Gillian and Khlaaf, Heidy and Yang, Jingying and Toner, Helen and Fong, Ruth and Maharaj, Tegan and Koh, Pang Wei and Hooker, Sara and Leung, Jade and Trask, Andrew and Bluemke, Emma and Lebensold, Jonathan and O'Keefe, Cullen and Koren, Mark and Ryffel, Théo and Rubinovitz, J. B. and Besiroglu, Tamay and Carugati, Federica and Clark, Jack and Eckersley, Peter and family=Haas, given=Sarah, prefix=de, useprefix=true and Johnson, Maritza and Laurie, Ben and Ingerman, Alex and Krawczuk, Igor and Askell, Amanda and Cammarota, Rosario and Lohn, Andrew and Krueger, David and Stix, Charlotte and Henderson, Peter and Graham, Logan and Prunkl, Carina and Martin, Bianca and Seger, Elizabeth and Zilberman, Noa and {hÉigeartaigh}, Seán Ó and Kroeger, Frens and Sastry, Girish and Kagan, Rebecca and Weller, Adrian and Tse, Brian and Barnes, Elizabeth and Dafoe, Allan and Scharre, Paul and Herbert-Voss, Ariel and Rasser, Martijn and Sodhani, Shagun and Flynn, Carrick and Gilbert, Thomas Krendl and Dyer, Lisa and Khan, Saif and Bengio, Yoshua and Anderljung, Markus},
  date = {2020-04-20},
  eprint = {2004.07213},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2004.07213},
  url = {http://arxiv.org/abs/2004.07213},
  urldate = {2023-05-25},
  abstract = {With the recent wave of progress in artificial intelligence (AI) has come a growing awareness of the large-scale impacts of AI systems, and recognition that existing regulations and norms in industry and academia are insufficient to ensure responsible AI development. In order for AI developers to earn trust from system users, customers, civil society, governments, and other stakeholders that they are building AI responsibly, they will need to make verifiable claims to which they can be held accountable. Those outside of a given organization also need effective means of scrutinizing such claims. This report suggests various steps that different stakeholders can take to improve the verifiability of claims made about AI systems and their associated development processes, with a focus on providing evidence about the safety, security, fairness, and privacy protection of AI systems. We analyze ten mechanisms for this purpose--spanning institutions, software, and hardware--and make recommendations aimed at implementing, exploring, or improving those mechanisms.},
  pubstate = {preprint},
  keywords = {Computer Science - Computers and Society},
  file = {/Users/eragon/Zotero/storage/3CWYAIZX/Brundage et al. - 2020 - Toward Trustworthy AI Development Mechanisms for .pdf}
}

@article{buhrmesterAnalysisExplainersBlack2021,
  title = {Analysis of {{Explainers}} of {{Black Box Deep Neural Networks}} for {{Computer Vision}}: {{A Survey}}},
  shorttitle = {Analysis of {{Explainers}} of {{Black Box Deep Neural Networks}} for {{Computer Vision}}},
  author = {Buhrmester, Vanessa and Münch, David and Arens, Michael},
  date = {2021-12},
  journaltitle = {Machine Learning and Knowledge Extraction},
  volume = {3},
  number = {4},
  pages = {966--989},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2504-4990},
  doi = {10.3390/make3040048},
  url = {https://www.mdpi.com/2504-4990/3/4/48},
  urldate = {2023-02-20},
  abstract = {Deep Learning is a state-of-the-art technique to make inference on extensive or complex data. As a black box model due to their multilayer nonlinear structure, Deep Neural Networks are often criticized as being non-transparent and their predictions not traceable by humans. Furthermore, the models learn from artificially generated datasets, which often do not reflect reality. By basing decision-making algorithms on Deep Neural Networks, prejudice and unfairness may be promoted unknowingly due to a lack of transparency. Hence, several so-called explanators, or explainers, have been developed. Explainers try to give insight into the inner structure of machine learning black boxes by analyzing the connection between the input and output. In this survey, we present the mechanisms and properties of explaining systems for Deep Neural Networks for Computer Vision tasks. We give a comprehensive overview about the taxonomy of related studies and compare several survey papers that deal with explainability in general. We work out the drawbacks and gaps and summarize further research ideas.},
  issue = {4},
  langid = {english},
  keywords = {black box,Deep Neural Network,ethics,explainable AI,explainer,explanator,interpretability,trust},
  file = {/Users/eragon/Zotero/storage/CINU3FEY/Buhrmester et al. - 2021 - Analysis of Explainers of Black Box Deep Neural Ne.pdf}
}

@article{cannyComputationalApproachEdge1986,
  title = {A Computational Approach to Edge Detection},
  author = {Canny, John},
  date = {1986},
  journaltitle = {IEEE Transactions on pattern analysis and machine intelligence},
  number = {6},
  pages = {679--698},
  publisher = {{Ieee}}
}

@inproceedings{caoReMixImagetoImageTranslation2021,
  title = {{{ReMix}}: {{Towards Image-to-Image Translation With Limited Data}}},
  shorttitle = {{{ReMix}}},
  author = {Cao, Jie and Hou, Luanxuan and Yang, Ming-Hsuan and He, Ran and Sun, Zhenan},
  date = {2021},
  pages = {15018--15027},
  url = {https://openaccess.thecvf.com/content/CVPR2021/html/Cao_ReMix_Towards_Image-to-Image_Translation_With_Limited_Data_CVPR_2021_paper.html},
  urldate = {2023-03-31},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/RR2W2HL8/Cao et al. - 2021 - ReMix Towards Image-to-Image Translation With Lim.pdf}
}

@article{caytonAlgorithmsManifoldLearning,
  title = {Algorithms for Manifold Learning},
  author = {Cayton, Lawrence},
  pages = {17},
  abstract = {Manifold learning is a popular recent approach to nonlinear dimensionality reduction. Algorithms for this task are based on the idea that the dimensionality of many data sets is only artificially high; though each data point consists of perhaps thousands of features, it may be described as a function of only a few underlying parameters. That is, the data points are actually samples from a low-dimensional manifold that is embedded in a high-dimensional space. Manifold learning algorithms attempt to uncover these parameters in order to find a low-dimensional representation of the data. In this paper, we discuss the motivation, background, and algorithms proposed for manifold learning. Isomap, Locally Linear Embedding, Laplacian Eigenmaps, Semidefinite Embedding, and a host of variants of these algorithms are examined.},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/ZUWTU4DG/Cayton - Algorithms for manifold learning.pdf}
}

@inproceedings{chakrabortyGeneralizingAdversarialExplanations2022,
  title = {Generalizing {{Adversarial Explanations}} with {{Grad-CAM}}},
  booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Chakraborty, Tanmay and Trehan, Utkarsh and Mallat, Khawla and Dugelay, Jean-Luc},
  date = {2022-06},
  pages = {186--192},
  publisher = {{IEEE}},
  location = {{New Orleans, LA, USA}},
  doi = {10.1109/CVPRW56347.2022.00031},
  url = {https://ieeexplore.ieee.org/document/9857321/},
  urldate = {2022-10-03},
  abstract = {Gradient-weighted Class Activation Mapping (GradCAM), is an example-based explanation method that provides a gradient activation heat map as an explanation for Convolution Neural Network (CNN) models. The drawback of this method is that it cannot be used to generalize CNN behaviour. In this paper, we present a novel method that extends Grad-CAM from example-based explanations to a method for explaining global model behaviour. This is achieved by introducing two new metrics, (i) Mean Observed Dissimilarity (MOD) and (ii) Variation in Dissimilarity (VID), for model generalization. These metrics are computed by comparing a Normalized Inverted Structural Similarity Index (NISSIM) metric of the Grad-CAM generated heatmap for samples from the original test set and samples from the adversarial test set. For our experiment, we study adversarial attacks on deep models such as VGG16, ResNet50, and ResNet101, and wide models such as InceptionNetv3 and XceptionNet using Fast Gradient Sign Method (FGSM). We then compute the metrics MOD and VID for the automatic face recognition (AFR) use case with the VGGFace2 dataset. We observe a consistent shift in the region highlighted in the Grad-CAM heatmap, reflecting its participation to the decision making, across all models under adversarial attacks. The proposed method can be used to understand adversarial attacks and explain the behaviour of black box CNN models for image analysis.},
  eventtitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  isbn = {978-1-66548-739-9},
  langid = {english},
  keywords = {read},
  file = {/Users/eragon/Zotero/storage/6G98B28D/Chakraborty et al. - 2022 - Generalizing Adversarial Explanations with Grad-CA.pdf}
}

@inproceedings{chattopadhayGradCAMGeneralizedGradientBased2018,
  title = {Grad-{{CAM}}++: {{Generalized Gradient-Based Visual Explanations}} for {{Deep Convolutional Networks}}},
  shorttitle = {Grad-{{CAM}}++},
  booktitle = {2018 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Chattopadhay, Aditya and Sarkar, Anirban and Howlader, Prantik and Balasubramanian, Vineeth N},
  date = {2018-03},
  pages = {839--847},
  publisher = {{IEEE}},
  location = {{Lake Tahoe, NV}},
  doi = {10.1109/WACV.2018.00097},
  url = {https://ieeexplore.ieee.org/document/8354201/},
  urldate = {2023-02-20},
  eventtitle = {2018 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  isbn = {978-1-5386-4886-5},
  file = {/Users/eragon/Zotero/storage/CQ5ZLPUW/Chattopadhay et al. - 2018 - Grad-CAM++ Generalized Gradient-Based Visual Expl.pdf}
}

@inproceedings{chattopadhyayGradCAMImprovedVisual2018,
  title = {Grad-{{CAM}}++: {{Improved Visual Explanations}} for {{Deep Convolutional Networks}}},
  shorttitle = {Grad-{{CAM}}++},
  booktitle = {2018 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Chattopadhyay, Aditya and Sarkar, Anirban and Howlader, Prantik and Balasubramanian, Vineeth N.},
  date = {2018-03},
  eprint = {1710.11063},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {839--847},
  doi = {10.1109/WACV.2018.00097},
  url = {http://arxiv.org/abs/1710.11063},
  urldate = {2022-10-03},
  abstract = {Over the last decade, Convolutional Neural Network (CNN) models have been highly successful in solving complex vision problems. However, these deep models are perceived as "black box" methods considering the lack of understanding of their internal functioning. There has been a significant recent interest in developing explainable deep learning models, and this paper is an effort in this direction. Building on a recently proposed method called Grad-CAM, we propose a generalized method called Grad-CAM++ that can provide better visual explanations of CNN model predictions, in terms of better object localization as well as explaining occurrences of multiple object instances in a single image, when compared to state-of-the-art. We provide a mathematical derivation for the proposed method, which uses a weighted combination of the positive partial derivatives of the last convolutional layer feature maps with respect to a specific class score as weights to generate a visual explanation for the corresponding class label. Our extensive experiments and evaluations, both subjective and objective, on standard datasets showed that Grad-CAM++ provides promising human-interpretable visual explanations for a given CNN architecture across multiple tasks including classification, image caption generation and 3D action recognition; as well as in new settings such as knowledge distillation.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/eragon/Zotero/storage/V2RWR75J/Chattopadhyay et al. - 2018 - Grad-CAM++ Improved Visual Explanations for Deep .pdf}
}

@inproceedings{chattopadhyayGradCAMImprovedVisual2018a,
  title = {Grad-{{CAM}}++: {{Improved Visual Explanations}} for {{Deep Convolutional Networks}}},
  shorttitle = {Grad-{{CAM}}++},
  booktitle = {2018 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Chattopadhyay, Aditya and Sarkar, Anirban and Howlader, Prantik and Balasubramanian, Vineeth N.},
  date = {2018-03},
  eprint = {1710.11063},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {839--847},
  doi = {10.1109/WACV.2018.00097},
  url = {http://arxiv.org/abs/1710.11063},
  urldate = {2023-02-20},
  abstract = {Over the last decade, Convolutional Neural Network (CNN) models have been highly successful in solving complex vision problems. However, these deep models are perceived as ”black box” methods considering the lack of understanding of their internal functioning. There has been a significant recent interest in developing explainable deep learning models, and this paper is an effort in this direction. Building on a recently proposed method called Grad-CAM, we propose a generalized method called Grad-CAM++ that can provide better visual explanations of CNN model predictions, in terms of better object localization as well as explaining occurrences of multiple object instances in a single image, when compared to state-of-the-art. We provide a mathematical derivation for the proposed method, which uses a weighted combination of the positive partial derivatives of the last convolutional layer feature maps with respect to a specific class score as weights to generate a visual explanation for the corresponding class label. Our extensive experiments and evaluations, both subjective and objective, on standard datasets showed that Grad-CAM++ provides promising human-interpretable visual explanations for a given CNN architecture across multiple tasks including classification, image caption generation and 3D action recognition; as well as in new settings such as knowledge distillation.},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/eragon/Zotero/storage/YXP4LN3A/Chattopadhyay et al. - 2018 - Grad-CAM++ Improved Visual Explanations for Deep .pdf}
}

@article{chengImproveDeepLearning2022,
  title = {Improve the {{Deep Learning Models}} in {{Forestry Based}} on {{Explanations}} and {{Expertise}}},
  author = {Cheng, Ximeng and Doosthosseini, Ali and Kunkel, Julian},
  date = {2022-05-19},
  journaltitle = {Frontiers in Plant Science},
  shortjournal = {Front. Plant Sci.},
  volume = {13},
  pages = {902105},
  issn = {1664-462X},
  doi = {10.3389/fpls.2022.902105},
  url = {https://www.frontiersin.org/articles/10.3389/fpls.2022.902105/full},
  urldate = {2022-09-26},
  abstract = {In forestry studies, deep learning models have achieved excellent performance in many application scenarios (e.g., detecting forest damage). However, the unclear model decisions (i.e., black-box) undermine the credibility of the results and hinder their practicality. This study intends to obtain explanations of such models through the use of explainable artificial intelligence methods, and then use feature unlearning methods to improve their performance, which is the first such attempt in the field of forestry. Results of three experiments show that the model training can be guided by expertise to gain specific knowledge, which is reflected by explanations. For all three experiments based on synthetic and real leaf images, the improvement of models is quantified in the classification accuracy (up to 4.6\%) and three indicators of explanation assessment (i.e., root-mean-square error, cosine similarity, and the proportion of important pixels). Besides, the introduced expertise in annotation matrix form was automatically created in all experiments. This study emphasizes that studies of deep learning in forestry should not only pursue model performance (e.g., higher classification accuracy) but also focus on the explanations and try to improve models according to the expertise.},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/WDJK8WMR/Cheng et al. - 2022 - Improve the Deep Learning Models in Forestry Based.pdf}
}

@online{chenGridMaskDataAugmentation2020,
  title = {{{GridMask Data Augmentation}}},
  author = {Chen, Pengguang and Liu, Shu and Zhao, Hengshuang and Jia, Jiaya},
  date = {2020-01-13},
  eprint = {2001.04086},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2001.04086},
  url = {http://arxiv.org/abs/2001.04086},
  urldate = {2023-03-31},
  abstract = {We propose a novel data augmentation method `GridMask' in this paper. It utilizes information removal to achieve state-of-the-art results in a variety of computer vision tasks. We analyze the requirement of information dropping. Then we show limitation of existing information dropping algorithms and propose our structured method, which is simple and yet very effective. It is based on the deletion of regions of the input image. Our extensive experiments show that our method outperforms the latest AutoAugment, which is way more computationally expensive due to the use of reinforcement learning to find the best policies. On the ImageNet dataset for recognition, COCO2017 object detection, and on Cityscapes dataset for semantic segmentation, our method all notably improves performance over baselines. The extensive experiments manifest the effectiveness and generality of the new method.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/eragon/Zotero/storage/CZXE5IRT/Chen et al. - 2020 - GridMask Data Augmentation.pdf}
}

@inproceedings{chenThisLooksThat2019,
  title = {This {{Looks Like That}}: {{Deep Learning}} for {{Interpretable Image Recognition}}},
  shorttitle = {This {{Looks Like That}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Chen, Chaofan and Li, Oscar and Tao, Daniel and Barnett, Alina and Rudin, Cynthia and Su, Jonathan K},
  date = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/adf7ee2dcf142b0e11888e72b43fcb75-Abstract.html},
  urldate = {2023-05-25},
  abstract = {When we are faced with challenging image classification tasks, we often explain our reasoning by dissecting the image, and pointing out prototypical aspects of one class or another. The mounting evidence for each of the classes helps us make our final decision. In this work, we introduce a deep network architecture -- prototypical part network (ProtoPNet), that reasons in a similar way: the network dissects the image by finding prototypical parts, and combines evidence from the prototypes to make a final classification. The model thus reasons in a way that is qualitatively similar to the way ornithologists, physicians, and others would explain to people on how to solve challenging image classification tasks. The network uses only image-level labels for training without any annotations for parts of images. We demonstrate our method on the CUB-200-2011 dataset and the Stanford Cars dataset. Our experiments show that ProtoPNet can achieve comparable accuracy with its analogous non-interpretable counterpart, and when several ProtoPNets are combined into a larger network, it can achieve an accuracy that is on par with some of the best-performing deep models. Moreover, ProtoPNet provides a level of interpretability that is absent in other interpretable deep models.},
  file = {/Users/eragon/Zotero/storage/PPLKB8BC/Chen et al. - 2019 - This Looks Like That Deep Learning for Interpreta.pdf}
}

@online{chenVisionTransformerAdapter2022,
  title = {Vision {{Transformer Adapter}} for {{Dense Predictions}}},
  author = {Chen, Zhe and Duan, Yuchen and Wang, Wenhai and He, Junjun and Lu, Tong and Dai, Jifeng and Qiao, Yu},
  date = {2022-05-17},
  eprint = {2205.08534},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2205.08534},
  url = {http://arxiv.org/abs/2205.08534},
  urldate = {2022-10-23},
  abstract = {This work investigates a simple yet powerful adapter for Vision Transformer (ViT). Unlike recent visual transformers that introduce vision-specific inductive biases into their architectures, ViT achieves inferior performance on dense prediction tasks due to lacking prior information of images. To solve this issue, we propose a Vision Transformer Adapter (ViT-Adapter), which can remedy the defects of ViT and achieve comparable performance to vision-specific models by introducing inductive biases via an additional architecture. Specifically, the backbone in our framework is a vanilla transformer that can be pre-trained with multi-modal data. When fine-tuning on downstream tasks, a modality-specific adapter is used to introduce the data and tasks' prior information into the model, making it suitable for these tasks. We verify the effectiveness of our ViT-Adapter on multiple downstream tasks, including object detection, instance segmentation, and semantic segmentation. Notably, when using HTC++, our ViT-Adapter-L yields 60.1 box AP and 52.1 mask AP on COCO test-dev, surpassing Swin-L by 1.4 box AP and 1.0 mask AP. For semantic segmentation, our ViT-Adapter-L establishes a new state-of-the-art of 60.5 mIoU on ADE20K val, 0.6 points higher than SwinV2-G. We hope that the proposed ViT-Adapter could serve as an alternative for vision-specific transformers and facilitate future research. The code and models will be released at https://github.com/czczup/ViT-Adapter.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/eragon/Zotero/storage/GIE2UAXD/Chen et al. - 2022 - Vision Transformer Adapter for Dense Predictions.pdf;/Users/eragon/Zotero/storage/XTIDT7IR/Chen et al. - 2022 - Vision Transformer Adapter for Dense Predictions.pdf}
}

@inproceedings{cheraghian3DCapsuleExtendingCapsule2019,
  title = {{{3DCapsule}}: {{Extending}} the {{Capsule Architecture}} to {{Classify 3D Point Clouds}}},
  shorttitle = {{{3DCapsule}}},
  booktitle = {2019 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Cheraghian, Ali and Petersson, Lars},
  date = {2019-01},
  pages = {1194--1202},
  publisher = {{IEEE}},
  location = {{Waikoloa Village, HI, USA}},
  doi = {10.1109/WACV.2019.00132},
  url = {https://ieeexplore.ieee.org/document/8658405/},
  urldate = {2022-05-10},
  abstract = {This paper introduces the 3DCapsule, which is a 3D extension of the recently introduced Capsule concept that makes it applicable to unordered point sets. The original Capsule relies on the existence of a spatial relationship between the elements in the feature map it is presented with, whereas in point permutation invariant formulations of 3D point set classification methods, such relationships are typically lost. Here, a new layer called ComposeCaps is introduced that, in lieu of a spatially relevant feature mapping, learns a new mapping that can be exploited by the 3DCapsule. Previous works in the 3D point set classification domain have focused on other parts of the architecture, whereas instead, the 3DCapsule is a drop-in replacement of the commonly used fully connected classifier. It is demonstrated via an ablation study, that when the 3DCapsule is applied to recent 3D point set classification architectures, it consistently shows an improvement, in particular when subjected to noisy data. Similarly, the ComposeCaps layer is evaluated and demonstrates an improvement over the baseline. In an apples-to-apples comparison against state-ofthe-art methods, again, better performance is demonstrated by the 3DCapsule.},
  eventtitle = {2019 {{IEEE Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  isbn = {978-1-72811-975-5},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/4H6GV4V8/Cheraghian and Petersson - 2019 - 3DCapsule Extending the Capsule Architecture to C.pdf}
}

@unpublished{choiAnalyzingLatentSpace2022,
  title = {Analyzing the {{Latent Space}} of {{GAN}} through {{Local Dimension Estimation}}},
  author = {Choi, Jaewoong and Hwang, Geonho and Cho, Hyunsoo and Kang, Myungjoo},
  date = {2022-05-26},
  eprint = {2205.13182},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.13182},
  url = {http://arxiv.org/abs/2205.13182},
  urldate = {2022-09-06},
  abstract = {The impressive success of style-based GANs (StyleGANs) in high-fidelity image synthesis has motivated research to understand the semantic properties of their latent spaces. Recently, a close relationship was observed between the semantically disentangled local perturbations and the local PCA components in the learned latent space \$\textbackslash mathcal\{W\}\$. However, understanding the number of disentangled perturbations remains challenging. Building upon this observation, we propose a local dimension estimation algorithm for an arbitrary intermediate layer in a pre-trained GAN model. The estimated intrinsic dimension corresponds to the number of disentangled local perturbations. In this perspective, we analyze the intermediate layers of the mapping network in StyleGANs. Our analysis clarifies the success of \$\textbackslash mathcal\{W\}\$-space in StyleGAN and suggests an alternative. Moreover, the intrinsic dimension estimation opens the possibility of unsupervised evaluation of global-basis-compatibility and disentanglement for a latent space. Our proposed metric, called Distortion, measures an inconsistency of intrinsic tangent space on the learned latent space. The metric is purely geometric and does not require any additional attribute information. Nevertheless, the metric shows a high correlation with the global-basis-compatibility and supervised disentanglement score. Our findings pave the way towards an unsupervised selection of globally disentangled latent space among the intermediate latent spaces in a GAN.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/eragon/Zotero/storage/XGUDBJX5/Choi et al. - 2022 - Analyzing the Latent Space of GAN through Local Di.pdf}
}

@inproceedings{choiStarGANV2Diverse2020,
  title = {{{StarGAN}} v2: {{Diverse Image Synthesis}} for {{Multiple Domains}}},
  shorttitle = {{{StarGAN}} V2},
  author = {Choi, Yunjey and Uh, Youngjung and Yoo, Jaejun and Ha, Jung-Woo},
  date = {2020},
  pages = {8188--8197},
  url = {https://openaccess.thecvf.com/content_CVPR_2020/html/Choi_StarGAN_v2_Diverse_Image_Synthesis_for_Multiple_Domains_CVPR_2020_paper.html},
  urldate = {2023-06-08},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {/Users/eragon/Zotero/storage/PFNN56RT/Choi et al. - 2020 - StarGAN v2 Diverse Image Synthesis for Multiple D.pdf}
}

@inproceedings{cholletXceptionDeepLearning2017,
  title = {Xception: {{Deep Learning}} with {{Depthwise Separable Convolutions}}},
  shorttitle = {Xception},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Chollet, Francois},
  date = {2017-07},
  pages = {1800--1807},
  publisher = {{IEEE}},
  location = {{Honolulu, HI}},
  doi = {10.1109/CVPR.2017.195},
  url = {http://ieeexplore.ieee.org/document/8099678/},
  urldate = {2023-02-20},
  abstract = {We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.},
  eventtitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-5386-0457-1},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/HLCMFNPM/Chollet - 2017 - Xception Deep Learning with Depthwise Separable C.pdf}
}

@article{cohenSeparabilityGeometryObject2020,
  title = {Separability and Geometry of Object Manifolds in Deep Neural Networks},
  author = {Cohen, Uri and Chung, SueYeon and Lee, Daniel D. and Sompolinsky, Haim},
  date = {2020-12},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {11},
  number = {1},
  pages = {746},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-14578-5},
  url = {http://www.nature.com/articles/s41467-020-14578-5},
  urldate = {2022-11-15},
  abstract = {Abstract             Stimuli are represented in the brain by the collective population responses of sensory neurons, and an object presented under varying conditions gives rise to a collection of neural population responses called an ‘object manifold’. Changes in the object representation along a hierarchical sensory system are associated with changes in the geometry of those manifolds, and recent theoretical progress connects this geometry with ‘classification capacity’, a quantitative measure of the ability to support object classification. Deep neural networks trained on object classification tasks are a natural testbed for the applicability of this relation. We show how classification capacity improves along the hierarchies of deep neural networks with different architectures. We demonstrate that changes in the geometry of the associated object manifolds underlie this improved capacity, and shed light on the functional roles different levels in the hierarchy play to achieve it, through orchestrated reduction of manifolds’ radius, dimensionality and inter-manifold correlations.},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/XPNY6C5V/Cohen et al. - 2020 - Separability and geometry of object manifolds in d.pdf}
}

@article{corniaPredictingHumanEye2018,
  title = {Predicting {{Human Eye Fixations}} via an {{LSTM-Based Saliency Attentive Model}}},
  author = {Cornia, Marcella and Baraldi, Lorenzo and Serra, Giuseppe and Cucchiara, Rita},
  date = {2018-10},
  journaltitle = {IEEE Transactions on Image Processing},
  volume = {27},
  number = {10},
  pages = {5142--5154},
  issn = {1941-0042},
  doi = {10.1109/TIP.2018.2851672},
  abstract = {Data-driven saliency has recently gained a lot of attention thanks to the use of convolutional neural networks for predicting gaze fixations. In this paper, we go beyond standard approaches to saliency prediction, in which gaze maps are computed with a feed-forward network, and present a novel model which can predict accurate saliency maps by incorporating neural attentive mechanisms. The core of our solution is a convolutional long short-term memory that focuses on the most salient regions of the input image to iteratively refine the predicted saliency map. In addition, to tackle the center bias typical of human eye fixations, our model can learn a set of prior maps generated with Gaussian functions. We show, through an extensive evaluation, that the proposed architecture outperforms the current state-of-the-art on public saliency prediction datasets. We further study the contribution of each key component to demonstrate their robustness on different scenarios.},
  eventtitle = {{{IEEE Transactions}} on {{Image Processing}}},
  keywords = {Computational modeling,Computer architecture,convolutional neural networks,deep learning,Deep learning,Feature extraction,human eye fixations,Predictive models,Saliency,Task analysis,Visualization},
  file = {/Users/eragon/Zotero/storage/T7PLI7VZ/Cornia et al. - 2018 - Predicting Human Eye Fixations via an LSTM-Based S.pdf}
}

@inproceedings{corniaSAMPushingLimits2018,
  title = {{{SAM}}: {{Pushing}} the {{Limits}} of {{Saliency Prediction Models}}},
  shorttitle = {{{SAM}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Cornia, Marcella and Baraldi, Lorenzo and Serra, Giuseppe and Cucchiara, Rita},
  date = {2018-06},
  pages = {1971--19712},
  publisher = {{IEEE}},
  location = {{Salt Lake City, UT, USA}},
  doi = {10.1109/CVPRW.2018.00250},
  url = {https://ieeexplore.ieee.org/document/8575416/},
  urldate = {2023-05-08},
  abstract = {The prediction of human eye fixations has been recently gaining a lot of attention thanks to the improvements shown by deep architectures. In our work, we go beyond classical feed-forward networks to predict saliency maps and propose a Saliency Attentive Model which incorporates neural attention mechanisms to iteratively refine predictions. Experiments demonstrate that the proposed strategy overcomes by a considerable margin the state of the art on the largest dataset available for saliency prediction. Here, we provide experimental results on other popular saliency datasets to confirm the effectiveness and the generalization capabilities of our model, which enable us to reach the state of the art on all considered datasets.},
  eventtitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  isbn = {978-1-5386-6100-0},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/HMKE3CQU/Cornia et al. - 2018 - SAM Pushing the Limits of Saliency Prediction Mod.pdf}
}

@inproceedings{cubukRandaugmentPracticalAutomated2020,
  title = {Randaugment: {{Practical}} Automated Data Augmentation with a Reduced Search Space},
  shorttitle = {Randaugment},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Cubuk, Ekin D. and Zoph, Barret and Shlens, Jonathon and Le, Quoc V.},
  date = {2020-06},
  pages = {3008--3017},
  publisher = {{IEEE}},
  location = {{Seattle, WA, USA}},
  doi = {10.1109/CVPRW50498.2020.00359},
  url = {https://ieeexplore.ieee.org/document/9150790/},
  urldate = {2023-01-16},
  abstract = {Recent work on automated augmentation strategies has led to state-of-the-art results in image classification and object detection. An obstacle to a large-scale adoption of these methods is that they require a separate and expensive search phase. A common way to overcome the expense of the search phase was to use a smaller proxy task. However, it was not clear if the optimized hyperparameters found on the proxy task are also optimal for the actual task. In this work, we rethink the process of designing automated augmentation strategies. We find that while previous work required a search for both magnitude and probability of each operation independently, it is sufficient to only search for a single distortion magnitude that jointly controls all operations. We hence propose a simplified search space that vastly reduces the computational expense of automated augmentation, and permits the removal of a separate proxy task.},
  eventtitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  isbn = {978-1-72819-360-1},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/F9U8Q2AG/Cubuk et al. - 2020 - Randaugment Practical automated data augmentation.pdf}
}

@online{dabkowskiRealTimeImage2017,
  title = {Real {{Time Image Saliency}} for {{Black Box Classifiers}}},
  author = {Dabkowski, Piotr and Gal, Yarin},
  date = {2017-05-22},
  eprint = {1705.07857},
  eprinttype = {arxiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.1705.07857},
  url = {http://arxiv.org/abs/1705.07857},
  urldate = {2022-11-24},
  abstract = {In this work we develop a fast saliency detection method that can be applied to any differentiable image classifier. We train a masking model to manipulate the scores of the classifier by masking salient parts of the input image. Our model generalises well to unseen images and requires a single forward pass to perform saliency detection, therefore suitable for use in real-time systems. We test our approach on CIFAR-10 and ImageNet datasets and show that the produced saliency maps are easily interpretable, sharp, and free of artifacts. We suggest a new metric for saliency and test our method on the ImageNet object localisation task. We achieve results outperforming other weakly supervised methods.},
  pubstate = {preprint},
  keywords = {Statistics - Machine Learning},
  file = {/Users/eragon/Zotero/storage/AFR2PAGY/Dabkowski and Gal - 2017 - Real Time Image Saliency for Black Box Classifiers.pdf}
}

@article{darnaiInternetAddictionFunctional2019,
  title = {Internet Addiction and Functional Brain Networks: Task-Related {{fMRI}} Study},
  shorttitle = {Internet Addiction and Functional Brain Networks},
  author = {Darnai, Gergely and Perlaki, Gábor and Zsidó, András N. and Inhóf, Orsolya and Orsi, Gergely and Horváth, Réka and Nagy, Szilvia Anett and Lábadi, Beatrix and Tényi, Dalma and Kovács, Norbert and Dóczi, Tamás and Demetrovics, Zsolt and Janszky, József},
  date = {2019-10-31},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {9},
  number = {1},
  pages = {15777},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/s41598-019-52296-1},
  url = {https://www.nature.com/articles/s41598-019-52296-1},
  urldate = {2022-06-15},
  abstract = {A common brain-related feature of addictions is the altered function of higher-order brain networks. Growing evidence suggests that Internet-related addictions are also associated with breakdown of functional brain networks. Taking into consideration the limited number of studies used in previous studies in Internet addiction (IA), our aim was to investigate the functional correlates of IA in the default mode network (DMN) and in the inhibitory control network (ICN). To observe these relationships, task-related fMRI responses to verbal Stroop and non-verbal Stroop-like tasks were measured in 60 healthy university students. The Problematic Internet Use Questionnaire (PIUQ) was used to assess IA. We found significant deactivations in areas related to the DMN (precuneus, posterior cingulate gyrus) and these areas were negatively correlated with PIUQ during incongruent stimuli. In Stroop task the incongruent\_minus\_congruent contrast showed positive correlation with PIUQ in areas related to the ICN (left inferior frontal gyrus, left frontal pole, left central opercular, left frontal opercular, left frontal orbital and left insular cortex). Altered DMN might explain some comorbid symptoms and might predict treatment outcomes, while altered ICN may be the reason for having difficulties in stopping and controlling overuse.},
  issue = {1},
  langid = {english},
  keywords = {Addiction,Neural circuits},
  file = {/Users/eragon/Zotero/storage/M38CKXQV/Darnai et al. - 2019 - Internet addiction and functional brain networks .pdf;/Users/eragon/Zotero/storage/SACIH974/s41598-019-52296-1.html}
}

@article{darnaiNeuralCorrelatesMental2023,
  title = {The Neural Correlates of Mental Fatigue and Reward Processing: {{A}} Task-Based {{fMRI}} Study},
  shorttitle = {The Neural Correlates of Mental Fatigue and Reward Processing},
  author = {Darnai, Gergely and Matuz, András and Alhour, Husamalddin Ali and Perlaki, Gábor and Orsi, Gergely and Arató, Ákos and Szente, Anna and Áfra, Eszter and Nagy, Szilvia Anett and Janszky, József and Csathó, Árpád},
  date = {2023-01-01},
  journaltitle = {NeuroImage},
  shortjournal = {NeuroImage},
  volume = {265},
  pages = {119812},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2022.119812},
  url = {https://www.sciencedirect.com/science/article/pii/S1053811922009338},
  urldate = {2023-01-25},
  abstract = {Increasing time spent on the task (i.e., the time-on-task (ToT) effect) often results in mental fatigue. Typical effects of ToT are decreasing levels of task-related motivation and the deterioration of cognitive performance. However, a massive body of research indicates that the detrimental effects can be reversed by extrinsic motivators, for example, providing rewards to fatigued participants. Although several attempts have been made to identify brain areas involved in mental fatigue and related reward processing, the neural correlates are still less understood. In this study, we used the psychomotor vigilance task to induce mental fatigue and blood oxygen-level-dependent functional magnetic resonance imaging to investigate the neural correlates of the ToT effect and the reward effect (i.e., providing extra monetary reward after fatigue induction) in a healthy young sample. Our results were interpreted in a recently proposed neurocognitive framework. The activation of the right middle frontal gyrus, right insula and right anterior cingulate gyrus decreased as fatigue emerged and the cognitive performance dropped. However, after providing an extra reward, the cognitive performance, as well as activation of these areas, increased. Moreover, the activation levels of all of the mentioned areas were negatively associated with reaction times. Our results confirm that the middle frontal gyrus, insula and anterior cingulate cortex play crucial roles in cost-benefit evaluations, a potential background mechanism underlying fatigue, as suggested by the neurocognitive framework.},
  langid = {english},
  keywords = {Anterior cingulate cortex,fMRI,Insula,Mental fatigue,Middle frontal gyrus,Motivation,Neurocognitive framework,Psychomotor vigilance task},
  file = {/Users/eragon/Zotero/storage/YT9VF7TD/Darnai et al. - 2023 - The neural correlates of mental fatigue and reward.pdf}
}

@online{dasOpportunitiesChallengesExplainable2020,
  title = {Opportunities and {{Challenges}} in {{Explainable Artificial Intelligence}} ({{XAI}}): {{A Survey}}},
  shorttitle = {Opportunities and {{Challenges}} in {{Explainable Artificial Intelligence}} ({{XAI}})},
  author = {Das, Arun and Rad, Paul},
  date = {2020-06-22},
  eprint = {2006.11371},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2006.11371},
  url = {http://arxiv.org/abs/2006.11371},
  urldate = {2023-01-17},
  abstract = {Nowadays, deep neural networks are widely used in mission critical systems such as healthcare, self-driving vehicles, and military which have direct impact on human lives. However, the black-box nature of deep neural networks challenges its use in mission critical applications, raising ethical and judicial concerns inducing lack of trust. Explainable Artificial Intelligence (XAI) is a field of Artificial Intelligence (AI) that promotes a set of tools, techniques, and algorithms that can generate high-quality interpretable, intuitive, human-understandable explanations of AI decisions. In addition to providing a holistic view of the current XAI landscape in deep learning, this paper provides mathematical summaries of seminal work. We start by proposing a taxonomy and categorizing the XAI techniques based on their scope of explanations, methodology behind the algorithms, and explanation level or usage which helps build trustworthy, interpretable, and self-explanatory deep learning models. We then describe the main principles used in XAI research and present the historical timeline for landmark studies in XAI from 2007 to 2020. After explaining each category of algorithms and approaches in detail, we then evaluate the explanation maps generated by eight XAI algorithms on image data, discuss the limitations of this approach, and provide potential future directions to improve XAI evaluation.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/eragon/Zotero/storage/24NF7HV4/Das and Rad - 2020 - Opportunities and Challenges in Explainable Artifi.pdf}
}

@online{dattaWhoThinkingPush2023,
  title = {Who's {{Thinking}}? {{A Push}} for {{Human-Centered Evaluation}} of {{LLMs}} Using the {{XAI Playbook}}},
  shorttitle = {Who's {{Thinking}}?},
  author = {Datta, Teresa and Dickerson, John P.},
  date = {2023-03-10},
  eprint = {2303.06223},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.06223},
  url = {http://arxiv.org/abs/2303.06223},
  urldate = {2023-05-19},
  abstract = {Deployed artificial intelligence (AI) often impacts humans, and there is no one-size-fits-all metric to evaluate these tools. Human-centered evaluation of AI-based systems combines quantitative and qualitative analysis and human input. It has been explored to some depth in the explainable AI (XAI) and human-computer interaction (HCI) communities. Gaps remain, but the basic understanding that humans interact with AI and accompanying explanations, and that humans' needs -- complete with their cognitive biases and quirks -- should be held front and center, is accepted by the community. In this paper, we draw parallels between the relatively mature field of XAI and the rapidly evolving research boom around large language models (LLMs). Accepted evaluative metrics for LLMs are not human-centered. We argue that many of the same paths tread by the XAI community over the past decade will be retread when discussing LLMs. Specifically, we argue that humans' tendencies -- again, complete with their cognitive biases and quirks -- should rest front and center when evaluating deployed LLMs. We outline three developed focus areas of human-centered evaluation of XAI: mental models, use case utility, and cognitive engagement, and we highlight the importance of exploring each of these concepts for LLMs. Our goal is to jumpstart human-centered LLM evaluation.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction},
  file = {/Users/eragon/Zotero/storage/YEMKIB4A/Datta and Dickerson - 2023 - Who's Thinking A Push for Human-Centered Evaluati.pdf}
}

@article{dengImageNetLargeScaleHierarchical,
  title = {{{ImageNet}}: {{A Large-Scale Hierarchical Image Database}}},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a largescale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 5001000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/CBVRWAR5/Deng et al. - ImageNet A Large-Scale Hierarchical Image Databas.pdf}
}

@inproceedings{dengImageNetLargeScaleHierarchical2009,
  title = {{{ImageNet}}: {{A Large-Scale Hierarchical Image Database}}},
  booktitle = {{{CVPR09}}},
  author = {Deng, J. and Dong, W. and Socher, R. and Li, L.-J. and Li, K. and Fei-Fei, L.},
  date = {2009}
}

@online{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  date = {2019-05-24},
  eprint = {1810.04805},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1810.04805},
  url = {http://arxiv.org/abs/1810.04805},
  urldate = {2022-10-21},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/eragon/Zotero/storage/UKB798MK/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf}
}

@online{devriesImprovedRegularizationConvolutional2017,
  title = {Improved {{Regularization}} of {{Convolutional Neural Networks}} with {{Cutout}}},
  author = {DeVries, Terrance and Taylor, Graham W.},
  date = {2017-11-29},
  eprint = {1708.04552},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1708.04552},
  url = {http://arxiv.org/abs/1708.04552},
  urldate = {2023-03-27},
  abstract = {Convolutional neural networks are capable of learning powerful representational spaces, which are necessary for tackling complex learning tasks. However, due to the model capacity required to capture such representations, they are often susceptible to overfitting and therefore require proper regularization in order to generalize well. In this paper, we show that the simple regularization technique of randomly masking out square regions of input during training, which we call cutout, can be used to improve the robustness and overall performance of convolutional neural networks. Not only is this method extremely easy to implement, but we also demonstrate that it can be used in conjunction with existing forms of data augmentation and other regularizers to further improve model performance. We evaluate this method by applying it to current state-of-the-art architectures on the CIFAR-10, CIFAR-100, and SVHN datasets, yielding new state-of-the-art results of 2.56\%, 15.20\%, and 1.30\% test error respectively. Code is available at https://github.com/uoguelph-mlrg/Cutout},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/eragon/Zotero/storage/6FFUVLAN/DeVries and Taylor - 2017 - Improved Regularization of Convolutional Neural Ne.pdf}
}

@online{dhamdhereHowImportantNeuron2018,
  title = {How {{Important Is}} a {{Neuron}}?},
  author = {Dhamdhere, Kedar and Sundararajan, Mukund and Yan, Qiqi},
  date = {2018-05-30},
  eprint = {1805.12233},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1805.12233},
  url = {http://arxiv.org/abs/1805.12233},
  urldate = {2022-11-28},
  abstract = {The problem of attributing a deep network's prediction to its \textbackslash emph\{input/base\} features is well-studied. We introduce the notion of \textbackslash emph\{conductance\} to extend the notion of attribution to the understanding the importance of \textbackslash emph\{hidden\} units. Informally, the conductance of a hidden unit of a deep network is the \textbackslash emph\{flow\} of attribution via this hidden unit. We use conductance to understand the importance of a hidden unit to the prediction for a specific input, or over a set of inputs. We evaluate the effectiveness of conductance in multiple ways, including theoretical properties, ablation studies, and a feature selection task. The empirical evaluations are done using the Inception network over ImageNet data, and a sentiment analysis network over reviews. In both cases, we demonstrate the effectiveness of conductance in identifying interesting insights about the internal workings of these networks.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/eragon/Zotero/storage/RBRVP5KX/Dhamdhere et al. - 2018 - How Important Is a Neuron.pdf}
}

@article{dieterRoleEmotionalInhibitory2017,
  title = {The Role of Emotional Inhibitory Control in Specific Internet Addiction – an {{fMRI}} Study},
  author = {Dieter, Julia and Hoffmann, Sabine and Mier, Daniela and Reinhard, Iris and Beutel, Martin and Vollstädt-Klein, Sabine and Kiefer, Falk and Mann, Karl and Leménager, Tagrid},
  date = {2017-05},
  journaltitle = {Behavioural Brain Research},
  shortjournal = {Behavioural Brain Research},
  volume = {324},
  pages = {1--14},
  issn = {01664328},
  doi = {10.1016/j.bbr.2017.01.046},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0166432816310130},
  urldate = {2022-06-15},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/JDFE99KJ/Dieter et al. - 2017 - The role of emotional inhibitory control in specif.pdf}
}

@online{dombrowskiExplanationsCanBe2019,
  title = {Explanations Can Be Manipulated and Geometry Is to Blame},
  author = {Dombrowski, Ann-Kathrin and Alber, Maximilian and Anders, Christopher J. and Ackermann, Marcel and Müller, Klaus-Robert and Kessel, Pan},
  date = {2019-09-25},
  eprint = {1906.07983},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1906.07983},
  url = {http://arxiv.org/abs/1906.07983},
  urldate = {2022-11-24},
  abstract = {Explanation methods aim to make neural networks more trustworthy and interpretable. In this paper, we demonstrate a property of explanation methods which is disconcerting for both of these purposes. Namely, we show that explanations can be manipulated arbitrarily by applying visually hardly perceptible perturbations to the input that keep the network's output approximately constant. We establish theoretically that this phenomenon can be related to certain geometrical properties of neural networks. This allows us to derive an upper bound on the susceptibility of explanations to manipulations. Based on this result, we propose effective mechanisms to enhance the robustness of explanations.},
  pubstate = {preprint},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/eragon/Zotero/storage/9FUQAKVN/Dombrowski et al. - 2019 - Explanations can be manipulated and geometry is to.pdf;/Users/eragon/Zotero/storage/YI5IAMS5/Dombrowski et al. - 2019 - Explanations can be manipulated and geometry is to.pdf}
}

@article{dongImpairedErrorMonitoringFunction2013,
  title = {Impaired {{Error-Monitoring Function}} in {{People}} with {{Internet Addiction Disorder}}: {{An Event-Related fMRI Study}}},
  shorttitle = {Impaired {{Error-Monitoring Function}} in {{People}} with {{Internet Addiction Disorder}}},
  author = {Dong, Guangheng and Shen, Yue and Huang, Jie and Du, Xiaoxia},
  date = {2013},
  journaltitle = {European Addiction Research},
  shortjournal = {EAR},
  volume = {19},
  number = {5},
  eprint = {23548798},
  eprinttype = {pmid},
  pages = {269--275},
  publisher = {{Karger Publishers}},
  issn = {1022-6877, 1421-9891},
  doi = {10.1159/000346783},
  url = {https://www.karger.com/Article/FullText/346783},
  urldate = {2022-06-15},
  abstract = {\textbf{\emph{Background:}} Internet addiction disorder (IAD) is rapidly becoming a prevalent mental health concern around the world. The neurobiological underpinnings of IAD should be studied to unravel the potential heterogeneity. This study was set to investigate the error-monitoring ability in IAD subjects. \textbf{\emph{Methods:}} Fifteen IAD subjects and 15 healthy controls (HC) participated in this study. Participants were asked to perform a fast Stroop task that may show error responses. Behavioral and neurobiological results in relation to error responses were compared between IAD subjects and HC. \textbf{\emph{Results:}} Compared to HC, IAD subjects showed increased activation in the anterior cingulate cortex (ACC) and decreased activation in the orbitofrontal cortex following error responses. Significant correlation was found between ACC activation and the Internet addiction test scores. \textbf{\emph{Conclusions:}} IAD subjects show an impaired error-monitoring ability compared to HC, which can be detected by the hyperactivation in ACC in error responses.},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/2NXKATUN/Dong et al. - 2013 - Impaired Error-Monitoring Function in People with .pdf}
}

@online{doshi-velezRigorousScienceInterpretable2017,
  title = {Towards {{A Rigorous Science}} of {{Interpretable Machine Learning}}},
  author = {Doshi-Velez, Finale and Kim, Been},
  date = {2017-03-02},
  eprint = {1702.08608},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1702.08608},
  urldate = {2023-03-09},
  abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/eragon/Zotero/storage/Y22HYSGW/Doshi-Velez and Kim - 2017 - Towards A Rigorous Science of Interpretable Machin.pdf}
}

@online{dosovitskiyImageWorth16x162021,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  date = {2021-06-03},
  eprint = {2010.11929},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2010.11929},
  url = {http://arxiv.org/abs/2010.11929},
  urldate = {2022-10-21},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/eragon/Zotero/storage/2VL6R42V/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Im.pdf}
}

@unpublished{dulhantyAuditingImageNetModeldriven2019,
  title = {Auditing {{ImageNet}}: {{Towards}} a {{Model-driven Framework}} for {{Annotating Demographic Attributes}} of {{Large-Scale Image Datasets}}},
  shorttitle = {Auditing {{ImageNet}}},
  author = {Dulhanty, Chris and Wong, Alexander},
  date = {2019-06-04},
  eprint = {1905.01347},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1905.01347},
  url = {http://arxiv.org/abs/1905.01347},
  urldate = {2022-09-06},
  abstract = {The ImageNet dataset ushered in a flood of academic and industry interest in deep learning for computer vision applications. Despite its significant impact, there has not been a comprehensive investigation into the demographic attributes of images contained within the dataset. Such a study could lead to new insights on inherent biases within ImageNet, particularly important given it is frequently used to pretrain models for a wide variety of computer vision tasks. In this work, we introduce a model-driven framework for the automatic annotation of apparent age and gender attributes in large-scale image datasets. Using this framework, we conduct the first demographic audit of the 2012 ImageNet Large Scale Visual Recognition Challenge (ILSVRC) subset of ImageNet and the "person" hierarchical category of ImageNet. We find that 41.62\% of faces in ILSVRC appear as female, 1.71\% appear as individuals above the age of 60, and males aged 15 to 29 account for the largest subgroup with 27.11\%. We note that the presented model-driven framework is not fair for all intersectional groups, so annotation are subject to bias. We present this work as the starting point for future development of unbiased annotation models and for the study of downstream effects of imbalances in the demographics of ImageNet. Code and annotations are available at: http://bit.ly/ImageNetDemoAudit},
  keywords = {Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/Users/eragon/Zotero/storage/8GNWL9GM/Dulhanty and Wong - 2019 - Auditing ImageNet Towards a Model-driven Framewor.pdf}
}

@inproceedings{dutagaciBenchmarkBestView2010,
  title = {A Benchmark for Best View Selection of {{3D}} Objects},
  booktitle = {Proceedings of the {{ACM}} Workshop on {{3D}} Object Retrieval},
  author = {Dutagaci, Helin and Cheung, Chun Pan and Godil, Afzal},
  date = {2010},
  pages = {45--50}
}

@online{dvornikImportanceVisualContext2019,
  title = {On the {{Importance}} of {{Visual Context}} for {{Data Augmentation}} in {{Scene Understanding}}},
  author = {Dvornik, Nikita and Mairal, Julien and Schmid, Cordelia},
  date = {2019-09-19},
  eprint = {1809.02492},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1809.02492},
  urldate = {2022-10-21},
  abstract = {Performing data augmentation for learning deep neural networks is known to be important for training visual recognition systems. By artificially increasing the number of training examples, it helps reducing overfitting and improves generalization. While simple image transformations can already improve predictive performance in most vision tasks, larger gains can be obtained by leveraging task-specific prior knowledge. In this work, we consider object detection, semantic and instance segmentation and augment the training images by blending objects in existing scenes, using instance segmentation annotations. We observe that randomly pasting objects on images hurts the performance, unless the object is placed in the right context. To resolve this issue, we propose an explicit context model by using a convolutional neural network, which predicts whether an image region is suitable for placing a given object or not. In our experiments, we show that our approach is able to improve object detection, semantic and instance segmentation on the PASCAL VOC12 and COCO datasets, with significant gains in a limited annotation scenario, i.e. when only one category is annotated. We also show that the method is not limited to datasets that come with expensive pixel-wise instance annotations and can be used when only bounding boxes are available, by employing weakly-supervised learning for instance masks approximation.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/eragon/Zotero/storage/F4LSAVHY/Dvornik et al. - 2019 - On the Importance of Visual Context for Data Augme.pdf}
}

@inproceedings{dvornikModelingVisualContext2018,
  title = {Modeling {{Visual Context}} Is {{Key}} to {{Augmenting Object Detection Datasets}}},
  author = {Dvornik, Nikita and Mairal, Julien and Schmid, Cordelia},
  date = {2018},
  pages = {364--380},
  url = {https://openaccess.thecvf.com/content_ECCV_2018/html/NIKITA_DVORNIK_Modeling_Visual_Context_ECCV_2018_paper.html},
  urldate = {2022-10-21},
  eventtitle = {Proceedings of the {{European Conference}} on {{Computer Vision}} ({{ECCV}})},
  file = {/Users/eragon/Zotero/storage/FR4JCRWJ/Dvornik et al. - 2018 - Modeling Visual Context is Key to Augmenting Objec.pdf}
}

@inproceedings{dvornikModelingVisualContext2018a,
  title = {Modeling {{Visual Context}} Is {{Key}} to {{Augmenting Object Detection Datasets}}},
  author = {Dvornik, Nikita and Mairal, Julien and Schmid, Cordelia},
  date = {2018},
  pages = {364--380},
  url = {https://openaccess.thecvf.com/content_ECCV_2018/html/NIKITA_DVORNIK_Modeling_Visual_Context_ECCV_2018_paper.html},
  urldate = {2023-03-31},
  eventtitle = {Proceedings of the {{European Conference}} on {{Computer Vision}} ({{ECCV}})},
  file = {/Users/eragon/Zotero/storage/QUAACLNF/Dvornik et al. - 2018 - Modeling Visual Context is Key to Augmenting Objec.pdf}
}

@inproceedings{dwibediCutPasteLearn2017,
  title = {Cut, {{Paste}} and {{Learn}}: {{Surprisingly Easy Synthesis}} for {{Instance Detection}}},
  shorttitle = {Cut, {{Paste}} and {{Learn}}},
  author = {Dwibedi, Debidatta and Misra, Ishan and Hebert, Martial},
  date = {2017},
  pages = {1301--1310},
  url = {https://openaccess.thecvf.com/content_iccv_2017/html/Dwibedi_Cut_Paste_and_ICCV_2017_paper.html},
  urldate = {2023-03-31},
  eventtitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}}},
  file = {/Users/eragon/Zotero/storage/QRE3LSHL/Dwibedi et al. - 2017 - Cut, Paste and Learn Surprisingly Easy Synthesis .pdf}
}

@article{e.kissQuantifierSpreadingChildren2017,
  title = {Quantifier Spreading: Children Misled by Ostensive Cues},
  shorttitle = {Quantifier Spreading},
  author = {É. Kiss, Katalin and Zétényi, Tamás},
  date = {2017-04-26},
  journaltitle = {Glossa: a journal of general linguistics},
  volume = {2},
  number = {1},
  issn = {2397-1835},
  doi = {10.5334/gjgl.147},
  url = {https://www.glossa-journal.org/article/id/4900/},
  urldate = {2022-10-24},
  abstract = {This paper calls attention to a methodological problem of acquisition experiments. It shows that the economy of the stimulus employed in child language experiments may lend an increased ostensive effect to the message communicated to the child. Thus, when the visual stimulus in a sentence-picture matching task is a minimal model abstracting away from the details of the situation, children often regard all the elements of the stimulus as ostensive clues to be represented in the corresponding sentence. The use of such minimal stimuli is mistaken when the experiment aims to test whether or not a certain element of the stimulus is relevant for the linguistic representation or interpretation. The paper illustrates this point by an experiment involving quantifier spreading. It is claimed that children find a universally quantified sentence like Every girl is riding a bicycle to be a false description of a picture showing three girls riding bicycles and a solo bicycle because they are misled to believe that all the elements in the visual stimulus are relevant, hence all of them are to be represented by the corresponding linguistic description. When the iconic drawings were replaced by photos taken in a natural environment rich in accidental details, the occurrence of quantifier spreading was radically reduced. It is shown that an extra object in the visual stimulus can lead to the rejection of the sentence also in the case of sentences involving no quantification, which gives further support to the claim that the source of the problem is not (or not only) the grammatical or cognitive difficulty of quantification but the unintended ostensive effect of the extra object.~~This article is part of the special collection: Acquisition of Quantification~A correction article related to this research can be found here:~http://doi.org/10.5334/gjgl.902},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/DTUMSFUY/É. Kiss and Zétényi - 2017 - Quantifier spreading children misled by ostensive.pdf}
}

@online{ehsanSocialConstructionXAI2022,
  title = {Social {{Construction}} of {{XAI}}: {{Do We Need One Definition}} to {{Rule Them All}}?},
  shorttitle = {Social {{Construction}} of {{XAI}}},
  author = {Ehsan, Upol and Riedl, Mark O.},
  date = {2022-11-11},
  eprint = {2211.06499},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2211.06499},
  url = {http://arxiv.org/abs/2211.06499},
  urldate = {2023-05-19},
  abstract = {There is a growing frustration amongst researchers and developers in Explainable AI (XAI) around the lack of consensus around what is meant by 'explainability'. Do we need one definition of explainability to rule them all? In this paper, we argue why a singular definition of XAI is neither feasible nor desirable at this stage of XAI's development. We view XAI through the lenses of Social Construction of Technology (SCOT) to explicate how diverse stakeholders (relevant social groups) have different interpretations (interpretative flexibility) that shape the meaning of XAI. Forcing a standardization (closure) on the pluralistic interpretations too early can stifle innovation and lead to premature conclusions. We share how we can leverage the pluralism to make progress in XAI without having to wait for a definitional consensus.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction},
  file = {/Users/eragon/Zotero/storage/2SMHTFHT/Ehsan and Riedl - 2022 - Social Construction of XAI Do We Need One Definit.pdf}
}

@online{eldanTinyStoriesHowSmall2023,
  title = {{{TinyStories}}: {{How Small Can Language Models Be}} and {{Still Speak Coherent English}}?},
  shorttitle = {{{TinyStories}}},
  author = {Eldan, Ronen and Li, Yuanzhi},
  date = {2023-05-12},
  eprint = {2305.07759},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.07759},
  url = {http://arxiv.org/abs/2305.07759},
  urldate = {2023-05-25},
  abstract = {Language models (LMs) are powerful tools for natural language processing, but they often struggle to produce coherent and fluent text when they are small. Models with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) can rarely generate coherent and consistent English text beyond a few words even after extensive training. This raises the question of whether the emergence of the ability to produce coherent English text only occurs at larger scales (with hundreds of millions of parameters or more) and complex architectures (with many layers of global attention). In this work, we introduce TinyStories, a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-olds usually understand, generated by GPT-3.5 and GPT-4. We show that TinyStories can be used to train and evaluate LMs that are much smaller than the state-of-the-art models (below 10 million total parameters), or have much simpler architectures (with only one transformer block), yet still produce fluent and consistent stories with several paragraphs that are diverse and have almost perfect grammar, and demonstrate reasoning capabilities. We also introduce a new paradigm for the evaluation of language models: We suggest a framework which uses GPT-4 to grade the content generated by these models as if those were stories written by students and graded by a (human) teacher. This new paradigm overcomes the flaws of standard benchmarks which often requires the model's output to be very structures, and moreover provides a multidimensional score for the model, providing scores for different capabilities such as grammar, creativity and consistency. We hope that TinyStories can facilitate the development, analysis and research of LMs, especially for low-resource or specialized domains, and shed light on the emergence of language capabilities in LMs.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/eragon/Zotero/storage/EX6NFW8T/Eldan and Li - 2023 - TinyStories How Small Can Language Models Be and .pdf}
}

@article{eldarInteractionEmotionalState2015,
  title = {Interaction between Emotional State and Learning Underlies Mood Instability},
  author = {Eldar, Eran and Niv, Yael},
  date = {2015-05-05},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {6},
  number = {1},
  pages = {6149},
  issn = {2041-1723},
  doi = {10.1038/ncomms7149},
  url = {http://www.nature.com/articles/ncomms7149},
  urldate = {2022-05-27},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/LA25CY39/Eldar and Niv - 2015 - Interaction between emotional state and learning u.pdf}
}

@inproceedings{elhoseinyComparativeAnalysisStudy2016,
  title = {A Comparative Analysis and Study of Multiview {{CNN}} Models for Joint Object Categorization and Pose Estimation},
  booktitle = {International {{Conference}} on {{Machine}} Learning},
  author = {Elhoseiny, Mohamed and El-Gaaly, Tarek and Bakry, Amr and Elgammal, Ahmed},
  date = {2016},
  pages = {888--897},
  publisher = {{PMLR}}
}

@article{eppelClassifyingSpecificImage,
  title = {Classifying a Specific Image Region Using Convolutional Nets with an {{ROI}} Mask as Input},
  author = {Eppel, Sagi},
  pages = {8},
  abstract = {Convolutional neural nets (CNN) are the leading computer vision method for classifying images. In some cases, it is desirable to classify only a specific region of the image that corresponds to a certain object. Hence, assuming that the region of the object in the image is known in advance and is given as a binary region of interest (ROI) mask, the goal is to classify the object in this region using a convolutional neural net. This goal is achieved using a standard image classification net with the addition of a side branch, which converts the ROI mask into an attention map. This map is then combined with the image classification net. This allows the net to focus the attention on the object region while still extracting contextual cues from the background. This approach was evaluated using the COCO object dataset and the OpenSurfaces materials dataset. In both cases, it gave superior results to methods that completely ignore the background region. In addition, it was found that combining the attention map at the first layer of the net gave better results than combining it at higher layers of the net. The advantages of this method are most apparent in the classification of small regions which demands a great deal of contextual information from the background.},
  langid = {english},
  keywords = {done}
}

@online{EthicalPrinciplesGovernance,
  title = {Ethical {{Principles}} and {{Governance Technology Development}} of {{AI}} in {{China}} | {{Elsevier Enhanced Reader}}},
  doi = {10.1016/j.eng.2019.12.015},
  url = {https://reader.elsevier.com/reader/sd/pii/S2095809920300011?token=BCF6223A379F2D2FFF8ED1953A89610B058E50DB52EACA96871713C72D2CF4A6B61BE11552799F45A55E826D0A6575EF&originRegion=eu-west-1&originCreation=20230117183919},
  urldate = {2023-01-17},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/DKV768WF/Ethical Principles and Governance Technology Devel.pdf}
}

@online{fabbriEnhancingUnderwaterImagery2018,
  title = {Enhancing {{Underwater Imagery}} Using {{Generative Adversarial Networks}}},
  author = {Fabbri, Cameron and Islam, Md Jahidul and Sattar, Junaed},
  date = {2018-01-11},
  eprint = {1801.04011},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1801.04011},
  urldate = {2022-11-19},
  abstract = {Autonomous underwater vehicles (AUVs) rely on a variety of sensors – acoustic, inertial and visual – for intelligent decision making. Due to its non-intrusive, passive nature, and high information content, vision is an attractive sensing modality, particularly at shallower depths. However, factors such as light refraction and absorption, suspended particles in the water, and color distortion affect the quality of visual data, resulting in noisy and distorted images. AUVs that rely on visual sensing thus face difficult challenges, and consequently exhibit poor performance on visiondriven tasks. This paper proposes a method to improve the quality of visual underwater scenes using Generative Adversarial Networks (GANs), with the goal of improving input to vision-driven behaviors further down the autonomy pipeline. Furthermore, we show how recently proposed methods are able to generate a dataset for the purpose of such underwater image restoration. For any visually-guided underwater robots, this improvement can result in increased safety and reliability through robust visual perception. To that effect, we present quantitative and qualitative data which demonstrates that images corrected through the proposed approach generate more visually appealing images, and also provide increased accuracy for a diver tracking algorithm.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {/Users/eragon/Zotero/storage/6I3ERC7B/Fabbri et al. - 2018 - Enhancing Underwater Imagery using Generative Adve.pdf}
}

@online{fabbriEnhancingUnderwaterImagery2018a,
  title = {Enhancing {{Underwater Imagery}} Using {{Generative Adversarial Networks}}},
  author = {Fabbri, Cameron and Islam, Md Jahidul and Sattar, Junaed},
  date = {2018-01-11},
  eprint = {1801.04011},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1801.04011},
  urldate = {2022-11-19},
  abstract = {Autonomous underwater vehicles (AUVs) rely on a variety of sensors – acoustic, inertial and visual – for intelligent decision making. Due to its non-intrusive, passive nature, and high information content, vision is an attractive sensing modality, particularly at shallower depths. However, factors such as light refraction and absorption, suspended particles in the water, and color distortion affect the quality of visual data, resulting in noisy and distorted images. AUVs that rely on visual sensing thus face difficult challenges, and consequently exhibit poor performance on visiondriven tasks. This paper proposes a method to improve the quality of visual underwater scenes using Generative Adversarial Networks (GANs), with the goal of improving input to vision-driven behaviors further down the autonomy pipeline. Furthermore, we show how recently proposed methods are able to generate a dataset for the purpose of such underwater image restoration. For any visually-guided underwater robots, this improvement can result in increased safety and reliability through robust visual perception. To that effect, we present quantitative and qualitative data which demonstrates that images corrected through the proposed approach generate more visually appealing images, and also provide increased accuracy for a diver tracking algorithm.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {/Users/eragon/Zotero/storage/UDWWMTHM/Fabbri et al. - 2018 - Enhancing Underwater Imagery using Generative Adve.pdf}
}

@inproceedings{fawziAdaptiveDataAugmentation2016,
  title = {Adaptive Data Augmentation for Image Classification},
  booktitle = {2016 {{IEEE International Conference}} on {{Image Processing}} ({{ICIP}})},
  author = {Fawzi, Alhussein and Samulowitz, Horst and Turaga, Deepak and Frossard, Pascal},
  date = {2016-09},
  pages = {3688--3692},
  issn = {2381-8549},
  doi = {10.1109/ICIP.2016.7533048},
  abstract = {Data augmentation is the process of generating samples by transforming training data, with the target of improving the accuracy and robustness of classifiers. In this paper, we propose a new automatic and adaptive algorithm for choosing the transformations of the samples used in data augmentation. Specifically, for each sample, our main idea is to seek a small transformation that yields maximal classification loss on the transformed sample. We employ a trust-region optimization strategy, which consists of solving a sequence of linear programs. Our data augmentation scheme is then integrated into a Stochastic Gradient Descent algorithm for training deep neural networks. We perform experiments on two datasets, and show that that the proposed scheme outperforms random data augmentation algorithms in terms of accuracy and robustness, while yielding comparable or superior results with respect to existing selective sampling approaches.},
  eventtitle = {2016 {{IEEE International Conference}} on {{Image Processing}} ({{ICIP}})},
  keywords = {Approximation algorithms,Data augmentation,image robustness,Neural networks,Optimization,Robustness,Training,Training data,transformation invariance,Transforms,trust-region optimization},
  file = {/Users/eragon/Zotero/storage/F8UQDTLN/Fawzi et al. - 2016 - Adaptive data augmentation for image classificatio.pdf}
}

@article{feffermanTestingManifoldHypothesis2016,
  title = {Testing the Manifold Hypothesis},
  author = {Fefferman, Charles and Mitter, Sanjoy and Narayanan, Hariharan},
  date = {2016-02-09},
  journaltitle = {Journal of the American Mathematical Society},
  shortjournal = {J. Amer. Math. Soc.},
  volume = {29},
  number = {4},
  pages = {983--1049},
  issn = {0894-0347, 1088-6834},
  doi = {10.1090/jams/852},
  url = {https://www.ams.org/jams/2016-29-04/S0894-0347-2016-00852-4/},
  urldate = {2022-11-15},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/EISPVQII/Fefferman et al. - 2016 - Testing the manifold hypothesis.pdf}
}

@inproceedings{fongInterpretableExplanationsBlack2017,
  title = {Interpretable {{Explanations}} of {{Black Boxes}} by {{Meaningful Perturbation}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Fong, Ruth and Vedaldi, Andrea},
  date = {2017-10},
  eprint = {1704.03296},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  pages = {3449--3457},
  doi = {10.1109/ICCV.2017.371},
  url = {http://arxiv.org/abs/1704.03296},
  urldate = {2023-03-24},
  abstract = {As machine learning algorithms are increasingly applied to high impact yet high risk tasks, such as medical diagnosis or autonomous driving, it is critical that researchers can explain how such algorithms arrived at their predictions. In recent years, a number of image saliency methods have been developed to summarize where highly complex neural networks "look" in an image for evidence for their predictions. However, these techniques are limited by their heuristic nature and architectural constraints. In this paper, we make two main contributions: First, we propose a general framework for learning different kinds of explanations for any black box algorithm. Second, we specialise the framework to find the part of an image most responsible for a classifier decision. Unlike previous works, our method is model-agnostic and testable because it is grounded in explicit and interpretable image perturbations.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/eragon/Zotero/storage/CYN285R2/Fong and Vedaldi - 2017 - Interpretable Explanations of Black Boxes by Meani.pdf}
}

@online{fongUnderstandingDeepNetworks2019,
  title = {Understanding {{Deep Networks}} via {{Extremal Perturbations}} and {{Smooth Masks}}},
  author = {Fong, Ruth and Patrick, Mandela and Vedaldi, Andrea},
  date = {2019-10-18},
  eprint = {1910.08485},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1910.08485},
  url = {http://arxiv.org/abs/1910.08485},
  urldate = {2022-11-24},
  abstract = {The problem of attribution is concerned with identifying the parts of an input that are responsible for a model's output. An important family of attribution methods is based on measuring the effect of perturbations applied to the input. In this paper, we discuss some of the shortcomings of existing approaches to perturbation analysis and address them by introducing the concept of extremal perturbations, which are theoretically grounded and interpretable. We also introduce a number of technical innovations to compute extremal perturbations, including a new area constraint and a parametric family of smooth perturbations, which allow us to remove all tunable hyper-parameters from the optimization problem. We analyze the effect of perturbations as a function of their area, demonstrating excellent sensitivity to the spatial properties of the deep neural network under stimulation. We also extend perturbation analysis to the intermediate layers of a network. This application allows us to identify the salient channels necessary for classification, which, when visualized using feature inversion, can be used to elucidate model behavior. Lastly, we introduce TorchRay, an interpretability library built on PyTorch.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/eragon/Zotero/storage/FDMYY4EH/Fong et al. - 2019 - Understanding Deep Networks via Extremal Perturbat.pdf}
}

@online{frenchMilkingCowMaskSemiSupervised2020,
  title = {Milking {{CowMask}} for {{Semi-Supervised Image Classification}}},
  author = {French, Geoff and Oliver, Avital and Salimans, Tim},
  date = {2020-06-05},
  eprint = {2003.12022},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2003.12022},
  url = {http://arxiv.org/abs/2003.12022},
  urldate = {2023-03-31},
  abstract = {Consistency regularization is a technique for semi-supervised learning that underlies a number of strong results for classification with few labeled data. It works by encouraging a learned model to be robust to perturbations on unlabeled data. Here, we present a novel mask-based augmentation method called CowMask. Using it to provide perturbations for semi-supervised consistency regularization, we achieve a state-of-the-art result on ImageNet with 10\% labeled data, with a top-5 error of 8.76\% and top-1 error of 26.06\%. Moreover, we do so with a method that is much simpler than many alternatives. We further investigate the behavior of CowMask for semi-supervised learning by running many smaller scale experiments on the SVHN, CIFAR-10 and CIFAR-100 data sets, where we achieve results competitive with the state of the art, indicating that CowMask is widely applicable. We open source our code at https://github.com/google-research/google-research/tree/master/milking\_cowmask},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/eragon/Zotero/storage/DK7AQMSI/French et al. - 2020 - Milking CowMask for Semi-Supervised Image Classifi.pdf}
}

@article{fristonComputationalPsychiatryBrain2014,
  title = {Computational Psychiatry: The Brain as a Phantastic Organ},
  shorttitle = {Computational Psychiatry},
  author = {Friston, Karl J and Stephan, Klaas Enno and Montague, Read and Dolan, Raymond J},
  date = {2014-07},
  journaltitle = {The Lancet Psychiatry},
  shortjournal = {The Lancet Psychiatry},
  volume = {1},
  number = {2},
  pages = {148--158},
  issn = {22150366},
  doi = {10.1016/S2215-0366(14)70275-5},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S2215036614702755},
  urldate = {2022-05-27},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/LMCF7GH2/Friston et al. - 2014 - Computational psychiatry the brain as a phantasti.pdf}
}

@online{ganVisionLanguagePretrainingBasics2022,
  title = {Vision-{{Language Pre-training}}: {{Basics}}, {{Recent Advances}}, and {{Future Trends}}},
  shorttitle = {Vision-{{Language Pre-training}}},
  author = {Gan, Zhe and Li, Linjie and Li, Chunyuan and Wang, Lijuan and Liu, Zicheng and Gao, Jianfeng},
  date = {2022-10-17},
  eprint = {2210.09263},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2210.09263},
  url = {http://arxiv.org/abs/2210.09263},
  urldate = {2022-11-19},
  abstract = {This paper surveys vision-language pre-training (VLP) methods for multimodal intelligence that have been developed in the last few years. We group these approaches into three categories: (\$i\$) VLP for image-text tasks, such as image captioning, image-text retrieval, visual question answering, and visual grounding; (\$ii\$) VLP for core computer vision tasks, such as (open-set) image classification, object detection, and segmentation; and (\$iii\$) VLP for video-text tasks, such as video captioning, video-text retrieval, and video question answering. For each category, we present a comprehensive review of state-of-the-art methods, and discuss the progress that has been made and challenges still being faced, using specific systems and models as case studies. In addition, for each category, we discuss advanced topics being actively explored in the research community, such as big foundation models, unified modeling, in-context few-shot learning, knowledge, robustness, and computer vision in the wild, to name a few.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/eragon/Zotero/storage/HSY8C64S/Gan et al. - 2022 - Vision-Language Pre-training Basics, Recent Advan.pdf}
}

@online{ganVisionLanguagePretrainingBasics2022a,
  title = {Vision-{{Language Pre-training}}: {{Basics}}, {{Recent Advances}}, and {{Future Trends}}},
  shorttitle = {Vision-{{Language Pre-training}}},
  author = {Gan, Zhe and Li, Linjie and Li, Chunyuan and Wang, Lijuan and Liu, Zicheng and Gao, Jianfeng},
  date = {2022-10-17},
  eprint = {2210.09263},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2210.09263},
  urldate = {2022-11-19},
  abstract = {This paper surveys vision-language pre-training (VLP) methods for multimodal intelligence that have been developed in the last few years. We group these approaches into three categories: (i) VLP for image-text tasks, such as image captioning, image-text retrieval, visual question answering, and visual grounding; (ii) VLP for core computer vision tasks, such as (open-set) image classification, object detection, and segmentation; and (iii) VLP for video-text tasks, such as video captioning, video-text retrieval, and video question answering. For each category, we present a comprehensive review of state-of-the-art methods, and discuss the progress that has been made and challenges still being faced, using specific systems and models as case studies. In addition, for each category, we discuss advanced topics being actively explored in the research community, such as big foundation models, unified modeling, in-context few-shot learning, knowledge, robustness, and computer vision in the wild, to name a few.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/eragon/Zotero/storage/RZ5EPBRS/Gan et al. - 2022 - Vision-Language Pre-training Basics, Recent Advan.pdf}
}

@online{ganVisionLanguagePretrainingBasics2022b,
  title = {Vision-{{Language Pre-training}}: {{Basics}}, {{Recent Advances}}, and {{Future Trends}}},
  shorttitle = {Vision-{{Language Pre-training}}},
  author = {Gan, Zhe and Li, Linjie and Li, Chunyuan and Wang, Lijuan and Liu, Zicheng and Gao, Jianfeng},
  date = {2022-10-17},
  eprint = {2210.09263},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2210.09263},
  urldate = {2022-11-19},
  abstract = {This paper surveys vision-language pre-training (VLP) methods for multimodal intelligence that have been developed in the last few years. We group these approaches into three categories: (i) VLP for image-text tasks, such as image captioning, image-text retrieval, visual question answering, and visual grounding; (ii) VLP for core computer vision tasks, such as (open-set) image classification, object detection, and segmentation; and (iii) VLP for video-text tasks, such as video captioning, video-text retrieval, and video question answering. For each category, we present a comprehensive review of state-of-the-art methods, and discuss the progress that has been made and challenges still being faced, using specific systems and models as case studies. In addition, for each category, we discuss advanced topics being actively explored in the research community, such as big foundation models, unified modeling, in-context few-shot learning, knowledge, robustness, and computer vision in the wild, to name a few.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/eragon/Zotero/storage/WXMFAJ53/Gan et al. - 2022 - Vision-Language Pre-training Basics, Recent Advan.pdf}
}

@online{gaoGoingXAISystematic2022,
  title = {Going {{Beyond XAI}}: {{A Systematic Survey}} for {{Explanation-Guided Learning}}},
  shorttitle = {Going {{Beyond XAI}}},
  author = {Gao, Yuyang and Gu, Siyi and Jiang, Junji and Hong, Sungsoo Ray and Yu, Dazhou and Zhao, Liang},
  date = {2022-12-07},
  eprint = {2212.03954},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2212.03954},
  url = {http://arxiv.org/abs/2212.03954},
  urldate = {2023-05-19},
  abstract = {As the societal impact of Deep Neural Networks (DNNs) grows, the goals for advancing DNNs become more complex and diverse, ranging from improving a conventional model accuracy metric to infusing advanced human virtues such as fairness, accountability, transparency (FaccT), and unbiasedness. Recently, techniques in Explainable Artificial Intelligence (XAI) are attracting considerable attention, and have tremendously helped Machine Learning (ML) engineers in understanding AI models. However, at the same time, we started to witness the emerging need beyond XAI among AI communities; based on the insights learned from XAI, how can we better empower ML engineers in steering their DNNs so that the model's reasonableness and performance can be improved as intended? This article provides a timely and extensive literature overview of the field Explanation-Guided Learning (EGL), a domain of techniques that steer the DNNs' reasoning process by adding regularization, supervision, or intervention on model explanations. In doing so, we first provide a formal definition of EGL and its general learning paradigm. Secondly, an overview of the key factors for EGL evaluation, as well as summarization and categorization of existing evaluation procedures and metrics for EGL are provided. Finally, the current and potential future application areas and directions of EGL are discussed, and an extensive experimental study is presented aiming at providing comprehensive comparative studies among existing EGL models in various popular application domains, such as Computer Vision (CV) and Natural Language Processing (NLP) domains.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  file = {/Users/eragon/Zotero/storage/I4A6RZQ8/Gao et al. - 2022 - Going Beyond XAI A Systematic Survey for Explanat.pdf}
}

@online{geContributionsShapeTexture2022,
  title = {Contributions of {{Shape}}, {{Texture}}, and {{Color}} in {{Visual Recognition}}},
  author = {Ge, Yunhao and Xiao, Yao and Xu, Zhi and Wang, Xingrui and Itti, Laurent},
  date = {2022-07-19},
  eprint = {2207.09510},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2207.09510},
  urldate = {2023-02-20},
  abstract = {We investigate the contributions of three important features of the human visual system (HVS)\textasciitilde{} -- \textasciitilde shape, texture, and color \textasciitilde{} -- \textasciitilde to object classification. We build a humanoid vision engine (HVE) that explicitly and separately computes shape, texture, and color features from images. The resulting feature vectors are then concatenated to support the final classification. We show that HVE can summarize and rank-order the contributions of the three features to object recognition. We use human experiments to confirm that both HVE and humans predominantly use some specific features to support the classification of specific classes (e.g., texture is the dominant feature to distinguish a zebra from other quadrupeds, both for humans and HVE). With the help of HVE, given any environment (dataset), we can summarize the most important features for the whole task (task-specific; e.g., color is the most important feature overall for classification with the CUB dataset), and for each class (class-specific; e.g., shape is the most important feature to recognize boats in the iLab-20M dataset). To demonstrate more usefulness of HVE, we use it to simulate the open-world zero-shot learning ability of humans with no attribute labeling. Finally, we show that HVE can also simulate human imagination ability with the combination of different features. We will open-source the HVE engine and corresponding datasets.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/eragon/Zotero/storage/Y5V4GGGH/Ge et al. - 2022 - Contributions of Shape, Texture, and Color in Visu.pdf}
}

@unpublished{geirhosImageNettrainedCNNsAre2019,
  title = {{{ImageNet-trained CNNs}} Are Biased towards Texture; Increasing Shape Bias Improves Accuracy and Robustness},
  author = {Geirhos, Robert and Rubisch, Patricia and Michaelis, Claudio and Bethge, Matthias and Wichmann, Felix A. and Brendel, Wieland},
  date = {2019-01-14},
  eprint = {1811.12231},
  eprinttype = {arxiv},
  eprintclass = {cs, q-bio, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1811.12231},
  url = {http://arxiv.org/abs/1811.12231},
  urldate = {2022-09-06},
  abstract = {Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes. Some recent studies suggest a more important role of image textures. We here put these conflicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conflict. We show that ImageNet-trained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classification strategies. We then demonstrate that the same standard architecture (ResNet-50) that learns a texture-based representation on ImageNet is able to learn a shape-based representation instead when trained on "Stylized-ImageNet", a stylized version of ImageNet. This provides a much better fit for human behavioural performance in our well-controlled psychophysical lab setting (nine experiments totalling 48,560 psychophysical trials across 97 observers) and comes with a number of unexpected emergent benefits such as improved object detection performance and previously unseen robustness towards a wide range of image distortions, highlighting advantages of a shape-based representation.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  file = {/Users/eragon/Zotero/storage/7C8DHLXI/Geirhos et al. - 2019 - ImageNet-trained CNNs are biased towards texture\; .pdf}
}

@online{ghorbaniInterpretationNeuralNetworks2018,
  title = {Interpretation of {{Neural Networks}} Is {{Fragile}}},
  author = {Ghorbani, Amirata and Abid, Abubakar and Zou, James},
  date = {2018-11-06},
  eprint = {1710.10547},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1710.10547},
  url = {http://arxiv.org/abs/1710.10547},
  urldate = {2022-11-18},
  abstract = {In order for machine learning to be deployed and trusted in many applications, it is crucial to be able to reliably explain why the machine learning algorithm makes certain predictions. For example, if an algorithm classifies a given pathology image to be a malignant tumor, then the doctor may need to know which parts of the image led the algorithm to this classification. How to interpret black-box predictors is thus an important and active area of research. A fundamental question is: how much can we trust the interpretation itself? In this paper, we show that interpretation of deep learning predictions is extremely fragile in the following sense: two perceptively indistinguishable inputs with the same predicted label can be assigned very different interpretations. We systematically characterize the fragility of several widely-used feature-importance interpretation methods (saliency maps, relevance propagation, and DeepLIFT) on ImageNet and CIFAR-10. Our experiments show that even small random perturbation can change the feature importance and new systematic perturbations can lead to dramatically different interpretations without changing the label. We extend these results to show that interpretations based on exemplars (e.g. influence functions) are similarly fragile. Our analysis of the geometry of the Hessian matrix gives insight on why fragility could be a fundamental challenge to the current interpretation approaches.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/eragon/Zotero/storage/K7RINCR7/Ghorbani et al. - 2018 - Interpretation of Neural Networks is Fragile.pdf}
}

@article{gianferraraCognitiveMotorSkill2021,
  title = {Cognitive \& Motor Skill Transfer across Speeds: {{A}} Video Game Study},
  shorttitle = {Cognitive \& Motor Skill Transfer across Speeds},
  author = {Gianferrara, Pierre Giovanni and Betts, Shawn and Anderson, John Robert},
  editor = {Goldwater, Micah B.},
  date = {2021-10-12},
  journaltitle = {PLOS ONE},
  shortjournal = {PLoS ONE},
  volume = {16},
  number = {10},
  pages = {e0258242},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0258242},
  url = {https://dx.plos.org/10.1371/journal.pone.0258242},
  urldate = {2023-01-25},
  abstract = {We examined the detailed behavioral characteristics of transfer of skill and the ability of the adaptive control of thought rational (ACT-R) architecture to account for this with its new Controller module. We employed a simple action video game called Auto Orbit and investigated the control tuning of timing skills across speed perturbations of the environment. In Auto Orbit, players needed to learn to alternate turn and shot actions to blow and burst balloons under time constraints imposed by balloon resets and deflations. Cognitive and motor skill transfer was assessed both in terms of game performance and in terms of the details of their motor actions. We found that skill transfer across speeds necessitated the recalibration of action timing skills. In addition, we found that acquiring skill in Auto Orbit involved a progressive decrease in variability of behavior. Finally, we found that players with higher skill levels tended to be less variable in terms of action chunking and action timing. These findings further shed light on the complex cognitive and motor mechanisms of skill transfer across speeds in complex task environments.},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/6K6H7DM8/Gianferrara et al. - 2021 - Cognitive & motor skill transfer across speeds A .pdf}
}

@online{gilpinExplainingExplanationsOverview2019,
  title = {Explaining {{Explanations}}: {{An Overview}} of {{Interpretability}} of {{Machine Learning}}},
  shorttitle = {Explaining {{Explanations}}},
  author = {Gilpin, Leilani H. and Bau, David and Yuan, Ben Z. and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
  date = {2019-02-03},
  eprint = {1806.00069},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1806.00069},
  url = {http://arxiv.org/abs/1806.00069},
  urldate = {2022-11-24},
  abstract = {There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we provide our definition of explainability and show how it can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/eragon/Zotero/storage/MZWDDSBU/Gilpin et al. - 2019 - Explaining Explanations An Overview of Interpreta.pdf}
}

@online{gilpinExplanationNotTechnical2022,
  title = {"{{Explanation}}" Is {{Not}} a {{Technical Term}}: {{The Problem}} of {{Ambiguity}} in {{XAI}}},
  shorttitle = {"{{Explanation}}" Is {{Not}} a {{Technical Term}}},
  author = {Gilpin, Leilani H. and Paley, Andrew R. and Alam, Mohammed A. and Spurlock, Sarah and Hammond, Kristian J.},
  date = {2022-06-27},
  eprint = {2207.00007},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2207.00007},
  url = {http://arxiv.org/abs/2207.00007},
  urldate = {2023-05-19},
  abstract = {There is broad agreement that Artificial Intelligence (AI) systems, particularly those using Machine Learning (ML), should be able to "explain" their behavior. Unfortunately, there is little agreement as to what constitutes an "explanation." This has caused a disconnect between the explanations that systems produce in service of explainable Artificial Intelligence (XAI) and those explanations that users and other audiences actually need, which should be defined by the full spectrum of functional roles, audiences, and capabilities for explanation. In this paper, we explore the features of explanations and how to use those features in evaluating their utility. We focus on the requirements for explanations defined by their functional role, the knowledge states of users who are trying to understand them, and the availability of the information needed to generate them. Further, we discuss the risk of XAI enabling trust in systems without establishing their trustworthiness and define a critical next step for the field of XAI to establish metrics to guide and ground the utility of system-generated explanations.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction},
  file = {/Users/eragon/Zotero/storage/7W7JQQ3V/Gilpin et al. - 2022 - Explanation is Not a Technical Term The Problem.pdf}
}

@inproceedings{gongKeepAugmentSimpleInformationPreserving2021,
  title = {{{KeepAugment}}: {{A Simple Information-Preserving Data Augmentation Approach}}},
  shorttitle = {{{KeepAugment}}},
  booktitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Gong, Chengyue and Wang, Dilin and Li, Meng and Chandra, Vikas and Liu, Qiang},
  date = {2021-06},
  pages = {1055--1064},
  publisher = {{IEEE}},
  location = {{Nashville, TN, USA}},
  doi = {10.1109/CVPR46437.2021.00111},
  url = {https://ieeexplore.ieee.org/document/9578546/},
  urldate = {2023-05-08},
  abstract = {Data augmentation (DA) is an essential technique for training state-of-the-art deep learning systems. In this paper, we empirically show that the standard data augmentation methods may introduce distribution shift and consequently hurt the performance on unaugmented data during inference. To alleviate this issue, we propose a simple yet effective approach, dubbed KeepAugment, to increase the fidelity of augmented images. The idea is to use the saliency map to detect important regions on the original images and preserve these informative regions during augmentation. This information-preserving strategy allows us to generate more faithful training examples. Empirically, we demonstrate that our method significantly improves upon a number of prior art data augmentation schemes, e.g. AutoAugment, Cutout, random erasing, achieving promising results on image classification, semi-supervised image classification, multi-view multi-camera tracking and object detection.},
  eventtitle = {2021 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-66544-509-2},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/FQJIJTCL/Gong et al. - 2021 - KeepAugment A Simple Information-Preserving Data .pdf}
}

@inproceedings{gongReshapingVisualDatasets2013,
  title = {Reshaping {{Visual Datasets}} for {{Domain Adaptation}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Gong, Boqing and Grauman, Kristen and Sha, Fei},
  date = {2013},
  volume = {26},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2013/hash/2291d2ec3b3048d1a6f86c2c4591b7e0-Abstract.html},
  urldate = {2022-09-06},
  abstract = {In visual recognition problems, the common data distribution mismatches between training and testing make domain adaptation essential. However, image data is difficult to manually divide into the discrete domains required by adaptation algorithms, and the standard practice of equating datasets with domains is a weak proxy for all the real conditions that alter the statistics in complex ways (lighting, pose, background, resolution, etc.) We propose an approach to automatically discover latent domains in image or video datasets. Our formulation imposes two key properties on domains: maximum distinctiveness and maximum learnability. By maximum distinctiveness, we require the underlying distributions of the identified domains to be different from each other; by maximum learnability, we ensure that a strong discriminative model can be learned from the domain. We devise a nonparametric representation and efficient optimization procedure for distinctiveness, which, when coupled with our learnability constraint, can successfully discover domains among both training and test data. We extensively evaluate our approach on object recognition and human activity recognition tasks.},
  file = {/Users/eragon/Zotero/storage/2STZZQCC/Gong et al. - 2013 - Reshaping Visual Datasets for Domain Adaptation.pdf}
}

@online{gozalo-brizuelaChatGPTNotAll2023,
  title = {{{ChatGPT}} Is Not All You Need. {{A State}} of the {{Art Review}} of Large {{Generative AI}} Models},
  author = {Gozalo-Brizuela, Roberto and Garrido-Merchan, Eduardo C.},
  date = {2023-01-11},
  eprint = {2301.04655},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2301.04655},
  urldate = {2023-01-23},
  abstract = {During the last two years there has been a plethora of large generative models such as ChatGPT or Stable Diffusion that have been published. Concretely, these models are able to perform tasks such as being a general question and answering system or automatically creating artistic images that are revolutionizing several sectors. Consequently, the implications that these generative models have in the industry and society are enormous, as several job positions may be transformed. For example, Generative AI is capable of transforming effectively and creatively texts to images, like the DALLE-2 model; text to 3D images, like the Dreamfusion model; images to text, like the Flamingo model; texts to video, like the Phenaki model; texts to audio, like the AudioLM model; texts to other texts, like ChatGPT; texts to code, like the Codex model; texts to scientific texts, like the Galactica model or even create algorithms like AlphaTensor. This work consists on an attempt to describe in a concise way the main models are sectors that are affected by generative AI and to provide a taxonomy of the main generative models published recently.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/eragon/Zotero/storage/EV3Q5LBP/Gozalo-Brizuela and Garrido-Merchan - 2023 - ChatGPT is not all you need. A State of the Art Re.pdf}
}

@online{gozalo-brizuelaChatGPTNotAll2023a,
  title = {{{ChatGPT}} Is Not All You Need. {{A State}} of the {{Art Review}} of Large {{Generative AI}} Models},
  author = {Gozalo-Brizuela, Roberto and Garrido-Merchan, Eduardo C.},
  date = {2023-01-11},
  eprint = {2301.04655},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2301.04655},
  urldate = {2023-01-23},
  abstract = {During the last two years there has been a plethora of large generative models such as ChatGPT or Stable Diffusion that have been published. Concretely, these models are able to perform tasks such as being a general question and answering system or automatically creating artistic images that are revolutionizing several sectors. Consequently, the implications that these generative models have in the industry and society are enormous, as several job positions may be transformed. For example, Generative AI is capable of transforming effectively and creatively texts to images, like the DALLE-2 model; text to 3D images, like the Dreamfusion model; images to text, like the Flamingo model; texts to video, like the Phenaki model; texts to audio, like the AudioLM model; texts to other texts, like ChatGPT; texts to code, like the Codex model; texts to scientific texts, like the Galactica model or even create algorithms like AlphaTensor. This work consists on an attempt to describe in a concise way the main models are sectors that are affected by generative AI and to provide a taxonomy of the main generative models published recently.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/eragon/Zotero/storage/MEGIZKBE/Gozalo-Brizuela and Garrido-Merchan - 2023 - ChatGPT is not all you need. A State of the Art Re.pdf}
}

@online{gozalo-brizuelaChatGPTNotAll2023b,
  title = {{{ChatGPT}} Is Not All You Need. {{A State}} of the {{Art Review}} of Large {{Generative AI}} Models},
  author = {Gozalo-Brizuela, Roberto and Garrido-Merchan, Eduardo C.},
  date = {2023-01-11},
  eprint = {2301.04655},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2301.04655},
  urldate = {2023-01-23},
  abstract = {During the last two years there has been a plethora of large generative models such as ChatGPT or Stable Diffusion that have been published. Concretely, these models are able to perform tasks such as being a general question and answering system or automatically creating artistic images that are revolutionizing several sectors. Consequently, the implications that these generative models have in the industry and society are enormous, as several job positions may be transformed. For example, Generative AI is capable of transforming effectively and creatively texts to images, like the DALLE-2 model; text to 3D images, like the Dreamfusion model; images to text, like the Flamingo model; texts to video, like the Phenaki model; texts to audio, like the AudioLM model; texts to other texts, like ChatGPT; texts to code, like the Codex model; texts to scientific texts, like the Galactica model or even create algorithms like AlphaTensor. This work consists on an attempt to describe in a concise way the main models are sectors that are affected by generative AI and to provide a taxonomy of the main generative models published recently.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/eragon/Zotero/storage/Q7RHEHTD/Gozalo-Brizuela and Garrido-Merchan - 2023 - ChatGPT is not all you need. A State of the Art Re.pdf}
}

@article{graesserRunningHeadINTELLIGENT,
  title = {Running Head: {{INTELLIGENT TUTORING SYSTEMS}}},
  author = {Graesser, Arthur C and Conley, Mark W and Olney, Andrew},
  pages = {57},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/GQM9KMXS/Graesser et al. - Running head INTELLIGENT TUTORING SYSTEMS.pdf}
}

@article{gunningExplainableArtificialIntelligence,
  title = {Explainable {{Artificial Intelligence}} ({{XAI}})},
  author = {Gunning, David},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/C9PIY8TT/Gunning - Explainable Artificial Intelligence (XAI).pdf}
}

@article{han3D2SeqViewsAggregatingSequential2019,
  title = {{{3D2SeqViews}}: {{Aggregating Sequential Views}} for {{3D Global Feature Learning}} by {{CNN With Hierarchical Attention Aggregation}}},
  shorttitle = {{{3D2SeqViews}}},
  author = {Han, Zhizhong and Lu, Honglei and Liu, Zhenbao and Vong, Chi-Man and Liu, Yu-Shen and Zwicker, Matthias and Han, Junwei and Chen, C. L. Philip},
  date = {2019-08},
  journaltitle = {IEEE Transactions on Image Processing},
  shortjournal = {IEEE Trans. on Image Process.},
  volume = {28},
  number = {8},
  pages = {3986--3999},
  issn = {1057-7149, 1941-0042},
  doi = {10.1109/TIP.2019.2904460},
  url = {https://ieeexplore.ieee.org/document/8666059/},
  urldate = {2022-05-10},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/LMY7ERRG/Han et al. - 2019 - 3D2SeqViews Aggregating Sequential Views for 3D G.pdf}
}

@article{hanChangeVariableForeperiodEffects2022,
  title = {Change of {{Variable-Foreperiod Effects}} within an {{Experiment}}: {{A Bayesian Modeling Approach}}},
  shorttitle = {Change of {{Variable-Foreperiod Effects}} within an {{Experiment}}},
  author = {Han, Tianfang and Proctor, Robert W.},
  date = {2022-07-13},
  journaltitle = {Journal of Cognition},
  volume = {5},
  number = {1},
  pages = {40},
  publisher = {{Ubiquity Press}},
  issn = {2514-4820},
  doi = {10.5334/joc.235},
  url = {http://www.journalofcognition.org/articles/10.5334/joc.235/},
  urldate = {2023-01-25},
  abstract = {The framework of binding and retrieval in action control (BRAC) by Frings et al. (2020) proposed that repetition of any element in the previous trial triggers the retrieval of other elements in the same event file. Consistent with this framework, Los et al. (2014) argued that the temporal relation between the warning signal and the target stimulus on a trial is stored in a distinct memory trace (or, event file). Retrieval of the preceding memory trace, which is triggered by perceiving the same warning signal, leads to sequential foreperiod (SFP) effect. We modeled the data from four experiments using a Bayesian method to investigate whether the SFP effect changes over time. Results of Experiments 1, 3 and 4 support the multiple trace theory of preparation, which predicts an asymmetric sequential foreperiod effect, whereas those of Experiment 2 (extremely short foreperiods) support the repetition priming account by Capizzi et al. (2015). Moreover, the significance of the parameters showed that the asymmetry in Experiments 1 and 3 (non-aging distribution) developed gradually, whereas in Experiment 4 (uniform distribution), this asymmetry was significant from the beginning and did not change over time. Implications of these findings for temporal preparation models and BRAC framework were discussed.},
  issue = {1},
  langid = {english},
  keywords = {Bayesian modeling,foreperiod,multiple trace theory,non-aging foreperiod distribution,temporal preparation},
  file = {/Users/eragon/Zotero/storage/9XTPBQ8B/Han and Proctor - 2022 - Change of Variable-Foreperiod Effects within an Ex.pdf}
}

@article{hanRevisitingVariableforeperiodEffects2022,
  title = {Revisiting Variable-Foreperiod Effects: Evaluating the Repetition Priming Account},
  shorttitle = {Revisiting Variable-Foreperiod Effects},
  author = {Han, Tianfang and Proctor, Robert W.},
  date = {2022-05-01},
  journaltitle = {Attention, Perception, \& Psychophysics},
  shortjournal = {Atten Percept Psychophys},
  volume = {84},
  number = {4},
  pages = {1193--1207},
  issn = {1943-393X},
  doi = {10.3758/s13414-022-02476-5},
  url = {https://doi.org/10.3758/s13414-022-02476-5},
  urldate = {2023-01-25},
  abstract = {A warning signal preceding an imperative stimulus by a certain foreperiod can accelerate responses (foreperiod effect). When foreperiod is varied within a block, the foreperiod effect on reaction time (RT) is modulated by both the current and the prior foreperiods. Using a non-aging foreperiod distribution in a simple-reaction task, Capizzi et al. (Cognition, 134, 39-49, 2015) found equal sequential effects for different foreperiods, which they credited to repetition priming. The multiple-trace theory of Los et al. (Frontiers in Psychology, 5, Article 1058, 2014) attributes the slope of the foreperiod-RT function to the foreperiod distribution. We conducted three experiments that examined these predicted relations. Experiment 1 tested Capizzi et al.’s prediction in a choice-reaction task and found an increasing foreperiod-RT function but a larger sequential effect at the shorter foreperiod. Experiment 2 used two distinct short foreperiods with the same foreperiod distribution and found a decreasing foreperiod-RT function. By increasing the difference between the foreperiods used in Experiment 2, Experiment 3 yielded a larger sequential effect overall. The experiments provide evidence that, with a non-aging foreperiod distribution, the variable-foreperiod paradigm yields unequal sequential-effect sizes at the different foreperiods, consistent with the multiple-trace theory but contrary to Capizzi et al.’s repetition-priming account. The foreperiod-RT functions are similar to those of the fixed-foreperiod paradigm, which is not predicted by the multiple trace theory.},
  langid = {english},
  keywords = {Foreperiod,Non-aging foreperiod distribution,Repetition priming,Variable-foreperiod effect},
  file = {/Users/eragon/Zotero/storage/B2ESZKTL/Han and Proctor - 2022 - Revisiting variable-foreperiod effects evaluating.pdf}
}

@inproceedings{heBagTricksImage2019,
  title = {Bag of {{Tricks}} for {{Image Classification}} with {{Convolutional Neural Networks}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, Tong and Zhang, Zhi and Zhang, Hang and Zhang, Zhongyue and Xie, Junyuan and Li, Mu},
  date = {2019-06},
  pages = {558--567},
  publisher = {{IEEE}},
  location = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.00065},
  url = {https://ieeexplore.ieee.org/document/8954382/},
  urldate = {2023-02-28},
  abstract = {Much of the recent progress made in image classification research can be credited to training procedure refinements, such as changes in data augmentations and optimization methods. In the literature, however, most refinements are either briefly mentioned as implementation details or only visible in source code. In this paper, we will examine a collection of such refinements and empirically evaluate their impact on the final model accuracy through ablation study. We will show that, by combining these refinements together, we are able to improve various CNN models significantly. For example, we raise ResNet-50’s top-1 validation accuracy from 75.3\% to 79.29\% on ImageNet. We will also demonstrate that improvement on image classification accuracy leads to better transfer learning performance in other application domains such as object detection and semantic segmentation.},
  eventtitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72813-293-8},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/HZJMP92G/He et al. - 2019 - Bag of Tricks for Image Classification with Convol.pdf}
}

@inproceedings{heDeepResidualLearning2016,
  title = {Deep Residual Learning for Image Recognition},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  date = {2016},
  pages = {770--778}
}

@online{hendrycksAugMixSimpleData2020,
  title = {{{AugMix}}: {{A Simple Data Processing Method}} to {{Improve Robustness}} and {{Uncertainty}}},
  shorttitle = {{{AugMix}}},
  author = {Hendrycks, Dan and Mu, Norman and Cubuk, Ekin D. and Zoph, Barret and Gilmer, Justin and Lakshminarayanan, Balaji},
  date = {2020-02-17},
  eprint = {1912.02781},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1912.02781},
  urldate = {2023-01-16},
  abstract = {Modern deep neural networks can achieve high accuracy when the training distribution and test distribution are identically distributed, but this assumption is frequently violated in practice. When the train and test distributions are mismatched, accuracy can plummet. Currently there are few techniques that improve robustness to unforeseen data shifts encountered during deployment. In this work, we propose a technique to improve the robustness and uncertainty estimates of image classifiers. We propose AUGMIX, a data processing technique that is simple to implement, adds limited computational overhead, and helps models withstand unforeseen corruptions. AUGMIX significantly improves robustness and uncertainty measures on challenging image classification benchmarks, closing the gap between previous methods and the best possible performance in some cases by more than half.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/eragon/Zotero/storage/W3F6Q299/Hendrycks et al. - 2020 - AugMix A Simple Data Processing Method to Improve.pdf}
}

@article{herlambangModelingMotivationUsing2021,
  title = {Modeling Motivation Using Goal Competition in Mental Fatigue Studies},
  author = {Herlambang, Mega B. and Taatgen, Niels A. and Cnossen, Fokie},
  date = {2021-06-01},
  journaltitle = {Journal of Mathematical Psychology},
  shortjournal = {Journal of Mathematical Psychology},
  volume = {102},
  pages = {102540},
  issn = {0022-2496},
  doi = {10.1016/j.jmp.2021.102540},
  url = {https://www.sciencedirect.com/science/article/pii/S0022249621000316},
  urldate = {2023-01-25},
  abstract = {Motivation can counteract the effects of mental fatigue. However, the underlying mechanism by which motivation affects performance in mentally fatiguing tasks is obscure. In this paper, we propose goal competition as a paradigm to understand the role of motivation and built three models of mental fatigue studies to demonstrate the mechanism in a cognitive architecture named PRIMs. Each of these studies explored the impact of reward and mental fatigue on performance. Overall, performance decreased in nonreward conditions but remained stable in reward conditions. The comparisons between our models and empirical data showed that our models were able to capture human performance. We managed to model changes in performance levels by adjusting the value of the main task goals, which controls the competition with distractions. In all the tasks modeled, the best model fits were obtained by a linear decrease in goal activation, suggesting this is a general pattern. We discuss possible mechanisms for activation decrease, and the potential of goal competition to model motivation.},
  langid = {english},
  keywords = {Cognitive architecture,Cognitive modeling,Goal competition,Mental fatigue,Motivation,PRIMs},
  file = {/Users/eragon/Zotero/storage/IHKE5RUF/Herlambang et al. - 2021 - Modeling motivation using goal competition in ment.pdf}
}

@article{herlambangModelingMotivationUsing2021a,
  title = {Modeling Motivation Using Goal Competition in Mental Fatigue Studies},
  author = {Herlambang, Mega B. and Taatgen, Niels A. and Cnossen, Fokie},
  date = {2021-06-01},
  journaltitle = {Journal of Mathematical Psychology},
  shortjournal = {Journal of Mathematical Psychology},
  volume = {102},
  pages = {102540},
  issn = {0022-2496},
  doi = {10.1016/j.jmp.2021.102540},
  url = {https://www.sciencedirect.com/science/article/pii/S0022249621000316},
  urldate = {2023-01-25},
  abstract = {Motivation can counteract the effects of mental fatigue. However, the underlying mechanism by which motivation affects performance in mentally fatiguing tasks is obscure. In this paper, we propose goal competition as a paradigm to understand the role of motivation and built three models of mental fatigue studies to demonstrate the mechanism in a cognitive architecture named PRIMs. Each of these studies explored the impact of reward and mental fatigue on performance. Overall, performance decreased in nonreward conditions but remained stable in reward conditions. The comparisons between our models and empirical data showed that our models were able to capture human performance. We managed to model changes in performance levels by adjusting the value of the main task goals, which controls the competition with distractions. In all the tasks modeled, the best model fits were obtained by a linear decrease in goal activation, suggesting this is a general pattern. We discuss possible mechanisms for activation decrease, and the potential of goal competition to model motivation.},
  langid = {english},
  keywords = {Cognitive architecture,Cognitive modeling,Goal competition,Mental fatigue,Motivation,PRIMs},
  file = {/Users/eragon/Zotero/storage/CBD2HY2M/Herlambang et al. - 2021 - Modeling motivation using goal competition in ment.pdf}
}

@online{hintonDistillingKnowledgeNeural2015,
  title = {Distilling the {{Knowledge}} in a {{Neural Network}}},
  author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  date = {2015-03-09},
  eprint = {1503.02531},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1503.02531},
  urldate = {2022-11-18},
  abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions [3]. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators [1] have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/eragon/Zotero/storage/TRICABTJ/Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf}
}

@online{hohmanSummitScalingDeep2019,
  title = {Summit: {{Scaling Deep Learning Interpretability}} by {{Visualizing Activation}} and {{Attribution Summarizations}}},
  shorttitle = {Summit},
  author = {Hohman, Fred and Park, Haekyu and Robinson, Caleb and Chau, Duen Horng},
  date = {2019-09-02},
  eprint = {1904.02323},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1904.02323},
  urldate = {2023-02-20},
  abstract = {Deep learning is increasingly used in decision-making tasks. However, understanding how neural networks produce final predictions remains a fundamental challenge. Existing work on interpreting neural network predictions for images often focuses on explaining predictions for single images or neurons. As predictions are often computed from millions of weights that are optimized over millions of images, such explanations can easily miss a bigger picture. We present Summit, an interactive system that scalably and systematically summarizes and visualizes what features a deep learning model has learned and how those features interact to make predictions. Summit introduces two new scalable summarization techniques: (1) activation aggregation discovers important neurons, and (2) neuron-influence aggregation identifies relationships among such neurons. Summit combines these techniques to create the novel attribution graph that reveals and summarizes crucial neuron associations and substructures that contribute to a model's outcomes. Summit scales to large data, such as the ImageNet dataset with 1.2M images, and leverages neural network feature visualization and dataset examples to help users distill large, complex neural network models into compact, interactive visualizations. We present neural network exploration scenarios where Summit helps us discover multiple surprising insights into a prevalent, large-scale image classifier's learned representations and informs future neural network architecture design. The Summit visualization runs in modern web browsers and is open-sourced.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  file = {/Users/eragon/Zotero/storage/CWWZYDLX/Hohman et al. - 2019 - Summit Scaling Deep Learning Interpretability by .pdf}
}

@inproceedings{houDynamicVisualAttention2008,
  title = {Dynamic Visual Attention: Searching for Coding Length Increments},
  shorttitle = {Dynamic Visual Attention},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hou, Xiaodi and Zhang, Liqing},
  date = {2008},
  volume = {21},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2008/hash/a8baa56554f96369ab93e4f3bb068c22-Abstract.html},
  urldate = {2023-02-20},
  abstract = {A visual attention system should respond placidly when common stimuli are presented, while at the same time keep alert to anomalous visual inputs. In this paper, a dynamic visual attention model based on the rarity of features is proposed. We introduce the Incremental Coding Length (ICL) to measure the perspective entropy gain of each feature. The objective of our model is to maximize the entropy of the sampled visual features. In order to optimize energy consumption, the limit amount of energy of the system is re-distributed amongst features according to their Incremental Coding Length. By selecting features with large coding length increments, the computational system can achieve attention selectivity in both static and dynamic scenes. We demonstrate that the proposed model achieves superior accuracy in comparison to mainstream approaches in static saliency map generation. Moreover, we also show that our model captures several less-reported dynamic visual search behaviors, such as attentional swing and inhibition of return.},
  file = {/Users/eragon/Zotero/storage/Z67B86V2/Hou and Zhang - 2008 - Dynamic visual attention searching for coding len.pdf}
}

@unpublished{howardMobilenetsEfficientConvolutional2017,
  title = {Mobilenets: {{Efficient}} Convolutional Neural Networks for Mobile Vision Applications},
  author = {Howard, Andrew G and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  date = {2017},
  eprint = {1704.04861},
  eprinttype = {arxiv}
}

@online{howardUniversalLanguageModel2018,
  title = {Universal {{Language Model Fine-tuning}} for {{Text Classification}}},
  author = {Howard, Jeremy and Ruder, Sebastian},
  date = {2018-05-23},
  eprint = {1801.06146},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1801.06146},
  url = {http://arxiv.org/abs/1801.06146},
  urldate = {2023-02-27},
  abstract = {Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24\% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100x more data. We open-source our pretrained models and code.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/eragon/Zotero/storage/LQBEEZTM/Howard and Ruder - 2018 - Universal Language Model Fine-tuning for Text Clas.pdf}
}

@article{huangSnapMixSemanticallyProportional2021,
  title = {{{SnapMix}}: {{Semantically Proportional Mixing}} for {{Augmenting Fine-grained Data}}},
  shorttitle = {{{SnapMix}}},
  author = {Huang, Shaoli and Wang, Xinchao and Tao, Dacheng},
  date = {2021-05-18},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {35},
  number = {2},
  pages = {1628--1636},
  issn = {2374-3468},
  doi = {10.1609/aaai.v35i2.16255},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/16255},
  urldate = {2023-03-31},
  abstract = {Data mixing augmentation has proved effective in training deep models. Recent methods mix labels mainly according to the mixture proportion of image pixels. Due to the major discriminative information of a fine-grained image usually resides in subtle regions, these methods tend to introduce heavy label noise in fine-grained recognition. We propose Semantically Proportional Mixing (SnapMix) that exploits class activation map (CAM) to lessen the label noise in augmenting fine-grained data. SnapMix generates the target label for a mixed image by estimating its intrinsic semantic composition. This strategy can adapt to asymmetric mixing operations and ensure semantic correspondence between synthetic images and target labels. Experiments show that our method consistently outperforms existing mixed-based approaches regardless of different datasets or network depths. Further, by incorporating the mid-level features, the proposed SnapMix achieves top-level performance, demonstrating its potential to serve as a strong baseline for fine-grained recognition.},
  issue = {2},
  langid = {english},
  keywords = {Classification and Regression},
  file = {/Users/eragon/Zotero/storage/HYBXSL4J/Huang et al. - 2021 - SnapMix Semantically Proportional Mixing for Augm.pdf}
}

@article{huoSURVEYMANIFOLDBASEDLEARNING,
  title = {A {{SURVEY OF MANIFOLD-BASED LEARNING METHODS}}},
  author = {Huo, Xiaoming and Smith, Andrew K},
  pages = {34},
  abstract = {We review the ideas, algorithms, and numerical performance of manifold-based machine learning and dimension reduction methods. The representative methods include locally linear embedding (LLE), ISOMAP, Laplacian eigenmaps, Hessian eigenmaps, local tangent space alignment (LTSA), and charting. We describe the insights from these developments, as well as new opportunities for both researchers and practitioners. Potential applications in image and sensor data are illustrated. This chapter is based on an invited survey presentation that was delivered by Huo at the 2004 INFORMS Annual Meeting, which was held in Denver, CO, USA.},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/29TD2Y8L/Huo and Smith - A SURVEY OF MANIFOLD-BASED LEARNING METHODS.pdf}
}

@online{huPreliminaryStudyData2019,
  title = {A {{Preliminary Study}} on {{Data Augmentation}} of {{Deep Learning}} for {{Image Classification}}},
  author = {Hu, Benlin and Lei, Cheng and Wang, Dong and Zhang, Shu and Chen, Zhenyu},
  date = {2019-06-09},
  eprint = {1906.11887},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  url = {http://arxiv.org/abs/1906.11887},
  urldate = {2023-02-01},
  abstract = {Deep learning models have a large number of free parameters that need to be calculated by effective training of the models on a great deal of training data to improve their generalization performance. However, data obtaining and labeling is expensive in practice. Data augmentation is one of the methods to alleviate this problem. In this paper, we conduct a preliminary study on how three variables (augmentation method, augmentation rate and size of basic dataset per label) can affect the accuracy of deep learning for image classification. The study provides some guidelines: (1) it is better to use transformations that alter the geometry of the images rather than those just lighting and color. (2) 2-3 times augmentation rate is good enough for training. (3) the smaller amount of data, the more obvious contributions could have.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  file = {/Users/eragon/Zotero/storage/F4BUG7UU/Hu et al. - 2019 - A Preliminary Study on Data Augmentation of Deep L.pdf}
}

@online{inoueDataAugmentationPairing2018,
  title = {Data {{Augmentation}} by {{Pairing Samples}} for {{Images Classification}}},
  author = {Inoue, Hiroshi},
  date = {2018-04-11},
  eprint = {1801.02929},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1801.02929},
  urldate = {2023-03-30},
  abstract = {Data augmentation is a widely used technique in many machine learning tasks, such as image classification, to virtually enlarge the training dataset size and avoid overfitting. Traditional data augmentation techniques for image classification tasks create new samples from the original training data by, for example, flipping, distorting, adding a small amount of noise to, or cropping a patch from an original image. In this paper, we introduce a simple but surprisingly effective data augmentation technique for image classification tasks. With our technique, named SamplePairing, we synthesize a new sample from one image by overlaying another image randomly chosen from the training data (i.e., taking an average of two images for each pixel). By using two images randomly selected from the training set, we can generate \$N\^2\$ new samples from \$N\$ training samples. This simple data augmentation technique significantly improved classification accuracy for all the tested datasets; for example, the top-1 error rate was reduced from 33.5\% to 29.0\% for the ILSVRC 2012 dataset with GoogLeNet and from 8.22\% to 6.93\% in the CIFAR-10 dataset. We also show that our SamplePairing technique largely improved accuracy when the number of samples in the training set was very small. Therefore, our technique is more valuable for tasks with a limited amount of training data, such as medical imaging tasks.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/eragon/Zotero/storage/2XLFYTRF/Inoue - 2018 - Data Augmentation by Pairing Samples for Images Cl.pdf}
}

@online{IntroductionAdvancedExplainable,
  title = {Introduction: {{Advanced Explainable AI}} for Computer Vision — {{Advanced AI}} Explainability with Pytorch-Gradcam},
  url = {https://jacobgil.github.io/pytorch-gradcam-book/introduction.html},
  urldate = {2023-05-07}
}

@inproceedings{irieModernSelfReferentialWeight2022,
  title = {A {{Modern Self-Referential Weight Matrix That Learns}} to {{Modify Itself}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Irie, Kazuki and Schlag, Imanol and Csordás, Róbert and Schmidhuber, Jürgen},
  date = {2022-06-28},
  pages = {9660--9677},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v162/irie22b.html},
  urldate = {2023-04-26},
  abstract = {The weight matrix (WM) of a neural network (NN) is its program. The programs of many traditional NNs are learned through gradient descent in some error function, then remain fixed. The WM of a self-referential NN, however, can keep rapidly modifying all of itself during runtime. In principle, such NNs can meta-learn to learn, and meta-meta-learn to meta-learn to learn, and so on, in the sense of recursive self-improvement. While NN architectures potentially capable of implementing such behaviour have been proposed since the ’90s, there have been few if any practical studies. Here we revisit such NNs, building upon recent successes of fast weight programmers and closely related linear Transformers. We propose a scalable self-referential WM (SRWM) that learns to use outer products and the delta update rule to modify itself. We evaluate our SRWM in supervised few-shot learning and in multi-task reinforcement learning with procedurally generated game environments. Our experiments demonstrate both practical applicability and competitive performance of the proposed SRWM. Our code is public.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/M5S9MQ8R/Irie et al. - 2022 - A Modern Self-Referential Weight Matrix That Learn.pdf}
}

@inproceedings{irieModernSelfReferentialWeight2022a,
  title = {A {{Modern Self-Referential Weight Matrix That Learns}} to {{Modify Itself}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Irie, Kazuki and Schlag, Imanol and Csordás, Róbert and Schmidhuber, Jürgen},
  date = {2022-06-28},
  pages = {9660--9677},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v162/irie22b.html},
  urldate = {2023-05-22},
  abstract = {The weight matrix (WM) of a neural network (NN) is its program. The programs of many traditional NNs are learned through gradient descent in some error function, then remain fixed. The WM of a self-referential NN, however, can keep rapidly modifying all of itself during runtime. In principle, such NNs can meta-learn to learn, and meta-meta-learn to meta-learn to learn, and so on, in the sense of recursive self-improvement. While NN architectures potentially capable of implementing such behaviour have been proposed since the ’90s, there have been few if any practical studies. Here we revisit such NNs, building upon recent successes of fast weight programmers and closely related linear Transformers. We propose a scalable self-referential WM (SRWM) that learns to use outer products and the delta update rule to modify itself. We evaluate our SRWM in supervised few-shot learning and in multi-task reinforcement learning with procedurally generated game environments. Our experiments demonstrate both practical applicability and competitive performance of the proposed SRWM. Our code is public.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/XXCSHEM7/Irie et al. - 2022 - A Modern Self-Referential Weight Matrix That Learn.pdf}
}

@online{isolaImagetoImageTranslationConditional2018,
  title = {Image-to-{{Image Translation}} with {{Conditional Adversarial Networks}}},
  author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
  date = {2018-11-26},
  eprint = {1611.07004},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1611.07004},
  url = {http://arxiv.org/abs/1611.07004},
  urldate = {2023-06-08},
  abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/eragon/Zotero/storage/Y8LALE2I/Isola et al. - 2018 - Image-to-Image Translation with Conditional Advers.pdf}
}

@article{izenmanIntroductionManifoldLearning2012,
  title = {Introduction to Manifold Learning: {{Introduction}} to Manifold Learning},
  shorttitle = {Introduction to Manifold Learning},
  author = {Izenman, Alan Julian},
  date = {2012-09},
  journaltitle = {Wiley Interdisciplinary Reviews: Computational Statistics},
  shortjournal = {WIREs Comp Stat},
  volume = {4},
  number = {5},
  pages = {439--446},
  issn = {19395108},
  doi = {10.1002/wics.1222},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/wics.1222},
  urldate = {2022-11-15},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/VF7LW5BV/Izenman - 2012 - Introduction to manifold learning Introduction to.pdf}
}

@software{jacobPyTorchLibraryCAM2021,
  title = {{{PyTorch}} Library for {{CAM}} Methods},
  author = {Jacob, Gildenblat and {contributors}},
  date = {2021},
  url = {https://github.com/jacobgil/pytorch-grad-cam}
}

@online{jacoviTrendsExplainableAI2023,
  title = {Trends in {{Explainable AI}} ({{XAI}}) {{Literature}}},
  author = {Jacovi, Alon},
  date = {2023-01-13},
  eprint = {2301.05433},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2301.05433},
  url = {http://arxiv.org/abs/2301.05433},
  urldate = {2023-05-19},
  abstract = {The XAI literature is decentralized, both in terminology and in publication venues, but recent years saw the community converge around keywords that make it possible to more reliably discover papers automatically. We use keyword search using the SemanticScholar API and manual curation to collect a well-formatted and reasonably comprehensive set of 5199 XAI papers, available at https://github.com/alonjacovi/XAI-Scholar . We use this collection to clarify and visualize trends about the size and scope of the literature, citation trends, cross-field trends, and collaboration trends. Overall, XAI is becoming increasingly multidisciplinary, with relative growth in papers belonging to increasingly diverse (non-CS) scientific fields, increasing cross-field collaborative authorship, increasing cross-field citation activity. The collection can additionally be used as a paper discovery engine, by retrieving XAI literature which is cited according to specific constraints (for example, papers that are influential outside of their field, or influential to non-XAI research).},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/eragon/Zotero/storage/YHV8XDAQ/Jacovi - 2023 - Trends in Explainable AI (XAI) Literature.pdf}
}

@inproceedings{jaipuriaDeflatingDatasetBias2020,
  title = {Deflating {{Dataset Bias Using Synthetic Data Augmentation}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  author = {Jaipuria, Nikita and Zhang, Xianling and Bhasin, Rohan and Arafa, Mayar and Chakravarty, Punarjay and Shrivastava, Shubham and Manglani, Sagar and Murali, Vidya N.},
  date = {2020-06},
  pages = {3344--3353},
  publisher = {{IEEE}},
  location = {{Seattle, WA, USA}},
  doi = {10.1109/CVPRW50498.2020.00394},
  url = {https://ieeexplore.ieee.org/document/9150612/},
  urldate = {2023-01-16},
  abstract = {Deep Learning has seen an unprecedented increase in vision applications since the publication of large-scale object recognition datasets and introduction of scalable compute hardware. State-of-the-art methods for most vision tasks for Autonomous Vehicles (AVs) rely on supervised learning and often fail to generalize to domain shifts and/or outliers. Dataset diversity is thus key to successful real-world deployment. No matter how big the size of the dataset, capturing long tails of the distribution pertaining to task-specific environmental factors is impractical. The goal of this paper is to investigate the use of targeted synthetic data augmentation - combining the benefits of gaming engine simulations and sim2real style transfer techniques - for filling gaps in real datasets for vision tasks. Empirical studies on three different computer vision tasks of practical use to AVs parking slot detection, lane detection and monocular depth estimation - consistently show that having synthetic data in the training mix provides a significant boost in cross-dataset generalization performance as compared to training on real data only, for the same size of the training set.},
  eventtitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}} ({{CVPRW}})},
  isbn = {978-1-72819-360-1},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/EXIZZSHN/Jaipuria et al. - 2020 - Deflating Dataset Bias Using Synthetic Data Augmen.pdf}
}

@article{jiangMLVCNNMultiLoopViewConvolutional2019,
  title = {{{MLVCNN}}: {{Multi-Loop-View Convolutional Neural Network}} for {{3D Shape Retrieval}}},
  shorttitle = {{{MLVCNN}}},
  author = {Jiang, Jianwen and Bao, Di and Chen, Ziqiang and Zhao, Xibin and Gao, Yue},
  date = {2019-07-17},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI},
  volume = {33},
  pages = {8513--8520},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v33i01.33018513},
  url = {http://aaai.org/ojs/index.php/AAAI/article/view/4869},
  urldate = {2022-05-10},
  abstract = {3D shape retrieval has attracted much attention and become a hot topic in computer vision field recently.With the development of deep learning, 3D shape retrieval has also made great progress and many view-based methods have been introduced in recent years. However, how to represent 3D shapes better is still a challenging problem. At the same time, the intrinsic hierarchical associations among views still have not been well utilized. In order to tackle these problems, in this paper, we propose a multi-loop-view convolutional neural network (MLVCNN) framework for 3D shape retrieval. In this method, multiple groups of views are extracted from different loop directions first. Given these multiple loop views, the proposed MLVCNN framework introduces a hierarchical view-loop-shape architecture, i.e., the view level, the loop level, and the shape level, to conduct 3D shape representation from different scales. In the view-level, a convolutional neural network is first trained to extract view features. Then, the proposed Loop Normalization and LSTM are utilized for each loop of view to generate the loop-level features, which considering the intrinsic associations of the different views in the same loop. Finally, all the loop-level descriptors are combined into a shape-level descriptor for 3D shape representation, which is used for 3D shape retrieval. Our proposed method has been evaluated on the public 3D shape benchmark, i.e., ModelNet40. Experiments and comparisons with the state-of-the-art methods show that the proposed MLVCNN method can achieve significant performance improvement on 3D shape retrieval tasks. Our MLVCNN outperforms the state-of-the-art methods by the mAP of 4.84\% in 3D shape retrieval task. We have also evaluated the performance of the proposed method on the 3D shape classification task where MLVCNN also achieves superior performance compared with recent methods.},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/HFBXJNGW/Jiang et al. - 2019 - MLVCNN Multi-Loop-View Convolutional Neural Netwo.pdf}
}

@inproceedings{jiangMLVCNNMultiloopviewConvolutional2019,
  title = {{{MLVCNN}}: {{Multi-loop-view}} Convolutional Neural Network for {{3D}} Shape Retrieval},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Jiang, Jianwen and Bao, Di and Chen, Ziqiang and Zhao, Xibin and Gao, Yue},
  date = {2019},
  volume = {33},
  number = {01},
  pages = {8513--8520}
}

@article{jingSelfSupervisedVisualFeature2021,
  title = {Self-{{Supervised Visual Feature Learning With Deep Neural Networks}}: {{A Survey}}},
  shorttitle = {Self-{{Supervised Visual Feature Learning With Deep Neural Networks}}},
  author = {Jing, Longlong and Tian, Yingli},
  date = {2021-11},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {43},
  number = {11},
  pages = {4037--4058},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2020.2992393},
  abstract = {Large-scale labeled data are generally required to train deep neural networks in order to obtain better performance in visual feature learning from images or videos for computer vision applications. To avoid extensive cost of collecting and annotating large-scale datasets, as a subset of unsupervised learning methods, self-supervised learning methods are proposed to learn general image and video features from large-scale unlabeled data without using any human-annotated labels. This paper provides an extensive review of deep learning-based self-supervised general visual feature learning methods from images or videos. First, the motivation, general pipeline, and terminologies of this field are described. Then the common deep neural network architectures that used for self-supervised learning are summarized. Next, the schema and evaluation metrics of self-supervised learning methods are reviewed followed by the commonly used datasets for images, videos, audios, and 3D data, as well as the existing self-supervised visual feature learning methods. Finally, quantitative performance comparisons of the reviewed methods on benchmark datasets are summarized and discussed for both image and video feature learning. At last, this paper is concluded and lists a set of promising future directions for self-supervised visual feature learning.},
  eventtitle = {{{IEEE Transactions}} on {{Pattern Analysis}} and {{Machine Intelligence}}},
  keywords = {Annotations,convolutional neural network,deep learning,Feature extraction,Learning systems,Self-supervised learning,Task analysis,Training,transfer learning,unsupervised learning,Videos,Visualization},
  file = {/Users/eragon/Zotero/storage/27EY26DP/Jing and Tian - 2021 - Self-Supervised Visual Feature Learning With Deep .pdf;/Users/eragon/Zotero/storage/SYJ4HVXM/Jing and Tian - 2021 - Self-Supervised Visual Feature Learning With Deep .pdf}
}

@unpublished{kaiserDepthwiseSeparableConvolutions2017,
  title = {Depthwise {{Separable Convolutions}} for {{Neural Machine Translation}}},
  author = {Kaiser, Lukasz and Gomez, Aidan N. and Chollet, Francois},
  date = {2017-06-15},
  eprint = {1706.03059},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1706.03059},
  url = {http://arxiv.org/abs/1706.03059},
  urldate = {2022-05-24},
  abstract = {Depthwise separable convolutions reduce the number of parameters and computation used in convolutional operations while increasing representational efficiency. They have been shown to be successful in image classification models, both in obtaining better models than previously possible for a given parameter count (the Xception architecture) and considerably reducing the number of parameters required to perform at a given level (the MobileNets family of architectures). Recently, convolutional sequence-to-sequence networks have been applied to machine translation tasks with good results. In this work, we study how depthwise separable convolutions can be applied to neural machine translation. We introduce a new architecture inspired by Xception and ByteNet, called SliceNet, which enables a significant reduction of the parameter count and amount of computation needed to obtain results like ByteNet, and, with a similar parameter count, achieves new state-of-the-art results. In addition to showing that depthwise separable convolutions perform well for machine translation, we investigate the architectural changes that they enable: we observe that thanks to depthwise separability, we can increase the length of convolution windows, removing the need for filter dilation. We also introduce a new "super-separable" convolution operation that further reduces the number of parameters and computational cost for obtaining state-of-the-art results.},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/eragon/Zotero/storage/FKFRENQW/Kaiser et al. - 2017 - Depthwise Separable Convolutions for Neural Machin.pdf;/Users/eragon/Zotero/storage/CKNT4QE3/1706.html}
}

@unpublished{kaiserOneModelLearn2017,
  title = {One {{Model To Learn Them All}}},
  author = {Kaiser, Lukasz and Gomez, Aidan N. and Shazeer, Noam and Vaswani, Ashish and Parmar, Niki and Jones, Llion and Uszkoreit, Jakob},
  date = {2017-06-15},
  eprint = {1706.05137},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1706.05137},
  url = {http://arxiv.org/abs/1706.05137},
  urldate = {2022-05-24},
  abstract = {Deep learning yields great results across many fields, from speech recognition, image classification, to translation. But for each problem, getting a deep model to work well involves research into the architecture and a long period of tuning. We present a single model that yields good results on a number of problems spanning multiple domains. In particular, this single model is trained concurrently on ImageNet, multiple translation tasks, image captioning (COCO dataset), a speech recognition corpus, and an English parsing task. Our model architecture incorporates building blocks from multiple domains. It contains convolutional layers, an attention mechanism, and sparsely-gated layers. Each of these computational blocks is crucial for a subset of the tasks we train on. Interestingly, even if a block is not crucial for a task, we observe that adding it never hurts performance and in most cases improves it on all tasks. We also show that tasks with less data benefit largely from joint training with other tasks, while performance on large tasks degrades only slightly if at all.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/eragon/Zotero/storage/XFUZZAHB/Kaiser et al. - 2017 - One Model To Learn Them All.pdf;/Users/eragon/Zotero/storage/JL7JPL4N/1706.html}
}

@unpublished{kaiserOneModelLearn2017a,
  title = {One {{Model To Learn Them All}}},
  author = {Kaiser, Lukasz and Gomez, Aidan N. and Shazeer, Noam and Vaswani, Ashish and Parmar, Niki and Jones, Llion and Uszkoreit, Jakob},
  date = {2017-06-15},
  eprint = {1706.05137},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1706.05137},
  url = {http://arxiv.org/abs/1706.05137},
  urldate = {2022-05-24},
  abstract = {Deep learning yields great results across many fields, from speech recognition, image classification, to translation. But for each problem, getting a deep model to work well involves research into the architecture and a long period of tuning. We present a single model that yields good results on a number of problems spanning multiple domains. In particular, this single model is trained concurrently on ImageNet, multiple translation tasks, image captioning (COCO dataset), a speech recognition corpus, and an English parsing task. Our model architecture incorporates building blocks from multiple domains. It contains convolutional layers, an attention mechanism, and sparsely-gated layers. Each of these computational blocks is crucial for a subset of the tasks we train on. Interestingly, even if a block is not crucial for a task, we observe that adding it never hurts performance and in most cases improves it on all tasks. We also show that tasks with less data benefit largely from joint training with other tasks, while performance on large tasks degrades only slightly if at all.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/eragon/Zotero/storage/KZH4WB5C/Kaiser et al. - 2017 - One Model To Learn Them All.pdf;/Users/eragon/Zotero/storage/YD6DBVIN/1706.html}
}

@inproceedings{kanezakiRotationNetJointObject2018,
  title = {{{RotationNet}}: {{Joint Object Categorization}} and {{Pose Estimation Using Multiviews}} from {{Unsupervised Viewpoints}}},
  booktitle = {Proceedings of {{IEEE International Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Kanezaki, Asako and Matsushita, Yasuyuki and Nishida, Yoshifumi},
  date = {2018}
}

@article{kanezakiRotationNetJointObject2021,
  title = {{{RotationNet}} for {{Joint Object Categorization}} and {{Unsupervised Pose Estimation}} from {{Multi-View Images}}},
  author = {Kanezaki, Asako and Matsushita, Yasuyuki and Nishida, Yoshifumi},
  date = {2021-01-01},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  volume = {43},
  number = {1},
  pages = {269--283},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2019.2922640},
  url = {https://ieeexplore.ieee.org/document/8736864/},
  urldate = {2022-05-10},
  abstract = {We propose a Convolutional Neural Network (CNN)-based model “RotationNet,” which takes multi-view images of an object as input and jointly estimates its pose and object category. Unlike previous approaches that use known viewpoint labels for training, our method treats the viewpoint labels as latent variables, which are learned in an unsupervised manner during the training using an unaligned object dataset. RotationNet uses only a partial set of multi-view images for inference, and this property makes it useful in practical scenarios where only partial views are available. Moreover, our pose alignment strategy enables one to obtain view-specific feature representations shared across classes, which is important to maintain high accuracy in both object categorization and pose estimation. Effectiveness of RotationNet is demonstrated by its superior performance to the state-of-the-art methods of 3D object classification on 10- and 40-class ModelNet datasets. We also show that RotationNet, even trained without known poses, achieves comparable performance to the state-of-the-art methods on an object pose estimation dataset. Furthermore, our object ranking method based on classification by RotationNet achieved the first prize in two tracks of the 3D Shape Retrieval Contest (SHREC) 2017. Finally, we demonstrate the performance of real-world applications of RotationNet trained with our newly created multi-view image dataset using a moving USB camera.},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/JY98IMEX/Kanezaki et al. - 2021 - RotationNet for Joint Object Categorization and Un.pdf}
}

@unpublished{kangInterpretingUndesirablePixels2019,
  title = {Interpreting {{Undesirable Pixels}} for {{Image Classification}} on {{Black-Box Models}}},
  author = {Kang, Sin-Han and Jung, Hong-Gyu and Lee, Seong-Whan},
  date = {2019-12-16},
  eprint = {1909.12446},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1909.12446},
  urldate = {2022-09-26},
  abstract = {In an effort to interpret black-box models, researches for developing explanation methods have proceeded in recent years. Most studies have tried to identify input pixels that are crucial to the prediction of a classifier. While this approach is meaningful to analyse the characteristic of blackbox models, it is also important to investigate pixels that interfere with the prediction. To tackle this issue, in this paper, we propose an explanation method that visualizes undesirable regions to classify an image as a target class. To be specific, we divide the concept of undesirable regions into two terms: (1) factors for a target class, which hinder that black-box models identify intrinsic characteristics of a target class and (2) factors for non-target classes that are important regions for an image to be classified as other classes. We visualize such undesirable regions on heatmaps to qualitatively validate the proposed method. Furthermore, we present an evaluation metric to provide quantitative results on ImageNet.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/eragon/Zotero/storage/4MXQT9TW/Kang et al. - 2019 - Interpreting Undesirable Pixels for Image Classifi.pdf}
}

@unpublished{kasaeiOrthographicNetDeepTransfer2020,
  title = {{{OrthographicNet}}: {{A Deep Transfer Learning Approach}} for {{3D Object Recognition}} in {{Open-Ended Domains}}},
  shorttitle = {{{OrthographicNet}}},
  author = {Kasaei, Hamidreza},
  date = {2020-12-31},
  eprint = {1902.03057},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1902.03057},
  urldate = {2022-05-12},
  abstract = {Nowadays, service robots are appearing more and more in our daily life. For this type of robot, open-ended object category learning and recognition is necessary since no matter how extensive the training data used for batch learning, the robot might be faced with a new object when operating in a real-world environment. In this work, we present OrthographicNet, a Convolutional Neural Network (CNN)-based model, for 3D object recognition in open-ended domains. In particular, OrthographicNet generates a global rotation- and scale-invariant representation for a given 3D object, enabling robots to recognize the same or similar objects seen from different perspectives. Experimental results show that our approach yields significant improvements over the previous state-of-the-art approaches concerning object recognition performance and scalability in open-ended scenarios. Moreover, OrthographicNet demonstrates the capability of learning new categories from very few examples on-site. Regarding real-time performance, three real-world demonstrations validate the promising performance of the proposed architecture.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Robotics},
  file = {/Users/eragon/Zotero/storage/4YVS65GP/Kasaei - 2020 - OrthographicNet A Deep Transfer Learning Approach.pdf;/Users/eragon/Zotero/storage/IJ7UH3HT/1902.html}
}

@inproceedings{kasaeiPerceivingLearningRecognizing2018,
  title = {Perceiving, Learning, and Recognizing 3d Objects: {{An}} Approach to Cognitive Service Robots},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Kasaei, S and Sock, Juil and Lopes, Luis Seabra and Tomé, Ana Maria and Kim, Tae-Kyun},
  date = {2018}
}

@online{keaneIfOnlyWe2021,
  title = {If {{Only We Had Better Counterfactual Explanations}}: {{Five Key Deficits}} to {{Rectify}} in the {{Evaluation}} of {{Counterfactual XAI Techniques}}},
  shorttitle = {If {{Only We Had Better Counterfactual Explanations}}},
  author = {Keane, Mark T. and Kenny, Eoin M. and Delaney, Eoin and Smyth, Barry},
  date = {2021-02-26},
  eprint = {2103.01035},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2103.01035},
  urldate = {2023-02-13},
  abstract = {In recent years, there has been an explosion of AI research on counterfactual explanations as a solution to the problem of eXplainable AI (XAI). These explanations seem to offer technical, psychological and legal benefits over other explanation techniques. We survey 100 distinct counterfactual explanation methods reported in the literature. This survey addresses the extent to which these methods have been adequately evaluated, both psychologically and computationally, and quantifies the shortfalls occurring. For instance, only 21\% of these methods have been user tested. Five key deficits in the evaluation of these methods are detailed and a roadmap, with standardised benchmark evaluations, is proposed to resolve the issues arising; issues, that currently effectively block scientific progress in this field.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/eragon/Zotero/storage/6MMXAVTJ/Keane et al. - 2021 - If Only We Had Better Counterfactual Explanations.pdf}
}

@inproceedings{khanUnsupervisedPrimitiveDiscovery2019,
  title = {Unsupervised {{Primitive Discovery}} for {{Improved 3D Generative Modeling}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Khan, Salman H. and Guo, Yulan and Hayat, Munawar and Barnes, Nick},
  date = {2019-06},
  pages = {9731--9740},
  publisher = {{IEEE}},
  location = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.00997},
  url = {https://ieeexplore.ieee.org/document/8954393/},
  urldate = {2022-05-10},
  eventtitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72813-293-8},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/WTJN3BVS/Khan et al. - 2019 - Unsupervised Primitive Discovery for Improved 3D G.pdf}
}

@article{khoslaNovelDatasetFineGrained,
  title = {Novel {{Dataset}} for {{Fine-Grained Image Categorization}}: {{Stanford Dogs}}},
  author = {Khosla, Aditya and Jayadevaprakash, Nityananda and Yao, Bangpeng and Li, Fei-Fei},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/VND9PU96/Khosla et al. - Novel Dataset for Fine-Grained Image Categorizatio.pdf}
}

@online{kimCoMixupSaliencyGuided2021,
  title = {Co-{{Mixup}}: {{Saliency Guided Joint Mixup}} with {{Supermodular Diversity}}},
  shorttitle = {Co-{{Mixup}}},
  author = {Kim, Jang-Hyun and Choo, Wonho and Jeong, Hosan and Song, Hyun Oh},
  date = {2021-02-05},
  eprint = {2102.03065},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2102.03065},
  url = {http://arxiv.org/abs/2102.03065},
  urldate = {2023-04-04},
  abstract = {While deep neural networks show great performance on fitting to the training distribution, improving the networks' generalization performance to the test distribution and robustness to the sensitivity to input perturbations still remain as a challenge. Although a number of mixup based augmentation strategies have been proposed to partially address them, it remains unclear as to how to best utilize the supervisory signal within each input data for mixup from the optimization perspective. We propose a new perspective on batch mixup and formulate the optimal construction of a batch of mixup data maximizing the data saliency measure of each individual mixup data and encouraging the supermodular diversity among the constructed mixup data. This leads to a novel discrete optimization problem minimizing the difference between submodular functions. We also propose an efficient modular approximation based iterative submodular minimization algorithm for efficient mixup computation per each minibatch suitable for minibatch based neural network training. Our experiments show the proposed method achieves the state of the art generalization, calibration, and weakly supervised localization results compared to other mixup methods. The source code is available at https://github.com/snu-mllab/Co-Mixup.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/eragon/Zotero/storage/56YEZJBU/Kim et al. - 2021 - Co-Mixup Saliency Guided Joint Mixup with Supermo.pdf;/Users/eragon/Zotero/storage/7AQ4DGFT/Kim et al. - 2021 - Co-Mixup Saliency Guided Joint Mixup with Supermo.pdf}
}

@inproceedings{kimHelpMeHelp2023,
  title = {"{{Help Me Help}} the {{AI}}": {{Understanding How Explainability Can Support Human-AI Interaction}}},
  shorttitle = {"{{Help Me Help}} the {{AI}}"},
  booktitle = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Kim, Sunnie S. Y. and Watkins, Elizabeth Anne and Russakovsky, Olga and Fong, Ruth and Monroy-Hernández, Andrés},
  date = {2023-04-19},
  eprint = {2210.03735},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {1--17},
  doi = {10.1145/3544548.3581001},
  url = {http://arxiv.org/abs/2210.03735},
  urldate = {2023-05-25},
  abstract = {Despite the proliferation of explainable AI (XAI) methods, little is understood about end-users' explainability needs and behaviors around XAI explanations. To address this gap and contribute to understanding how explainability can support human-AI interaction, we conducted a mixed-methods study with 20 end-users of a real-world AI application, the Merlin bird identification app, and inquired about their XAI needs, uses, and perceptions. We found that participants desire practically useful information that can improve their collaboration with the AI, more so than technical system details. Relatedly, participants intended to use XAI explanations for various purposes beyond understanding the AI's outputs: calibrating trust, improving their task skills, changing their behavior to supply better inputs to the AI, and giving constructive feedback to developers. Finally, among existing XAI approaches, participants preferred part-based explanations that resemble human reasoning and explanations. We discuss the implications of our findings and provide recommendations for future XAI design.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Computers and Society,Computer Science - Human-Computer Interaction},
  file = {/Users/eragon/Zotero/storage/RI5PK9QQ/Kim et al. - 2023 - Help Me Help the AI Understanding How Explainab.pdf}
}

@inproceedings{kimPuzzleMixExploiting2020,
  title = {Puzzle {{Mix}}: {{Exploiting Saliency}} and {{Local Statistics}} for {{Optimal Mixup}}},
  shorttitle = {Puzzle {{Mix}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Kim, Jang-Hyun and Choo, Wonho and Song, Hyun Oh},
  date = {2020-11-21},
  pages = {5275--5285},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v119/kim20b.html},
  urldate = {2023-04-04},
  abstract = {While deep neural networks achieve great performance on fitting the training distribution, the learned networks are prone to overfitting and are susceptible to adversarial attacks. In this regard, a number of mixup based augmentation methods have been recently proposed. However, these approaches mainly focus on creating previously unseen virtual examples and can sometimes provide misleading supervisory signal to the network. To this end, we propose Puzzle Mix, a mixup method for explicitly utilizing the saliency information and the underlying statistics of the natural examples. This leads to an interesting optimization problem alternating between the multi-label objective for optimal mixing mask and saliency discounted optimal transport objective. Our experiments show Puzzle Mix achieves the state of the art generalization and the adversarial robustness results compared to other mixup methods on CIFAR-100, Tiny-ImageNet, and ImageNet datasets, and the source code is available at https://github.com/snu-mllab/PuzzleMix.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/B3UJ38HM/Kim et al. - 2020 - Puzzle Mix Exploiting Saliency and Local Statisti.pdf;/Users/eragon/Zotero/storage/KDMP48E9/Kim et al. - 2020 - Puzzle Mix Exploiting Saliency and Local Statisti.pdf}
}

@online{kindermansReliabilitySaliencyMethods2017,
  title = {The ({{Un}})Reliability of Saliency Methods},
  author = {Kindermans, Pieter-Jan and Hooker, Sara and Adebayo, Julius and Alber, Maximilian and Schütt, Kristof T. and Dähne, Sven and Erhan, Dumitru and Kim, Been},
  date = {2017-11-02},
  eprint = {1711.00867},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1711.00867},
  url = {http://arxiv.org/abs/1711.00867},
  urldate = {2022-11-18},
  abstract = {Saliency methods aim to explain the predictions of deep neural networks. These methods lack reliability when the explanation is sensitive to factors that do not contribute to the model prediction. We use a simple and common pre-processing step ---adding a constant shift to the input data--- to show that a transformation with no effect on the model can cause numerous methods to incorrectly attribute. In order to guarantee reliability, we posit that methods should fulfill input invariance, the requirement that a saliency method mirror the sensitivity of the model with respect to transformations of the input. We show, through several examples, that saliency methods that do not satisfy input invariance result in misleading attribution.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/eragon/Zotero/storage/EBUDRT4E/Kindermans et al. - 2017 - The (Un)reliability of saliency methods.pdf}
}

@unpublished{kingmaAdamMethodStochastic2014,
  title = {Adam: {{A}} Method for Stochastic Optimization},
  author = {Kingma, Diederik P and Ba, Jimmy},
  date = {2014},
  eprint = {1412.6980},
  eprinttype = {arxiv}
}

@online{kokhlikyanCaptumUnifiedGeneric2020,
  title = {Captum: {{A}} Unified and Generic Model Interpretability Library for {{PyTorch}}},
  shorttitle = {Captum},
  author = {Kokhlikyan, Narine and Miglani, Vivek and Martin, Miguel and Wang, Edward and Alsallakh, Bilal and Reynolds, Jonathan and Melnikov, Alexander and Kliushkina, Natalia and Araya, Carlos and Yan, Siqi and Reblitz-Richardson, Orion},
  date = {2020-09-16},
  eprint = {2009.07896},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2009.07896},
  url = {http://arxiv.org/abs/2009.07896},
  urldate = {2023-04-04},
  abstract = {In this paper we introduce a novel, unified, open-source model interpretability library for PyTorch [12]. The library contains generic implementations of a number of gradient and perturbation-based attribution algorithms, also known as feature, neuron and layer importance algorithms, as well as a set of evaluation metrics for these algorithms. It can be used for both classification and non-classification models including graph-structured models built on Neural Networks (NN). In this paper we give a high-level overview of supported attribution algorithms and show how to perform memory-efficient and scalable computations. We emphasize that the three main characteristics of the library are multimodality, extensibility and ease of use. Multimodality supports different modality of inputs such as image, text, audio or video. Extensibility allows adding new algorithms and features. The library is also designed for easy understanding and use. Besides, we also introduce an interactive visualization tool called Captum Insights that is built on top of Captum library and allows sample-based model debugging and visualization using feature importance metrics.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/eragon/Zotero/storage/A874V4BJ/Kokhlikyan et al. - 2020 - Captum A unified and generic model interpretabili.pdf}
}

@article{kononowiczSlowPotentialsTime2011,
  title = {Slow {{Potentials}} in {{Time Estimation}}: {{The Role}} of {{Temporal Accumulation}} and {{Habituation}}},
  shorttitle = {Slow {{Potentials}} in {{Time Estimation}}},
  author = {Kononowicz, Tadeusz and Van Rijn, Hedderik},
  date = {2011},
  journaltitle = {Frontiers in Integrative Neuroscience},
  volume = {5},
  issn = {1662-5145},
  url = {https://www.frontiersin.org/articles/10.3389/fnint.2011.00048},
  urldate = {2023-01-25},
  abstract = {Numerous studies have shown that contingent negative variation (CNV) measured at fronto-central and parietal–central areas is closely related to interval timing. However, the exact nature of the relation between CNV and the underlying timing mechanisms is still a topic of discussion. On the one hand, it has been proposed that the CNV measured at supplementary motor area (SMA) is a direct reflection of the unfolding of time since a perceived onset, whereas other work has suggested that the increased amplitude reflects decision processes involved in interval timing. Strong evidence for the first view has been reported by Macar et al. (1999), who showed that variations in temporal performance were reflected in the measured CNV amplitude. If the CNV measured at SMA is a direct function of the passing of time, habituation effects are not expected. Here we report two replication studies, which both failed to replicate the expected performance-dependent variations. Even more powerful linear-mixed effect analyses failed to find any performance related effects on the CNV amplitude, whereas habituation effects were found. These studies therefore suggest that the CNV amplitude does not directly reflect the unfolding of time.},
  file = {/Users/eragon/Zotero/storage/HXABVQ5M/Kononowicz and Van Rijn - 2011 - Slow Potentials in Time Estimation The Role of Te.pdf}
}

@article{kragelSimilarPatternsNeural2017,
  title = {Similar Patterns of Neural Activity Predict Memory Function during Encoding and Retrieval},
  author = {Kragel, James E. and Ezzyat, Youssef and Sperling, Michael R. and Gorniak, Richard and Worrell, Gregory A. and Berry, Brent M. and Inman, Cory and Lin, Jui-Jui and Davis, Kathryn A. and Das, Sandhitsu R. and Stein, Joel M. and Jobst, Barbara C. and Zaghloul, Kareem A. and Sheth, Sameer A. and Rizzuto, Daniel S. and Kahana, Michael J.},
  date = {2017-07},
  journaltitle = {NeuroImage},
  shortjournal = {NeuroImage},
  volume = {155},
  pages = {60--71},
  issn = {10538119},
  doi = {10.1016/j.neuroimage.2017.03.042},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1053811917302549},
  urldate = {2022-05-27},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/MZKV73Q3/Kragel et al. - 2017 - Similar patterns of neural activity predict memory.pdf}
}

@article{kriegeskorteCognitiveComputationalNeuroscience2018,
  title = {Cognitive Computational Neuroscience},
  author = {Kriegeskorte, Nikolaus and Douglas, Pamela K.},
  date = {2018-09},
  journaltitle = {Nature Neuroscience},
  shortjournal = {Nat Neurosci},
  volume = {21},
  number = {9},
  pages = {1148--1160},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/s41593-018-0210-5},
  url = {http://www.nature.com/articles/s41593-018-0210-5},
  urldate = {2022-05-27},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/UJ4XEVVV/Kriegeskorte and Douglas - 2018 - Cognitive computational neuroscience.pdf}
}

@article{krizhevskyLearningMultipleLayers,
  title = {Learning {{Multiple Layers}} of {{Features}} from {{Tiny Images}}},
  author = {Krizhevsky, Alex},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/T2BXLVFU/Krizhevsky - Learning Multiple Layers of Features from Tiny Ima.pdf}
}

@article{kruijneImplicitlyLearningWhen2022,
  title = {Implicitly Learning When to Be Ready: {{From}} Instances to Categories},
  shorttitle = {Implicitly Learning When to Be Ready},
  author = {Kruijne, Wouter and Galli, Riccardo M. and Los, Sander A.},
  date = {2022-04-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  volume = {29},
  number = {2},
  pages = {552--562},
  issn = {1531-5320},
  doi = {10.3758/s13423-021-02004-w},
  url = {https://doi.org/10.3758/s13423-021-02004-w},
  urldate = {2023-01-25},
  abstract = {There is growing appreciation for the role of long-term memory in guiding temporal preparation in speeded reaction time tasks. In experiments with variable foreperiods between a warning stimulus (S1) and a target stimulus (S2), preparation is affected by foreperiod distributions experienced in the past, long after the distribution has changed. These effects from memory can shape preparation largely implicitly, outside of participants’ awareness. Recent studies have demonstrated the associative nature of memory-guided preparation. When distinct S1s predict different foreperiods, they can trigger differential preparation accordingly. Here, we propose that memory-guided preparation allows for another key feature of learning: the ability to generalize across acquired associations and apply them to novel situations. Participants completed a variable foreperiod task where S1 was a unique image of either a face or a scene on each trial. Images of either category were paired with different distributions with predominantly shorter versus predominantly longer foreperiods. Participants displayed differential preparation to never-before seen images of either category, without being aware of the predictive nature of these categories. They continued doing so in a subsequent Transfer phase, after they had been informed that these contingencies no longer held. A novel rolling regression analysis revealed at a fine timescale how category-guided preparation gradually developed throughout the task, and that explicit information about these contingencies only briefly disrupted memory-guided preparation. These results offer new insights into temporal preparation as the product of a largely implicit process governed by associative learning from past experiences.},
  langid = {english},
  keywords = {Generalization,Long-term memory,Prediction,Temporal preparation,Time-course analysis},
  file = {/Users/eragon/Zotero/storage/XJSFMZLJ/Kruijne et al. - 2022 - Implicitly learning when to be ready From instanc.pdf}
}

@unpublished{kumawatLP3DCNNUnveilingLocal2019,
  title = {{{LP-3DCNN}}: {{Unveiling Local Phase}} in {{3D Convolutional Neural Networks}}},
  shorttitle = {{{LP-3DCNN}}},
  author = {Kumawat, Sudhakar and Raman, Shanmuganathan},
  date = {2019-04-06},
  eprint = {1904.03498},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1904.03498},
  urldate = {2022-05-10},
  abstract = {Traditional 3D Convolutional Neural Networks (CNNs) are computationally expensive, memory intensive, prone to overfit, and most importantly, there is a need to improve their feature learning capabilities. To address these issues, we propose Rectified Local Phase Volume (ReLPV) block, an efficient alternative to the standard 3D convolutional layer. The ReLPV block extracts the phase in a 3D local neighborhood (e.g., 3 × 3 × 3) of each position of the input map to obtain the feature maps. The phase is extracted by computing 3D Short Term Fourier Transform (STFT) at multiple fixed low frequency points in the 3D local neighborhood of each position. These feature maps at different frequency points are then linearly combined after passing them through an activation function. The ReLPV block provides significant parameter savings of at least, 33 to 133 times compared to the standard 3D convolutional layer with the filter sizes 3 × 3 × 3 to 13 × 13 × 13, respectively. We show that the feature learning capabilities of the ReLPV block are significantly better than the standard 3D convolutional layer. Furthermore, it produces consistently better results across different 3D data representations. We achieve state-of-the-art accuracy on the volumetric ModelNet10 and ModelNet40 datasets while utilizing only 11\% parameters of the current state-of-theart. We also improve the state-of-the-art on the UCF-101 split-1 action recognition dataset by 5.68\% (when trained from scratch) while using only 15\% of the parameters of the state-of-the-art. The project webpage is available at https://sites.google.com/view/lp-3dcnn/home.},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/eragon/Zotero/storage/T8PNMNJI/Kumawat and Raman - 2019 - LP-3DCNN Unveiling Local Phase in 3D Convolutiona.pdf}
}

@inproceedings{kumawatLp3dcnnUnveilingLocal2019,
  title = {Lp-3dcnn: {{Unveiling}} Local Phase in 3d Convolutional Neural Networks},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Kumawat, Sudhakar and Raman, Shanmuganathan},
  date = {2019},
  pages = {4903--4912}
}

@inproceedings{kuznetsovaExploitingViewspecificAppearance2016,
  title = {Exploiting View-Specific Appearance Similarities across Classes for Zero-Shot Pose Prediction: {{A}} Metric Learning Approach},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Kuznetsova, Alina and Hwang, Sung Ju and Rosenhahn, Bodo and Sigal, Leonid},
  date = {2016},
  volume = {30},
  number = {1}
}

@inproceedings{laiScalableTreebasedApproach2011,
  title = {A Scalable Tree-Based Approach for Joint Object and Pose Recognition},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Lai, Kevin and Bo, Liefeng and Ren, Xiaofeng and Fox, Dieter},
  date = {2011}
}

@unpublished{lanCouplformerRethinkingVision2021,
  title = {Couplformer:{{Rethinking Vision Transformer}} with {{Coupling Attention Map}}},
  shorttitle = {Couplformer},
  author = {Lan, Hai and Wang, Xihao and Wei, Xian},
  date = {2021-12-10},
  eprint = {2112.05425},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2112.05425},
  urldate = {2022-10-03},
  abstract = {With the development of the self-attention mechanism, the Transformer model has demonstrated its outstanding performance in the computer vision domain. However, the massive computation brought from the full attention mechanism became a heavy burden for memory consumption. Sequentially, the limitation of memory reduces the possibility of improving the Transformer model. To remedy this problem, we propose a novel memory economy attention mechanism named Couplformer, which decouples the attention map into two sub-matrices and generates the alignment scores from spatial information. A series of different scale image classification tasks are applied to evaluate the effectiveness of our model. The result of experiments shows that on the ImageNet-1k classification task, the Couplformer can significantly decrease 28\% memory consumption compared with regular Transformer while accessing sufficient accuracy requirements and outperforming 0.92\% on Top-1 accuracy while occupying the same memory footprint. As a result, the Couplformer can serve as an efficient backbone in visual tasks, and provide a novel perspective on the attention mechanism for researchers.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/eragon/Zotero/storage/FB5UAR58/Lan et al. - 2021 - CouplformerRethinking Vision Transformer with Cou.pdf}
}

@unpublished{langExplainingStyleTraining2021,
  title = {Explaining in {{Style}}: {{Training}} a {{GAN}} to Explain a Classifier in {{StyleSpace}}},
  shorttitle = {Explaining in {{Style}}},
  author = {Lang, Oran and Gandelsman, Yossi and Yarom, Michal and Wald, Yoav and Elidan, Gal and Hassidim, Avinatan and Freeman, William T. and Isola, Phillip and Globerson, Amir and Irani, Michal and Mosseri, Inbar},
  date = {2021-09-01},
  eprint = {2104.13369},
  eprinttype = {arxiv},
  eprintclass = {cs, eess, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2104.13369},
  url = {http://arxiv.org/abs/2104.13369},
  urldate = {2022-09-06},
  abstract = {Image classification models can depend on multiple different semantic attributes of the image. An explanation of the decision of the classifier needs to both discover and visualize these properties. Here we present StylEx, a method for doing this, by training a generative model to specifically explain multiple attributes that underlie classifier decisions. A natural source for such attributes is the StyleSpace of StyleGAN, which is known to generate semantically meaningful dimensions in the image. However, because standard GAN training is not dependent on the classifier, it may not represent these attributes which are important for the classifier decision, and the dimensions of StyleSpace may represent irrelevant attributes. To overcome this, we propose a training procedure for a StyleGAN, which incorporates the classifier model, in order to learn a classifier-specific StyleSpace. Explanatory attributes are then selected from this space. These can be used to visualize the effect of changing multiple attributes per image, thus providing image-specific explanations. We apply StylEx to multiple domains, including animals, leaves, faces and retinal images. For these, we show how an image can be modified in different ways to change its classifier output. Our results show that the method finds attributes that align well with semantic ones, generate meaningful image-specific explanations, and are human-interpretable as measured in user-studies.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Electrical Engineering and Systems Science - Image and Video Processing,Statistics - Machine Learning},
  file = {/Users/eragon/Zotero/storage/9ULRZW8X/Lang et al. - 2021 - Explaining in Style Training a GAN to explain a c.pdf}
}

@incollection{leeMeshSaliency2005,
  title = {Mesh Saliency},
  booktitle = {{{ACM SIGGRAPH}} 2005 {{Papers}}},
  author = {Lee, Chang Ha and Varshney, Amitabh and Jacobs, David W},
  date = {2005},
  pages = {659--666}
}

@inproceedings{leeSmoothMixSimpleEffective2020,
  title = {{{SmoothMix}}: {{A Simple Yet Effective Data Augmentation}} to {{Train Robust Classifiers}}},
  shorttitle = {{{SmoothMix}}},
  author = {Lee, Jin-Ha and Zaheer, Muhammad Zaigham and Astrid, Marcella and Lee, Seung-Ik},
  date = {2020},
  pages = {756--757},
  url = {https://openaccess.thecvf.com/content_CVPRW_2020/html/w45/Lee_SmoothMix_A_Simple_Yet_Effective_Data_Augmentation_to_Train_Robust_CVPRW_2020_paper.html},
  urldate = {2023-03-29},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}}},
  file = {/Users/eragon/Zotero/storage/UAHLZ9ZT/Lee et al. - 2020 - SmoothMix A Simple Yet Effective Data Augmentatio.pdf}
}

@online{leinoInfluenceDirectedExplanationsDeep2018,
  title = {Influence-{{Directed Explanations}} for {{Deep Convolutional Networks}}},
  author = {Leino, Klas and Sen, Shayak and Datta, Anupam and Fredrikson, Matt and Li, Linyi},
  date = {2018-11-13},
  eprint = {1802.03788},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1802.03788},
  url = {http://arxiv.org/abs/1802.03788},
  urldate = {2022-11-28},
  abstract = {We study the problem of explaining a rich class of behavioral properties of deep neural networks. Distinctively, our influence-directed explanations approach this problem by peering inside the network to identify neurons with high influence on a quantity and distribution of interest, using an axiomatically-justified influence measure, and then providing an interpretation for the concepts these neurons represent. We evaluate our approach by demonstrating a number of its unique capabilities on convolutional neural networks trained on ImageNet. Our evaluation demonstrates that influence-directed explanations (1) identify influential concepts that generalize across instances, (2) can be used to extract the "essence" of what the network learned about a class, and (3) isolate individual features the network uses to make decisions and distinguish related classes.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/eragon/Zotero/storage/KSZYTVZ2/Leino et al. - 2018 - Influence-Directed Explanations for Deep Convoluti.pdf}
}

@article{lethamInterpretableClassifiersUsing2015,
  title = {Interpretable Classifiers Using Rules and {{Bayesian}} Analysis: {{Building}} a Better Stroke Prediction Model},
  shorttitle = {Interpretable Classifiers Using Rules and {{Bayesian}} Analysis},
  author = {Letham, Benjamin and Rudin, Cynthia and McCormick, Tyler H. and Madigan, David},
  date = {2015-09-01},
  journaltitle = {The Annals of Applied Statistics},
  shortjournal = {Ann. Appl. Stat.},
  volume = {9},
  number = {3},
  issn = {1932-6157},
  doi = {10.1214/15-AOAS848},
  url = {https://projecteuclid.org/journals/annals-of-applied-statistics/volume-9/issue-3/Interpretable-classifiers-using-rules-and-Bayesian-analysis--Building-a/10.1214/15-AOAS848.full},
  urldate = {2023-02-20},
  file = {/Users/eragon/Zotero/storage/QK3NKI6J/Letham et al. - 2015 - Interpretable classifiers using rules and Bayesian.pdf}
}

@online{liAttributeMixSemantic2020,
  title = {Attribute {{Mix}}: {{Semantic Data Augmentation}} for {{Fine Grained Recognition}}},
  shorttitle = {Attribute {{Mix}}},
  author = {Li, Hao and Zhang, Xiaopeng and Xiong, Hongkai and Tian, Qi},
  date = {2020-07-09},
  eprint = {2004.02684},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2004.02684},
  urldate = {2023-02-20},
  abstract = {Collecting fine-grained labels usually requires expert-level domain knowledge and is prohibitive to scale up. In this paper, we propose Attribute Mix, a data augmentation strategy at attribute level to expand the fine-grained samples. The principle lies in that attribute features are shared among fine-grained sub-categories, and can be seamlessly transferred among images. Toward this goal, we propose an automatic attribute mining approach to discover attributes that belong to the same super-category, and Attribute Mix is operated by mixing semantically meaningful attribute features from two images. Attribute Mix is a simple but effective data augmentation strategy that can significantly improve the recognition performance without increasing the inference budgets. Furthermore, since attributes can be shared among images from the same super-category, we further enrich the training samples with attribute level labels using images from the generic domain. Experiments on widely used fine-grained benchmarks demonstrate the effectiveness of our proposed method.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/eragon/Zotero/storage/FXHPB36H/Li et al. - 2020 - Attribute Mix Semantic Data Augmentation for Fine.pdf}
}

@article{liBrainStructuresFunctional2015,
  title = {Brain Structures and Functional Connectivity Associated with Individual Differences in {{Internet}} Tendency in Healthy Young Adults},
  author = {Li, Weiwei and Li, Yadan and Yang, Wenjing and Zhang, Qinglin and Wei, Dongtao and Li, Wenfu and Hitchman, Glenn and Qiu, Jiang},
  date = {2015-04},
  journaltitle = {Neuropsychologia},
  shortjournal = {Neuropsychologia},
  volume = {70},
  pages = {134--144},
  issn = {00283932},
  doi = {10.1016/j.neuropsychologia.2015.02.019},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0028393215000809},
  urldate = {2022-06-17},
  abstract = {Internet addiction (IA) incurs significant social and financial costs in the form of physical side-effects, academic and occupational impairment, and serious relationship problems. The majority of previous studies on Internet addiction disorders (IAD) have focused on structural and functional abnormalities, while few studies have simultaneously investigated the structural and functional brain alterations underlying individual differences in IA tendencies measured by questionnaires in a healthy sample. Here we combined structural (regional gray matter volume, rGMV) and functional (resting-state functional connectivity, rsFC) information to explore the neural mechanisms underlying IAT in a large sample of 260 healthy young adults. The results showed that IAT scores were significantly and positively correlated with rGMV in the right dorsolateral prefrontal cortex (DLPFC, one key node of the cognitive control network, CCN), which might reflect reduced functioning of inhibitory control. More interestingly, decreased anticorrelations between the right DLPFC and the medial prefrontal cortex/rostral anterior cingulate cortex (mPFC/rACC, one key node of the default mode network, DMN) were associated with higher IAT scores, which might be associated with reduced efficiency of the CCN and DMN (e.g., diminished cognitive control and self-monitoring). Furthermore, the Stroop interference effect was positively associated with the volume of the DLPFC and with the IA scores, as well as with the connectivity between DLPFC and mPFC, which further indicated that rGMV variations in the DLPFC and decreased anticonnections between the DLPFC and mPFC may reflect addiction-related reduced inhibitory control and cognitive efficiency. These findings suggest the combination of structural and functional information can provide a valuable basis for further understanding of the mechanisms and pathogenesis of IA.},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/5I7PEANZ/Li et al. - 2015 - Brain structures and functional connectivity assoc.pdf}
}

@dataset{liCaltech101,
  title = {Caltech 101},
  author = {Li, Fei-Fei and Andreetto and Marco, Marc'Aurelio and Ranzato, Marc'Aurelio},
  doi = {10.22002/D1.20086}
}

@article{liDeepLIFTDeepLabelSpecific2022,
  title = {Deep-{{LIFT}}: {{Deep Label-Specific Feature Learning}} for {{Image Annotation}}},
  shorttitle = {Deep-{{LIFT}}},
  author = {Li, Junbing and Zhang, Changqing and Zhou, Joey Tianyi and Fu, Huazhu and Xia, Shuyin and Hu, Qinghua},
  date = {2022-08},
  journaltitle = {IEEE Transactions on Cybernetics},
  shortjournal = {IEEE Trans. Cybern.},
  volume = {52},
  number = {8},
  pages = {7732--7741},
  issn = {2168-2267, 2168-2275},
  doi = {10.1109/TCYB.2021.3049630},
  url = {https://ieeexplore.ieee.org/document/9352498/},
  urldate = {2023-02-20}
}

@article{liDeepLIFTDeepLabelSpecific2022a,
  title = {Deep-{{LIFT}}: {{Deep Label-Specific Feature Learning}} for {{Image Annotation}}},
  shorttitle = {Deep-{{LIFT}}},
  author = {Li, Junbing and Zhang, Changqing and Zhou, Joey Tianyi and Fu, Huazhu and Xia, Shuyin and Hu, Qinghua},
  date = {2022-08},
  journaltitle = {IEEE Transactions on Cybernetics},
  shortjournal = {IEEE Trans. Cybern.},
  volume = {52},
  number = {8},
  pages = {7732--7741},
  issn = {2168-2267, 2168-2275},
  doi = {10.1109/TCYB.2021.3049630},
  url = {https://ieeexplore.ieee.org/document/9352498/},
  urldate = {2023-05-07},
  file = {/Users/eragon/Zotero/storage/8DSXU3YH/Li et al. - 2022 - Deep-LIFT Deep Label-Specific Feature Learning fo.pdf}
}

@unpublished{liFpnnFieldProbing2016,
  title = {Fpnn: {{Field}} Probing Neural Networks for {{3D}} Data},
  author = {Li, Yangyan and Pirk, Soeren and Su, Hao and Qi, Charles R and Guibas, Leonidas J},
  date = {2016},
  eprint = {1605.06240},
  eprinttype = {arxiv}
}

@unpublished{liHowDoesNeural2021,
  title = {How {{Does}} a {{Neural Network}}'s {{Architecture Impact Its Robustness}} to {{Noisy Labels}}?},
  author = {Li, Jingling and Zhang, Mozhi and Xu, Keyulu and Dickerson, John P. and Ba, Jimmy},
  date = {2021-11-27},
  eprint = {2012.12896},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2012.12896},
  url = {http://arxiv.org/abs/2012.12896},
  urldate = {2022-09-06},
  abstract = {Noisy labels are inevitable in large real-world datasets. In this work, we explore an area understudied by previous works -- how the network's architecture impacts its robustness to noisy labels. We provide a formal framework connecting the robustness of a network to the alignments between its architecture and target/noise functions. Our framework measures a network's robustness via the predictive power in its representations -- the test performance of a linear model trained on the learned representations using a small set of clean labels. We hypothesize that a network is more robust to noisy labels if its architecture is more aligned with the target function than the noise. To support our hypothesis, we provide both theoretical and empirical evidence across various neural network architectures and different domains. We also find that when the network is well-aligned with the target function, its predictive power in representations could improve upon state-of-the-art (SOTA) noisy-label-training methods in terms of test accuracy and even outperform sophisticated methods that use clean labels.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/eragon/Zotero/storage/P4PY7AZY/Li et al. - 2021 - How Does a Neural Network's Architecture Impact It.pdf}
}

@article{liHyperbandNovelBanditbased2017,
  title = {Hyperband: {{A}} Novel Bandit-Based Approach to Hyperparameter Optimization},
  author = {Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
  date = {2017},
  journaltitle = {The Journal of Machine Learning Research},
  volume = {18},
  number = {1},
  pages = {6765--6816},
  publisher = {{JMLR.org}}
}

@article{lillywhiteCoverageEthicsArtificial2021,
  title = {Coverage of Ethics within the Artificial Intelligence and Machine Learning Academic Literature: {{The}} Case of Disabled People},
  shorttitle = {Coverage of Ethics within the Artificial Intelligence and Machine Learning Academic Literature},
  author = {Lillywhite, Aspen and Wolbring, Gregor},
  date = {2021-05-04},
  journaltitle = {Assistive Technology},
  shortjournal = {Assistive Technology},
  volume = {33},
  number = {3},
  pages = {129--135},
  issn = {1040-0435, 1949-3614},
  doi = {10.1080/10400435.2019.1593259},
  url = {https://www.tandfonline.com/doi/full/10.1080/10400435.2019.1593259},
  urldate = {2023-01-17},
  abstract = {Disabled people are often the anticipated users of scientific and technological products and processes advanced and enabled by artificial intelligence (AI) and machine learning (ML). Disabled people are also impacted by societal impacts of AI/ML. Many ethical issues are identified within AI/ML as fields and within individual applications of AI/ML. At the same time, problems have been identified in how ethics discourses engage with disabled people. The aim of our scoping review was to better understand to what extent and how the AI/ML focused academic literature engaged with the ethics of AI/ML in relation to disabled people. Of the n = 1659 abstracts engaging with AI/ML and ethics downloaded from Scopus (which includes all Medline articles) and the 70 databases of EBSCO ALL, we found 54 relevant abstracts using the term “patient” and 11 relevant abstracts mentioning terms linked to “impair*”, “disab*” and “deaf”. Our study suggests a gap in the literature that should be filled given the many AI/ML related ethical issues identified in the literature and their impact on disabled people.},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/JVXBDRJB/Lillywhite and Wolbring - 2021 - Coverage of ethics within the artificial intellige.pdf}
}

@article{linDivergenceMeasuresBased,
  title = {Divergence {{Measures Based}} on the {{Shannon Entropy}}},
  author = {Lin, Jianhua},
  abstract = {A new class of information-theoretic divergence measures based on the Shannon entropy is introduced. Unlike the well-known Kullback divergences, the new measures do not require the condition of absolute continuity to be satisfied by the probability distributions involved. More importantly, their close relationship with the variational distance and the probability of misclassification error are established in terms of bounds. These bounds are crucial in many applications of divergence measures. The new measures are also well characterized by the properties of nonnegativity, finiteness, semiboundedness, and boundedness.},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/6U26YDRA/Lin - Divergence Measures Based on the Shannon Entropy.pdf}
}

@article{linGeometricViewpointManifold2015,
  title = {A Geometric Viewpoint of Manifold Learning},
  author = {Lin, Binbin and He, Xiaofei and Ye, Jieping},
  date = {2015-12},
  journaltitle = {Applied Informatics},
  shortjournal = {Appl Inform},
  volume = {2},
  number = {1},
  pages = {3},
  issn = {2196-0089},
  doi = {10.1186/s40535-015-0006-6},
  url = {https://applied-informatics-j.springeropen.com/articles/10.1186/s40535-015-0006-6},
  urldate = {2022-11-15},
  abstract = {In many data analysis tasks, one is often confronted with very high dimensional data. The manifold assumption, which states that the data is sampled from a submanifold embedded in much higher dimensional Euclidean space, has been widely adopted by many researchers. In the last 15 years, a large number of manifold learning algorithms have been proposed. Many of them rely on the evaluation of the geometrical and topological of the data manifold. In this paper, we present a review of these methods on a novel geometric perspective. We categorize these methods by three main groups: Laplacian-based, Hessian-based, and parallel field-based methods. We show the connection and difference between these three groups on their continuous and discrete counterparts. The discussion is focused on the problem of dimensionality reduction and semi-supervised learning.},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/XTDVR7N7/Lin et al. - 2015 - A geometric viewpoint of manifold learning.pdf}
}

@online{linNetworkNetwork2014,
  title = {Network {{In Network}}},
  author = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
  date = {2014-03-04},
  eprint = {1312.4400},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1312.4400},
  urldate = {2022-10-21},
  abstract = {We propose a novel deep network structure called "Network In Network" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.},
  pubstate = {preprint},
  version = {3},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/eragon/Zotero/storage/DNGUDZR4/Lin et al. - 2014 - Network In Network.pdf}
}

@online{linNetworkNetwork2014a,
  title = {Network {{In Network}}},
  author = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
  date = {2014-03-04},
  eprint = {1312.4400},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1312.4400},
  url = {http://arxiv.org/abs/1312.4400},
  urldate = {2022-11-18},
  abstract = {We propose a novel deep network structure called "Network In Network" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/eragon/Zotero/storage/ZYBABWJK/Lin et al. - 2014 - Network In Network.pdf}
}

@unpublished{liuConvNet2020s2022,
  title = {A {{ConvNet}} for the 2020s},
  author = {Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
  date = {2022-03-02},
  eprint = {2201.03545},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2201.03545},
  url = {http://arxiv.org/abs/2201.03545},
  urldate = {2022-05-24},
  abstract = {The "Roaring 20s" of visual recognition began with the introduction of Vision Transformers (ViTs), which quickly superseded ConvNets as the state-of-the-art image classification model. A vanilla ViT, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation. It is the hierarchical Transformers (e.g., Swin Transformers) that reintroduced several ConvNet priors, making Transformers practically viable as a generic vision backbone and demonstrating remarkable performance on a wide variety of vision tasks. However, the effectiveness of such hybrid approaches is still largely credited to the intrinsic superiority of Transformers, rather than the inherent inductive biases of convolutions. In this work, we reexamine the design spaces and test the limits of what a pure ConvNet can achieve. We gradually "modernize" a standard ResNet toward the design of a vision Transformer, and discover several key components that contribute to the performance difference along the way. The outcome of this exploration is a family of pure ConvNet models dubbed ConvNeXt. Constructed entirely from standard ConvNet modules, ConvNeXts compete favorably with Transformers in terms of accuracy and scalability, achieving 87.8\% ImageNet top-1 accuracy and outperforming Swin Transformers on COCO detection and ADE20K segmentation, while maintaining the simplicity and efficiency of standard ConvNets.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/eragon/Zotero/storage/999DKXMN/Liu et al. - 2022 - A ConvNet for the 2020s.pdf;/Users/eragon/Zotero/storage/AUAFDKDJ/2201.html}
}

@unpublished{liuConvNet2020s2022a,
  title = {A {{ConvNet}} for the 2020s},
  author = {Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
  date = {2022-03-02},
  eprint = {2201.03545},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2201.03545},
  urldate = {2022-05-24},
  abstract = {The "Roaring 20s" of visual recognition began with the introduction of Vision Transformers (ViTs), which quickly superseded ConvNets as the state-of-the-art image classification model. A vanilla ViT, on the other hand, faces difficulties when applied to general computer vision tasks such as object detection and semantic segmentation. It is the hierarchical Transformers (e.g., Swin Transformers) that reintroduced several ConvNet priors, making Transformers practically viable as a generic vision backbone and demonstrating remarkable performance on a wide variety of vision tasks. However, the effectiveness of such hybrid approaches is still largely credited to the intrinsic superiority of Transformers, rather than the inherent inductive biases of convolutions. In this work, we reexamine the design spaces and test the limits of what a pure ConvNet can achieve. We gradually "modernize" a standard ResNet toward the design of a vision Transformer, and discover several key components that contribute to the performance difference along the way. The outcome of this exploration is a family of pure ConvNet models dubbed ConvNeXt. Constructed entirely from standard ConvNet modules, ConvNeXts compete favorably with Transformers in terms of accuracy and scalability, achieving 87.8\% ImageNet top-1 accuracy and outperforming Swin Transformers on COCO detection and ADE20K segmentation, while maintaining the simplicity and efficiency of standard ConvNets.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/eragon/Zotero/storage/ZDMJIZFK/Liu et al. - 2022 - A ConvNet for the 2020s.pdf;/Users/eragon/Zotero/storage/6UILFDK7/2201.html}
}

@inproceedings{liuDataAugmentationLatent2018,
  title = {Data {{Augmentation}} via {{Latent Space Interpolation}} for {{Image Classification}}},
  booktitle = {2018 24th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  author = {Liu, Xiaofeng and Zou, Yang and Kong, Lingsheng and Diao, Zhihui and Yan, Junliang and Wang, Jun and Li, Site and Jia, Ping and You, Jane},
  date = {2018-08},
  pages = {728--733},
  publisher = {{IEEE}},
  location = {{Beijing}},
  doi = {10.1109/ICPR.2018.8545506},
  url = {https://ieeexplore.ieee.org/document/8545506/},
  urldate = {2023-02-01},
  abstract = {Effective training of the deep neural networks requires much data to avoid underdetermined and poor generalization. Data Augmentation alleviates this by using existing data more effectively. However standard data augmentation produces only limited plausible alternative data by for example, flipping, distorting, adding noise to, cropping a patch from the original samples. In this paper, we introduce the adversarial autoencoder (AAE) to impose the feature representations with uniform distribution and apply the linear interpolation on latent space, which is potential to generate a much broader set of augmentations for image classification. As a possible “recognition via generation” framework, it has potentials for several other classification tasks. Our experiments on the ILSVRC 2012, CIFAR-10 datasets show that the latent space interpolation (LSI) improves the generalization and performance of state-of-the-art deep neural networks.},
  eventtitle = {2018 24th {{International Conference}} on {{Pattern Recognition}} ({{ICPR}})},
  isbn = {978-1-5386-3788-3},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/WNUQMD5F/Liu et al. - 2018 - Data Augmentation via Latent Space Interpolation f.pdf}
}

@inproceedings{liuDensepointLearningDensely2019,
  title = {Densepoint: {{Learning}} Densely Contextual Representation for Efficient Point Cloud Processing},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Liu, Yongcheng and Fan, Bin and Meng, Gaofeng and Lu, Jiwen and Xiang, Shiming and Pan, Chunhong},
  date = {2019},
  pages = {5239--5248}
}

@online{liuMoreConvNets2020s2022,
  title = {More {{ConvNets}} in the 2020s: {{Scaling}} up {{Kernels Beyond}} 51x51 Using {{Sparsity}}},
  shorttitle = {More {{ConvNets}} in the 2020s},
  author = {Liu, Shiwei and Chen, Tianlong and Chen, Xiaohan and Chen, Xuxi and Xiao, Qiao and Wu, Boqian and Pechenizkiy, Mykola and Mocanu, Decebal and Wang, Zhangyang},
  date = {2022-09-30},
  eprint = {2207.03620},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2207.03620},
  url = {http://arxiv.org/abs/2207.03620},
  urldate = {2022-10-23},
  abstract = {Transformers have quickly shined in the computer vision world since the emergence of Vision Transformers (ViTs). The dominant role of convolutional neural networks (CNNs) seems to be challenged by increasingly effective transformer-based models. Very recently, a couple of advanced convolutional models strike back with large kernels motivated by the local-window attention mechanism, showing appealing performance and efficiency. While one of them, i.e. RepLKNet, impressively manages to scale the kernel size to 31x31 with improved performance, the performance starts to saturate as the kernel size continues growing, compared to the scaling trend of advanced ViTs such as Swin Transformer. In this paper, we explore the possibility of training extreme convolutions larger than 31x31 and test whether the performance gap can be eliminated by strategically enlarging convolutions. This study ends up with a recipe for applying extremely large kernels from the perspective of sparsity, which can smoothly scale up kernels to 61x61 with better performance. Built on this recipe, we propose Sparse Large Kernel Network (SLaK), a pure CNN architecture equipped with sparse factorized 51x51 kernels that can perform on par with or better than state-of-the-art hierarchical Transformers and modern ConvNet architectures like ConvNeXt and RepLKNet, on ImageNet classification as well as a wide range of downstream tasks including semantic segmentation on ADE20K, object detection on PASCAL VOC 2007, and object detection/segmentation on MS COCO.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/eragon/Zotero/storage/TP3ZV2CB/Liu et al. - 2022 - More ConvNets in the 2020s Scaling up Kernels Bey.pdf}
}

@unpublished{liuRelationShapeConvolutionalNeural2019,
  title = {Relation-{{Shape Convolutional Neural Network}} for {{Point Cloud Analysis}}},
  author = {Liu, Yongcheng and Fan, Bin and Xiang, Shiming and Pan, Chunhong},
  date = {2019-05-25},
  eprint = {1904.07601},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1904.07601},
  urldate = {2022-05-10},
  abstract = {Point cloud analysis is very challenging, as the shape implied in irregular points is difficult to capture. In this paper, we propose RS-CNN, namely, Relation-Shape Convolutional Neural Network, which extends regular grid CNN to irregular configuration for point cloud analysis. The key to RS-CNN is learning from relation, i.e., the geometric topology constraint among points. Specifically, the convolutional weight for local point set is forced to learn a high-level relation expression from predefined geometric priors, between a sampled point from this point set and the others. In this way, an inductive local representation with explicit reasoning about the spatial layout of points can be obtained, which leads to much shape awareness and robustness. With this convolution as a basic operator, RS-CNN, a hierarchical architecture can be developed to achieve contextual shape-aware learning for point cloud analysis. Extensive experiments on challenging benchmarks across three tasks verify RS-CNN achieves the state of the arts.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computational Geometry,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Robotics},
  file = {/Users/eragon/Zotero/storage/MNFJHGTQ/Liu et al. - 2019 - Relation-Shape Convolutional Neural Network for Po.pdf;/Users/eragon/Zotero/storage/2M4C8Z3Y/1904.html}
}

@misc{liuRelationShapeConvolutionalNeural2019a,
  title = {Relation-{{Shape Convolutional Neural Network}} for {{Point Cloud Analysis}}},
  author = {Liu, Yongcheng and Fan, Bin and Xiang, Shiming and Pan, Chunhong},
  date = {2019}
}

@unpublished{liuSwinTransformerHierarchical2021,
  title = {Swin {{Transformer}}: {{Hierarchical Vision Transformer}} Using {{Shifted Windows}}},
  shorttitle = {Swin {{Transformer}}},
  author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  date = {2021-08-17},
  eprint = {2103.14030},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2103.14030},
  url = {http://arxiv.org/abs/2103.14030},
  urldate = {2022-05-24},
  abstract = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with \textbackslash textbf\{S\}hifted \textbackslash textbf\{win\}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at\textasciitilde\textbackslash url\{https://github.com/microsoft/Swin-Transformer\}.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/eragon/Zotero/storage/VYTFS4U7/Liu et al. - 2021 - Swin Transformer Hierarchical Vision Transformer .pdf}
}

@online{liuSwinTransformerV22022,
  title = {Swin {{Transformer V2}}: {{Scaling Up Capacity}} and {{Resolution}}},
  shorttitle = {Swin {{Transformer V2}}},
  author = {Liu, Ze and Hu, Han and Lin, Yutong and Yao, Zhuliang and Xie, Zhenda and Wei, Yixuan and Ning, Jia and Cao, Yue and Zhang, Zheng and Dong, Li and Wei, Furu and Guo, Baining},
  date = {2022-04-11},
  eprint = {2111.09883},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2111.09883},
  urldate = {2022-10-21},
  abstract = {Large-scale NLP models have been shown to significantly improve the performance on language tasks with no signs of saturation. They also demonstrate amazing few-shot capabilities like that of human beings. This paper aims to explore large-scale models in computer vision. We tackle three major issues in training and application of large vision models, including training instability, resolution gaps between pre-training and fine-tuning, and hunger on labelled data. Three main techniques are proposed: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) A log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) A self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images. Through these techniques, this paper successfully trained a 3 billion-parameter Swin Transformer V2 model, which is the largest dense vision model to date, and makes it capable of training with images of up to 1,536\$\textbackslash times\$1,536 resolution. It set new performance records on 4 representative vision tasks, including ImageNet-V2 image classification, COCO object detection, ADE20K semantic segmentation, and Kinetics-400 video action classification. Also note our training is much more efficient than that in Google's billion-level visual models, which consumes 40 times less labelled data and 40 times less training time. Code is available at \textbackslash url\{https://github.com/microsoft/Swin-Transformer\}.},
  pubstate = {preprint},
  version = {2},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/eragon/Zotero/storage/I53USPS2/Liu et al. - 2022 - Swin Transformer V2 Scaling Up Capacity and Resol.pdf}
}

@article{liVisualizingLossLandscape,
  title = {Visualizing the {{Loss Landscape}} of {{Neural Nets}}},
  author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  abstract = {Neural network training relies on our ability to find “good” minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and wellchosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effect on the underlying loss landscape, are not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple “filter normalization” method that helps us visualize loss function curvature and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/BHP4A5SP/Li et al. - Visualizing the Loss Landscape of Neural Nets.pdf}
}

@inproceedings{liVisualizingLossLandscape2018,
  title = {Visualizing the {{Loss Landscape}} of {{Neural Nets}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  date = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2018/hash/a41b3bb3e6b050b6c9067c67f663b915-Abstract.html},
  urldate = {2023-03-27},
  abstract = {Neural network training relies on our ability to find "good" minimizers of highly non-convex loss functions. It is well known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effect on the underlying loss landscape, is not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple "filter normalization" method that helps us visualize loss function curvature, and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.},
  file = {/Users/eragon/Zotero/storage/G9DSS3BH/Li et al. - 2018 - Visualizing the Loss Landscape of Neural Nets.pdf}
}

@online{loshchilovDecoupledWeightDecay2019,
  title = {Decoupled {{Weight Decay Regularization}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  date = {2019-01-04},
  eprint = {1711.05101},
  eprinttype = {arxiv},
  eprintclass = {cs, math},
  doi = {10.48550/arXiv.1711.05101},
  url = {http://arxiv.org/abs/1711.05101},
  urldate = {2023-05-09},
  abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \textbackslash emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common implementations of these algorithms employ L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by \textbackslash emph\{decoupling\} the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). Our proposed decoupled weight decay has already been adopted by many researchers, and the community has implemented it in TensorFlow and PyTorch; the complete source code for our experiments is available at https://github.com/loshchil/AdamW-and-SGDW},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Optimization and Control},
  file = {/Users/eragon/Zotero/storage/54R5MVU4/Loshchilov and Hutter - 2019 - Decoupled Weight Decay Regularization.pdf}
}

@article{losWarningStimulusRetrieval2021,
  title = {The Warning Stimulus as Retrieval Cue: {{The}} Role of Associative Memory in Temporal Preparation},
  shorttitle = {The Warning Stimulus as Retrieval Cue},
  author = {Los, Sander A. and Nieuwenstein, Jurre and Bouharab, Anass and Stephens, David J. and Meeter, Martijn and Kruijne, Wouter},
  date = {2021-03-01},
  journaltitle = {Cognitive Psychology},
  shortjournal = {Cognitive Psychology},
  volume = {125},
  pages = {101378},
  issn = {0010-0285},
  doi = {10.1016/j.cogpsych.2021.101378},
  url = {https://www.sciencedirect.com/science/article/pii/S0010028521000013},
  urldate = {2022-11-28},
  abstract = {In a warned reaction time task, the warning stimulus (S1) initiates a process of temporal preparation, which promotes a speeded response to the impending target stimulus (S2). According to the multiple trace theory of temporal preparation (MTP), participants learn the timing of S2 by storing a memory trace on each trial, which contains a temporal profile of the events on that trial. On each new trial, S1 serves as a retrieval cue that implicitly and associatively activates memory traces created on earlier trials, which jointly drive temporal preparation for S2. The idea that S1 assumes this role as a retrieval cue was tested across eight experiments, in which two different S1s were associated with two different distributions of S1-S2 intervals: one with predominantly short and one with predominantly long intervals. Experiments differed regarding the S1 features that made up a pair, ranging from highly distinct (e.g., tone and flash) to more similar (e.g., red and green flash) and verbal (i.e., “short” vs “long”). Exclusively for pairs of highly distinct S1s, the results showed that the S1 cue modified temporal preparation, even in participants who showed no awareness of the contingency. This cueing effect persisted in a subsequent transfer phase, in which the contingency between S1 and the timing of S2 was broken – a fact participants were informed of in advance. Together, these findings support the role of S1 as an implicit retrieval cue, consistent with MTP.},
  langid = {english},
  keywords = {Associative learning,Long-term memory,Temporal orienting,Temporal preparation},
  file = {/Users/eragon/Zotero/storage/6K22DPKT/Los et al. - 2021 - The warning stimulus as retrieval cue The role of.pdf}
}

@inproceedings{loweObjectRecognitionLocal1999,
  title = {Object Recognition from Local Scale-Invariant Features},
  booktitle = {Proceedings of the Seventh {{IEEE}} International Conference on Computer Vision},
  author = {Lowe, David G},
  date = {1999},
  volume = {2},
  pages = {1150--1157},
  publisher = {{Ieee}}
}

@inproceedings{lundbergUnifiedApproachInterpreting2017,
  title = {A {{Unified Approach}} to {{Interpreting Model Predictions}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Lundberg, Scott M and Lee, Su-In},
  date = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html},
  urldate = {2022-11-28},
  abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
  file = {/Users/eragon/Zotero/storage/H2QC8KUK/Lundberg and Lee - 2017 - A Unified Approach to Interpreting Model Predictio.pdf}
}

@online{luPretrainedTransformersUniversal2021,
  title = {Pretrained {{Transformers}} as {{Universal Computation Engines}}},
  author = {Lu, Kevin and Grover, Aditya and Abbeel, Pieter and Mordatch, Igor},
  date = {2021-06-30},
  eprint = {2103.05247},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2103.05247},
  urldate = {2022-05-24},
  abstract = {We investigate the capability of a transformer pretrained on natural language to generalize to other modalities with minimal finetuning – in particular, without finetuning of the self-attention and feedforward layers of the residual blocks. We consider such a model, which we call a Frozen Pretrained Transformer (FPT), and study finetuning it on a variety of sequence classification tasks spanning numerical computation, vision, and protein fold prediction. In contrast to prior works which investigate finetuning on the same modality as the pretraining dataset, we show that pretraining on natural language can improve performance and compute efficiency on non-language downstream tasks. Additionally, we perform an analysis of the architecture, comparing the performance of a random initialized transformer to a random LSTM. Combining the two insights, we find language-pretrained transformers can obtain strong performance on a variety of non-language tasks1.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/eragon/Zotero/storage/3KZTMW2F/Lu et al. - 2021 - Pretrained Transformers as Universal Computation E.pdf}
}

@article{maci�tRESEARCHSOCIALMEDIA2018,
  title = {A {{RESEARCH ON SOCIAL MEDIA ADDICTION AND DOPAMINE DRIVEN FEEDBACK}}},
  author = {Maci̇t, Hüseyin Bilal and Maci̇t, Gamze and Güngör, Orhan},
  date = {2018-12-27},
  journaltitle = {Mehmet Akif Ersoy Üniversitesi İktisadi ve İdari Bilimler Fakültesi Dergisi},
  volume = {5},
  number = {3},
  pages = {882--897},
  issn = {2149-1658},
  doi = {10.30798/makuiibf.435845},
  url = {https://dergipark.org.tr/en/pub/makuiibf/issue/41626/435845},
  urldate = {2022-06-15},
  abstract = {Human relationships in societies consisted of face-to-face relationships until the middle of the 20th century. Throughout their lives, people have established social relationships with a limited number of people, sharing their sadness and happiness with them. The great technological developments coming up to date and began with the invention of the transistor in the mid-20th century allowed the technological communication tools to enter through our pockets and caused great changes in the way of communication of the societies. The virtual chat culture, which started with the introduction of the Internet into the houses, has reached the dimension of media sharing with the spread of mobile devices. Everyone in society has got the chance to become mediatic and famous, and many of them have started to make an effort for it. However, statistical and medical researches have made in recent years that this attractive media is addictive. Research has been conducted on changes in chemical movements and physiological behavior of individuals’ brain and nervous system. In this study, statistical information of the year 2018 on the use of internet and social media in the world are presented. In addition, the concept of addiction, addicted behavior and symptoms were examined. Research made about substances which cause biological and psychological addiction like alcohol, cigarettes and pills and chemicals which affect on behaviour of individuals such as dopamine. Symptoms of mania, depression and bipolar disorder were examined, and similarities between behaviors of individuals using social media for a long time and individuals who are addicted and sick were examined.},
  issue = {3},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/RVN8BTVM/Maci̇t et al. - 2018 - A RESEARCH ON SOCIAL MEDIA ADDICTION AND DOPAMINE .pdf}
}

@article{maLearningMultiViewRepresentation2019,
  title = {Learning {{Multi-View Representation With LSTM}} for 3-{{D Shape Recognition}} and {{Retrieval}}},
  author = {Ma, Chao and Guo, Yulan and Yang, Jungang and An, Wei},
  date = {2019-05},
  journaltitle = {IEEE Transactions on Multimedia},
  shortjournal = {IEEE Trans. Multimedia},
  volume = {21},
  number = {5},
  pages = {1169--1182},
  issn = {1520-9210, 1941-0077},
  doi = {10.1109/TMM.2018.2875512},
  url = {https://ieeexplore.ieee.org/document/8490588/},
  urldate = {2022-05-10},
  abstract = {Shape representation for 3-D models is an important topic in computer vision, multimedia analysis, and computer graphics. Recent multiview-based methods demonstrate promising performance for 3-D shape recognition and retrieval. However, most multiview-based methods ignore the correlations of multiple views or suffer from high computional cost. In this paper, we propose a novel multiview-based network architecture for 3-D shape recognition and retrieval. Our network combines convolutional neural networks (CNNs) with long short-term memory (LSTM) to exploit the correlative information from multiple views. Well-pretrained CNNs with residual connections are first used to extract a low-level feature of each view image rendered from a 3-D shape. Then, a LSTM and a sequence voting layer are employed to aggregate these features into a shape descriptor. The highway network and a three-step training strategy are also adopted to boost the optimization of the deep network. Experimental results on two public datasets demonstrate that the proposed method achieves promising performance for 3-D shape recognition and the state-of-the-art performance for the 3-D shape retrieval.},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/8SCQU9C9/Ma et al. - 2019 - Learning Multi-View Representation With LSTM for 3.pdf}
}

@inproceedings{mallyaPiggybackAdaptingSingle2018,
  title = {Piggyback: {{Adapting}} a {{Single Network}} to {{Multiple Tasks}} by {{Learning}} to {{Mask Weights}}},
  shorttitle = {Piggyback},
  author = {Mallya, Arun and Davis, Dillon and Lazebnik, Svetlana},
  date = {2018},
  pages = {67--82},
  url = {https://openaccess.thecvf.com/content_ECCV_2018/html/Arun_Mallya_Piggyback_Adapting_a_ECCV_2018_paper.html},
  urldate = {2023-05-22},
  eventtitle = {Proceedings of the {{European Conference}} on {{Computer Vision}} ({{ECCV}})},
  file = {/Users/eragon/Zotero/storage/Q3R9FH5A/Mallya et al. - 2018 - Piggyback Adapting a Single Network to Multiple T.pdf}
}

@article{ManifoldHypothesis2022,
  title = {Manifold {{Hypothesis}}},
  date = {2022},
  pages = {21},
  abstract = {When are gradient-based explanations meaningful? We propose a necessary criterion: explanations need to be aligned with the tangent space of the data manifold. To test this hypothesis, we employ autoencoders to estimate and generate data manifolds. Across a range of different datasets – MNIST, EMNIST, CIFAR10, X-ray pneumonia and Diabetic Retinopathy detection – we demonstrate empirically that the more an explanation is aligned with the tangent space of the data, the more interpretable it tends to be. In particular, popular post-hoc explanation methods such as Integrated Gradients and SmoothGrad tend to align their results with the data manifold. The same is true for the outcome of adversarial training, which has been claimed to lead to more interpretable explanations. Empirically, alignment with the data manifold happens early during training, and to some degree even when training with random labels. However, we theoretically prove that good generalization of neural networks does not imply good or bad alignment of model gradients with the data manifold. This leads to a number of interesting follow-up questions regarding gradient-based explanations.},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/W9CTURFM/2022 - Manifold Hypothesis.pdf}
}

@online{marcinkevicsInterpretabilityExplainabilityMachine2023,
  title = {Interpretability and {{Explainability}}: {{A Machine Learning Zoo Mini-tour}}},
  shorttitle = {Interpretability and {{Explainability}}},
  author = {Marcinkevičs, Ričards and Vogt, Julia E.},
  date = {2023-03-01},
  eprint = {2012.01805},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2012.01805},
  urldate = {2023-03-09},
  abstract = {In this review, we examine the problem of designing interpretable and explainable machine learning models. Interpretability and explainability lie at the core of many machine learning and statistical applications in medicine, economics, law, and natural sciences. Although interpretability and explainability have escaped a clear universal definition, many techniques motivated by these properties have been developed over the recent 30 years with the focus currently shifting towards deep learning methods. In this review, we emphasise the divide between interpretability and explainability and illustrate these two different research directions with concrete examples of the state-of-the-art. The review is intended for a general machine learning audience with interest in exploring the problems of interpretation and explanation beyond logistic regression or random forest variable importance. This work is not an exhaustive literature survey, but rather a primer focusing selectively on certain lines of research which the authors found interesting or informative.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/eragon/Zotero/storage/SG7PEFBQ/Marcinkevičs and Vogt - 2023 - Interpretability and Explainability A Machine Lea.pdf}
}

@inproceedings{maturanaVoxNet3DConvolutional2015,
  title = {{{VoxNet}}: {{A 3D}} Convolutional Neural Network for Real-Time Object Recognition},
  booktitle = {2015 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Maturana, Daniel and Scherer, Sebastian},
  date = {2015},
  publisher = {{IEEE}}
}

@article{mehrabiSurveyBiasFairness2021,
  title = {A {{Survey}} on {{Bias}} and {{Fairness}} in {{Machine Learning}}},
  author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
  date = {2021-07-13},
  journaltitle = {ACM Computing Surveys},
  shortjournal = {ACM Comput. Surv.},
  volume = {54},
  number = {6},
  pages = {115:1--115:35},
  issn = {0360-0300},
  doi = {10.1145/3457607},
  url = {https://doi.org/10.1145/3457607},
  urldate = {2022-09-06},
  abstract = {With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.},
  keywords = {deep learning,Fairness and bias in artificial intelligence,machine learning,natural language processing,representation learning},
  file = {/Users/eragon/Zotero/storage/3X7JEMXP/Mehrabi et al. - 2021 - A Survey on Bias and Fairness in Machine Learning.pdf}
}

@online{melas-kyriaziMathematicalFoundationsManifold2020,
  title = {The {{Mathematical Foundations}} of {{Manifold Learning}}},
  author = {Melas-Kyriazi, Luke},
  date = {2020-10-30},
  eprint = {2011.01307},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2011.01307},
  urldate = {2022-11-15},
  abstract = {Manifold learning is a popular and quickly-growing subfield of machine learning based on the assumption that one's observed data lie on a low-dimensional manifold embedded in a higher-dimensional space. This thesis presents a mathematical perspective on manifold learning, delving into the intersection of kernel learning, spectral graph theory, and differential geometry. Emphasis is placed on the remarkable interplay between graphs and manifolds, which forms the foundation for the widely-used technique of manifold regularization. This work is written to be accessible to a broad mathematical audience, including machine learning researchers and practitioners interested in understanding the theorems underlying popular manifold learning algorithms and dimensionality reduction techniques.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/eragon/Zotero/storage/G4YANMUW/Melas-Kyriazi - 2020 - The Mathematical Foundations of Manifold Learning.pdf}
}

@unpublished{micikeviciusMixedPrecisionTraining2017,
  title = {Mixed Precision Training},
  author = {Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and others},
  date = {2017},
  eprint = {1710.03740},
  eprinttype = {arxiv}
}

@online{micikeviciusMixedPrecisionTraining2018,
  title = {Mixed {{Precision Training}}},
  author = {Micikevicius, Paulius and Narang, Sharan and Alben, Jonah and Diamos, Gregory and Elsen, Erich and Garcia, David and Ginsburg, Boris and Houston, Michael and Kuchaiev, Oleksii and Venkatesh, Ganesh and Wu, Hao},
  date = {2018-02-15},
  eprint = {1710.03740},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1710.03740},
  urldate = {2023-04-11},
  abstract = {Deep neural networks have enabled progress in a wide variety of applications. Growing the size of the neural network typically results in improved accuracy. As model sizes grow, the memory and compute requirements for training these models also increases. We introduce a technique to train deep neural networks using half precision floating point numbers. In our technique, weights, activations and gradients are stored in IEEE half-precision format. Half-precision floating numbers have limited numerical range compared to single-precision numbers. We propose two techniques to handle this loss of information. Firstly, we recommend maintaining a single-precision copy of the weights that accumulates the gradients after each optimizer step. This single-precision copy is rounded to half-precision format during training. Secondly, we propose scaling the loss appropriately to handle the loss of information with half-precision gradients. We demonstrate that this approach works for a wide variety of models including convolution neural networks, recurrent neural networks and generative adversarial networks. This technique works for large scale models with more than 100 million parameters trained on large datasets. Using this approach, we can reduce the memory consumption of deep learning models by nearly 2x. In future processors, we can also expect a significant computation speedup using half-precision hardware units.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/eragon/Zotero/storage/9PYB7T5N/Micikevicius et al. - 2018 - Mixed Precision Training.pdf}
}

@online{miconiBackpropamineTrainingSelfmodifying2020,
  title = {Backpropamine: Training Self-Modifying Neural Networks with Differentiable Neuromodulated Plasticity},
  shorttitle = {Backpropamine},
  author = {Miconi, Thomas and Rawal, Aditya and Clune, Jeff and Stanley, Kenneth O.},
  date = {2020-02-24},
  eprint = {2002.10585},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2002.10585},
  url = {http://arxiv.org/abs/2002.10585},
  urldate = {2023-05-22},
  abstract = {The impressive lifelong learning in animal brains is primarily enabled by plastic changes in synaptic connectivity. Importantly, these changes are not passive, but are actively controlled by neuromodulation, which is itself under the control of the brain. The resulting self-modifying abilities of the brain play an important role in learning and adaptation, and are a major basis for biological reinforcement learning. Here we show for the first time that artificial neural networks with such neuromodulated plasticity can be trained with gradient descent. Extending previous work on differentiable Hebbian plasticity, we propose a differentiable formulation for the neuromodulation of plasticity. We show that neuromodulated plasticity improves the performance of neural networks on both reinforcement learning and supervised learning tasks. In one task, neuromodulated plastic LSTMs with millions of parameters outperform standard LSTMs on a benchmark language modeling task (controlling for the number of parameters). We conclude that differentiable neuromodulation of plasticity offers a powerful new framework for training neural networks.},
  pubstate = {preprint},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {/Users/eragon/Zotero/storage/CZECKRCZ/Miconi et al. - 2020 - Backpropamine training self-modifying neural netw.pdf}
}

@online{millerExplainableAIBeware2017,
  title = {Explainable {{AI}}: {{Beware}} of {{Inmates Running}} the {{Asylum Or}}: {{How I Learnt}} to {{Stop Worrying}} and {{Love}} the {{Social}} and {{Behavioural Sciences}}},
  shorttitle = {Explainable {{AI}}},
  author = {Miller, Tim and Howe, Piers and Sonenberg, Liz},
  date = {2017-12-04},
  eprint = {1712.00547},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1712.00547},
  url = {http://arxiv.org/abs/1712.00547},
  urldate = {2022-11-24},
  abstract = {In his seminal book `The Inmates are Running the Asylum: Why High-Tech Products Drive Us Crazy And How To Restore The Sanity' [2004, Sams Indianapolis, IN, USA], Alan Cooper argues that a major reason why software is often poorly designed (from a user perspective) is that programmers are in charge of design decisions, rather than interaction designers. As a result, programmers design software for themselves, rather than for their target audience, a phenomenon he refers to as the `inmates running the asylum'. This paper argues that explainable AI risks a similar fate. While the re-emergence of explainable AI is positive, this paper argues most of us as AI researchers are building explanatory agents for ourselves, rather than for the intended users. But explainable AI is more likely to succeed if researchers and practitioners understand, adopt, implement, and improve models from the vast and valuable bodies of research in philosophy, psychology, and cognitive science, and if evaluation of these models is focused more on people than on technology. From a light scan of literature, we demonstrate that there is considerable scope to infuse more results from the social and behavioural sciences into explainable AI, and present some key results from these fields that are relevant to explainable AI.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/eragon/Zotero/storage/9W8R7Q8N/Miller et al. - 2017 - Explainable AI Beware of Inmates Running the Asyl.pdf}
}

@unpublished{mitsuharaEmbeddingHumanKnowledge2019,
  title = {Embedding {{Human Knowledge}} into {{Deep Neural Network}} via {{Attention Map}}},
  author = {Mitsuhara, Masahiro and Fukui, Hiroshi and Sakashita, Yusuke and Ogata, Takanori and Hirakawa, Tsubasa and Yamashita, Takayoshi and Fujiyoshi, Hironobu},
  date = {2019-12-19},
  eprint = {1905.03540},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1905.03540},
  urldate = {2022-10-03},
  abstract = {In this work, we aim to realize a method for embedding human knowledge into deep neural networks. While the conventional method to embed human knowledge has been applied for non-deep machine learning, it is challenging to apply it for deep learning models due to the enormous number of model parameters. To tackle this problem, we focus on the attention mechanism of an attention branch network (ABN). In this paper, we propose a fine-tuning method that utilizes a single-channel attention map which is manually edited by a human expert. Our fine-tuning method can train a network so that the output attention map corresponds to the edited ones. As a result, the fine-tuned network can output an attention map that takes into account human knowledge. Experimental results with ImageNet, CUB-200-2010, and IDRiD demonstrate that it is possible to obtain a clear attention map for a visual explanation and improve the classification performance. Our findings can be a novel framework for optimizing networks through human intuitive editing via a visual interface and suggest new possibilities for human-machine cooperation in addition to the improvement of visual explanations.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,done},
  file = {/Users/eragon/Zotero/storage/2SVPKSTS/Mitsuhara et al. - 2019 - Embedding Human Knowledge into Deep Neural Network.pdf}
}

@online{mitsuharaEmbeddingHumanKnowledge2019a,
  title = {Embedding {{Human Knowledge}} into {{Deep Neural Network}} via {{Attention Map}}},
  author = {Mitsuhara, Masahiro and Fukui, Hiroshi and Sakashita, Yusuke and Ogata, Takanori and Hirakawa, Tsubasa and Yamashita, Takayoshi and Fujiyoshi, Hironobu},
  date = {2019-12-19},
  eprint = {1905.03540},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1905.03540},
  url = {http://arxiv.org/abs/1905.03540},
  urldate = {2023-05-08},
  abstract = {In this work, we aim to realize a method for embedding human knowledge into deep neural networks. While the conventional method to embed human knowledge has been applied for non-deep machine learning, it is challenging to apply it for deep learning models due to the enormous number of model parameters. To tackle this problem, we focus on the attention mechanism of an attention branch network (ABN). In this paper, we propose a fine-tuning method that utilizes a single-channel attention map which is manually edited by a human expert. Our fine-tuning method can train a network so that the output attention map corresponds to the edited ones. As a result, the fine-tuned network can output an attention map that takes into account human knowledge. Experimental results with ImageNet, CUB-200-2010, and IDRiD demonstrate that it is possible to obtain a clear attention map for a visual explanation and improve the classification performance. Our findings can be a novel framework for optimizing networks through human intuitive editing via a visual interface and suggest new possibilities for human-machine cooperation in addition to the improvement of visual explanations.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/eragon/Zotero/storage/UFXBIUL3/Mitsuhara et al. - 2019 - Embedding Human Knowledge into Deep Neural Network.pdf}
}

@article{mittelstadtPrinciplesAloneCannot2019,
  title = {Principles Alone Cannot Guarantee Ethical {{AI}}},
  author = {Mittelstadt, Brent},
  date = {2019-11-04},
  journaltitle = {Nature Machine Intelligence},
  shortjournal = {Nat Mach Intell},
  volume = {1},
  number = {11},
  pages = {501--507},
  issn = {2522-5839},
  doi = {10.1038/s42256-019-0114-4},
  url = {https://www.nature.com/articles/s42256-019-0114-4},
  urldate = {2023-01-17},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/GGNRTJLZ/Mittelstadt - 2019 - Principles alone cannot guarantee ethical AI.pdf}
}

@report{momennejadOfflineReplaySupports2017,
  type = {preprint},
  title = {Offline {{Replay Supports Planning}}: {{fMRI Evidence}} from {{Reward Revaluation}}},
  shorttitle = {Offline {{Replay Supports Planning}}},
  author = {Momennejad, Ida and Ross Otto, A. and Daw, Nathaniel D. and Norman, Kenneth A.},
  date = {2017-10-02},
  institution = {{Neuroscience}},
  doi = {10.1101/196758},
  url = {http://biorxiv.org/lookup/doi/10.1101/196758},
  urldate = {2022-05-27},
  abstract = {Making decisions in sequentially structured tasks requires integrating distally acquired information. The extensive computational cost of such integration challenges planning methods that integrate online, at decision time. Furthermore, it remains unclear whether “offline” integration during replay supports planning, and if so which memories should be replayed. Inspired by machine learning, we propose that (a) offline replay of trajectories facilitates integrating representations that guide decisions, and (b) unsigned prediction errors (uncertainty) trigger such integrative replay. We designed a 2-step revaluation task for fMRI, whereby participants needed to integrate changes in rewards with past knowledge to optimally replan decisions. As predicted, we found that (a) multi-voxel pattern evidence for off-task replay predicts subsequent replanning; (b) neural sensitivity to uncertainty predicts subsequent replay and replanning; (c) off-task hippocampus and anterior cingulate activity increase when revaluation is required. These findings elucidate how the brain leverages offline mechanisms in planning and goaldirected behavior under uncertainty.},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/3FBAMY7W/Momennejad et al. - 2017 - Offline Replay Supports Planning fMRI Evidence fr.pdf}
}

@article{montavonMethodsInterpretingUnderstanding2018,
  title = {Methods for {{Interpreting}} and {{Understanding Deep Neural Networks}}},
  author = {Montavon, Grégoire and Samek, Wojciech and Müller, Klaus-Robert},
  date = {2018-02},
  journaltitle = {Digital Signal Processing},
  shortjournal = {Digital Signal Processing},
  volume = {73},
  eprint = {1706.07979},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  pages = {1--15},
  issn = {10512004},
  doi = {10.1016/j.dsp.2017.10.011},
  url = {http://arxiv.org/abs/1706.07979},
  urldate = {2022-11-24},
  abstract = {This paper provides an entry point to the problem of interpreting a deep neural network model and explaining its predictions. It is based on a tutorial given at ICASSP 2017. It introduces some recently proposed techniques of interpretation, along with theory, tricks and recommendations, to make most efficient use of these techniques on real data. It also discusses a number of practical applications.},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/eragon/Zotero/storage/R96FQEWE/Montavon et al. - 2018 - Methods for Interpreting and Understanding Deep Ne.pdf}
}

@online{MoodRepresentationMomentum,
  title = {Mood as {{Representation}} of {{Momentum}} | {{Elsevier Enhanced Reader}}},
  doi = {10.1016/j.tics.2015.07.010},
  url = {https://reader.elsevier.com/reader/sd/pii/S1364661315001746?token=52D5CDE080CFCCB0A2DB3DF7FBB520945AFD13439A2A5F94287E6E10A3DAF7CE517F685221C4189C3CAD1073A2546694&originRegion=eu-west-1&originCreation=20220527120802},
  urldate = {2022-05-27},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/T9PV9KK5/Mood as Representation of Momentum  Elsevier Enha.pdf;/Users/eragon/Zotero/storage/KWSUIBWI/S1364661315001746.html}
}

@inproceedings{moosavi-dezfooliDeepFoolSimpleAccurate2016,
  title = {{{DeepFool}}: {{A Simple}} and {{Accurate Method}} to {{Fool Deep Neural Networks}}},
  shorttitle = {{{DeepFool}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Frossard, Pascal},
  date = {2016-06},
  pages = {2574--2582},
  publisher = {{IEEE}},
  location = {{Las Vegas, NV, USA}},
  doi = {10.1109/CVPR.2016.282},
  url = {http://ieeexplore.ieee.org/document/7780651/},
  urldate = {2023-02-20},
  eventtitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-4673-8851-1},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/MNIGFFPR/Moosavi-Dezfooli et al. - 2016 - DeepFool A Simple and Accurate Method to Fool Dee.pdf}
}

@article{morleyEthicsAIHealth2020,
  title = {The Ethics of {{AI}} in Health Care: {{A}} Mapping Review},
  shorttitle = {The Ethics of {{AI}} in Health Care},
  author = {Morley, Jessica and Machado, Caio C.V. and Burr, Christopher and Cowls, Josh and Joshi, Indra and Taddeo, Mariarosaria and Floridi, Luciano},
  date = {2020-09},
  journaltitle = {Social Science \& Medicine},
  shortjournal = {Social Science \& Medicine},
  volume = {260},
  pages = {113172},
  issn = {02779536},
  doi = {10.1016/j.socscimed.2020.113172},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0277953620303919},
  urldate = {2023-01-17},
  abstract = {This article presents a mapping review of the literature concerning the ethics of artificial intelligence (AI) in health care. The goal of this review is to summarise current debates and identify open questions for future research. Five literature databases were searched to support the following research question: how can the primary ethical risks presented by AI-health be categorised, and what issues must policymakers, regulators and developers consider in order to be ‘ethically mindful? A series of screening stages were carried out—for example, removing articles that focused on digital health in general (e.g. data sharing, data access, data privacy, surveillance/nudging, consent, ownership of health data, evidence of efficacy)—yielding a total of 156 papers that were included in the review.},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/KKP8B7T8/Morley et al. - 2020 - The ethics of AI in health care A mapping review.pdf}
}

@inproceedings{morrisonClosingLoopRobotic2018,
  title = {Closing the {{Loop}} for {{Robotic Grasping}}: {{A Real-time}}, {{Generative Grasp Synthesis Approach}}},
  booktitle = {Proc.\textbackslash{} of {{Robotics}}: {{Science}} and {{Systems}} ({{RSS}})},
  author = {Morrison, Douglas and Corke, Peter and Leitner, Jürgen},
  date = {2018}
}

@online{mudrakartaDidModelUnderstand2018,
  title = {Did the {{Model Understand}} the {{Question}}?},
  author = {Mudrakarta, Pramod Kaushik and Taly, Ankur and Sundararajan, Mukund and Dhamdhere, Kedar},
  date = {2018-05-14},
  eprint = {1805.05492},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1805.05492},
  url = {http://arxiv.org/abs/1805.05492},
  urldate = {2022-11-28},
  abstract = {We analyze state-of-the-art deep learning models for three tasks: question answering on (1) images, (2) tables, and (3) passages of text. Using the notion of \textbackslash emph\{attribution\} (word importance), we find that these deep networks often ignore important question terms. Leveraging such behavior, we perturb questions to craft a variety of adversarial examples. Our strongest attacks drop the accuracy of a visual question answering model from \$61.1\textbackslash\%\$ to \$19\textbackslash\%\$, and that of a tabular question answering model from \$33.5\textbackslash\%\$ to \$3.3\textbackslash\%\$. Additionally, we show how attributions can strengthen attacks proposed by Jia and Liang (2017) on paragraph comprehension models. Our results demonstrate that attributions can augment standard measures of accuracy and empower investigation of model performance. When a model is accurate but for the wrong reasons, attributions can surface erroneous logic in the model that indicates inadequacies in the test data.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/eragon/Zotero/storage/87TE479A/Mudrakarta et al. - 2018 - Did the Model Understand the Question.pdf}
}

@online{muellerExplanationHumanAISystems2019,
  title = {Explanation in {{Human-AI Systems}}: {{A Literature Meta-Review}}, {{Synopsis}} of {{Key Ideas}} and {{Publications}}, and {{Bibliography}} for {{Explainable AI}}},
  shorttitle = {Explanation in {{Human-AI Systems}}},
  author = {Mueller, Shane T. and Hoffman, Robert R. and Clancey, William and Emrey, Abigail and Klein, Gary},
  date = {2019-02-05},
  eprint = {1902.01876},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1902.01876},
  url = {http://arxiv.org/abs/1902.01876},
  urldate = {2022-11-24},
  abstract = {This is an integrative review that address the question, "What makes for a good explanation?" with reference to AI systems. Pertinent literatures are vast. Thus, this review is necessarily selective. That said, most of the key concepts and issues are expressed in this Report. The Report encapsulates the history of computer science efforts to create systems that explain and instruct (intelligent tutoring systems and expert systems). The Report expresses the explainability issues and challenges in modern AI, and presents capsule views of the leading psychological theories of explanation. Certain articles stand out by virtue of their particular relevance to XAI, and their methods, results, and key points are highlighted. It is recommended that AI/XAI researchers be encouraged to include in their research reports fuller details on their empirical or experimental methods, in the fashion of experimental psychology research reports: details on Participants, Instructions, Procedures, Tasks, Dependent Variables (operational definitions of the measures and metrics), Independent Variables (conditions), and Control Conditions.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/eragon/Zotero/storage/6X7WSPJM/Mueller et al. - 2019 - Explanation in Human-AI Systems A Literature Meta.pdf;/Users/eragon/Zotero/storage/YS246RSU/Mueller et al. - 2019 - Explanation in Human-AI Systems A Literature Meta.pdf}
}

@article{munozExtendingGGCNNAutomated,
  title = {Extending {{GG-CNN}} through {{Automated Model Space Exploration}} Using {{Knowledge Transfer}}},
  author = {Muñoz, Mario Ríos and Schomaker, Lambert and Kasaei, S Hamidreza}
}

@article{nasserCueReactivityYoungAdults2020,
  title = {Cue-{{Reactivity Among Young Adults With Problematic Instagram Use}} in {{Response}} to {{Instagram-Themed Risky Behavior Cues}}: {{A Pilot fMRI Study}}},
  shorttitle = {Cue-{{Reactivity Among Young Adults With Problematic Instagram Use}} in {{Response}} to {{Instagram-Themed Risky Behavior Cues}}},
  author = {Nasser, Nisha Syed and Sharifat, Hamed and Rashid, Aida Abdul and Hamid, Suzana Ab and Rahim, Ezamin Abdul and Loh, Jia Ling and Ching, Siew Mooi and Hoo, Fan Kee and Ismail, Siti Irma Fadillah and Tyagi, Rohit and Mohammad, Mazlyfarina and Suppiah, Subapriya},
  date = {2020},
  journaltitle = {Frontiers in Psychology},
  volume = {11},
  issn = {1664-1078},
  url = {https://www.frontiersin.org/article/10.3389/fpsyg.2020.556060},
  urldate = {2022-06-15},
  abstract = {BackgroundProblematic Instagram use (PIGU), a specific type of internet addiction, is prevalent among adolescents and young adults. In certain instances, Instagram acts as a platform for exhibiting photos of risk-taking behavior that the subjects with PIGU upload to gain likes as a surrogate for gaining peer acceptance and popularity.AimsThe primary objective was to evaluate whether addiction-specific cues compared with neutral cues, i.e., negative emotional valence cues vs. positive emotional valence cues, would elicit activation of the dopaminergic reward network (i.e., precuneus, nucleus accumbens, and amygdala) and consecutive deactivation of the executive control network [i.e., medial prefrontal cortex (mPFC) and dorsolateral prefrontal cortex (dlPFC)], in the PIGU subjects.MethodAn fMRI cue-induced reactivity study was performed using negative emotional valence, positive emotional valence, and truly neutral cues, using Instagram themes. Thirty subjects were divided into PIGU and healthy control (HC) groups, based on a set of diagnostic criteria using behavioral tests, including the Modified Instagram Addiction Test (IGAT), to assess the severity of PIGU. In-scanner recordings of the subjects’ responses to the images and regional activity of the neural addiction pathways were recorded.ResultsNegative emotional valence {$>$} positive emotional valence cues elicited increased activations in the precuneus in the PIGU group. A negative and moderate correlation was observed between PSC at the right mPFC with the IGAT scores of the PIGU subjects when corrected for multiple comparisons [r = −0.777, (p {$<$} 0.004, two-tailed)].ConclusionAddiction-specific Instagram-themed cues identify the neurobiological underpinnings of Instagram addiction. Activations of the dopaminergic reward system and deactivation of the executive control network indicate converging neuropathological pathways between Instagram addiction and other types of addictions.},
  file = {/Users/eragon/Zotero/storage/554SVQFF/Nasser et al. - 2020 - Cue-Reactivity Among Young Adults With Problematic.pdf}
}

@report{nasserValidationEmotionalStimuli2020,
  type = {preprint},
  title = {Validation of {{Emotional Stimuli Flashcards}} for {{Conducting}} ‘{{Response}} to {{Reward}}’ {{fMRI}} Study among {{Malaysian}} Students},
  author = {Nasser, Nisha Syed and Sharifat, Hamed and Rashid, Aida Abdul and Hamid, Suzana Ab and Rahim, Ezamin Abdul and Mohamad, Mazlyfarina and Tyagi, Rohit and Ismail, Siti Irma Fadhilah and Mooi, Ching Siew and Suppiah, Subapriya},
  date = {2020-01-20},
  institution = {{Psychiatry and Clinical Psychology}},
  doi = {10.1101/2020.01.17.20017202},
  url = {http://medrxiv.org/lookup/doi/10.1101/2020.01.17.20017202},
  urldate = {2022-06-15},
  abstract = {Problematic Instagram Use (PIGU) is a specific-Internet-addiction disorder observed among the youth of today. fMRI, is able to objectively assess regional brain activation in response to addiction-specific rewards, e.g. viewing picture flashcards. Pictures uploaded onto Instagram by PIGUs have often been associated with risky behaviours in their efforts to gain more ‘Likes’, thus it is hypothesized that PIGUs are more drawn to ‘Negative-Emotional’ cues. To date, there is no local database with addiction-specific cues. Objective: To conduct an out-of-scanner validation study to create a database of pictures using ‘Negative-Emotional’ cues that evoke responses of arousal among PIGUs. Method: Forty-four Malaysian undergraduate students (20 PIGUs, 24 controls) were randomly recruited based on the evaluation using the Smartphone-Addiction-ScaleMalay version (SAS-M) and modified Instagram Addiction Test (IGAT); and fulfilled Lin et al. (2016) definition of addiction. They were administered 200 content-specific pictures that were multidimensional i.e. arousal (excitation/relaxation effects), approach-avoidance (motivational direction) and emotional valence (positive/negative feelings) to describe the PIGUs perception of the psychological properties of the pictures using a 9-point Likert scale. Results: PIGUs viewing ‘Negative-Emotional’ cues demonstrated significant positive correlations between arousal \& valence (z = 4.834, p {$<$} .001, effect size = 0.69) and arousal \& avoidance-approach (z = 4.625, p {$<$} .001, effect size= 0.66) as compared to controls and were more frequently aroused by ‘NegativeEmotional’ type of stimuli. Conclusion: A database of validated, addiction-specific pictures can be developed to potentiate any future cue-induced response to reward fMRI studies to assess PIGU.},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/MB4LH6V5/Nasser et al. - 2020 - Validation of Emotional Stimuli Flashcards for Con.pdf}
}

@online{naveedSurveyImageMixing2023,
  title = {Survey: {{Image Mixing}} and {{Deleting}} for {{Data Augmentation}}},
  shorttitle = {Survey},
  author = {Naveed, Humza and Anwar, Saeed and Hayat, Munawar and Javed, Kashif and Mian, Ajmal},
  date = {2023-02-06},
  eprint = {2106.07085},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2106.07085},
  url = {http://arxiv.org/abs/2106.07085},
  urldate = {2023-03-31},
  abstract = {Neural networks are prone to overfitting and memorizing data patterns. To avoid over-fitting and enhance their generalization and performance, various methods have been suggested in the literature, including dropout, regularization, label smoothing, etc. One such method is augmentation which introduces different types of corruption in the data to prevent the model from overfitting and to memorize patterns present in the data. A sub-area of data augmentation is image mixing and deleting. This specific type of augmentation either deletes image regions or mixes two images to hide or make particular characteristics of images confusing for the network, forcing it to emphasize the overall structure of the object in an image. Models trained with this approach have proven to perform and generalize well compared to those trained without image mixing or deleting. An added benefit that comes with this method of training is robustness against image corruption. Due to its low computational cost and recent success, researchers have proposed many image mixing and deleting techniques. We furnish an in-depth survey of image mixing and deleting techniques and provide categorization via their most distinguishing features. We initiate our discussion with some fundamental relevant concepts. Next, we present essentials, such as each category's strengths and limitations, describing their working mechanism, basic formulations, and applications. We also discuss the general challenges and recommend possible future research directions for image mixing and deleting data augmentation techniques. Datasets and codes for evaluation are publicly available here.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/eragon/Zotero/storage/GQFF7RGP/Naveed et al. - 2023 - Survey Image Mixing and Deleting for Data Augment.pdf;/Users/eragon/Zotero/storage/VWVT6GT3/Naveed et al. - 2023 - Survey Image Mixing and Deleting for Data Augment.pdf}
}

@article{nazarSystematicReviewHuman2021,
  title = {A {{Systematic Review}} of {{Human}}–{{Computer Interaction}} and {{Explainable Artificial Intelligence}} in {{Healthcare With Artificial Intelligence Techniques}}},
  author = {Nazar, Mobeen and Alam, Muhammad Mansoor and Yafi, Eiad and Su’ud, Mazliham Mohd},
  date = {2021},
  journaltitle = {IEEE Access},
  volume = {9},
  pages = {153316--153348},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3127881},
  abstract = {Artificial intelligence (AI) is one of the emerging technologies. In recent decades, artificial intelligence (AI) has gained widespread acceptance in a variety of fields, including virtual support, healthcare, and security. Human-Computer Interaction (HCI) is a field that has been combining AI and human-computer engagement over the past several years in order to create an interactive intelligent system for user interaction. AI, in conjunction with HCI, is being used in a variety of fields by employing various algorithms and employing HCI to provide transparency to the user, allowing them to trust the machine. The comprehensive examination of both the areas of AI and HCI, as well as their subfields, has been explored in this work. The main goal of this article was to discover a point of intersection between the two fields. The understanding of Explainable Artificial Intelligence (XAI), which is a linking point of HCI and XAI, was gained through a literature review conducted in this research. The literature survey encompassed themes identified in the literature (such as XAI and its areas, major XAI aims, and XAI problems and challenges). The study’s other major focus was on the use of AI, HCI, and XAI in healthcare. The poll also addressed the shortcomings in XAI in healthcare, as well as the field’s future potential. As a result, the literature indicates that XAI in healthcare is still a novel subject that has to be explored more in the future.},
  eventtitle = {{{IEEE Access}}},
  keywords = {Artificial intelligence,Data models,deep learning,explainable artificial intelligence,healthcare,Human computer interaction,human-centered design,human-computer interaction,machine learning,Medical services,Security,Systematics,usability,Usability,user-centered design},
  file = {/Users/eragon/Zotero/storage/SQVS5MKD/Nazar et al. - 2021 - A Systematic Review of Human–Computer Interaction .pdf}
}

@inproceedings{neyshaburWhatBeingTransferred2020,
  title = {What Is Being Transferred in Transfer Learning?},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Neyshabur, Behnam and Sedghi, Hanie and Zhang, Chiyuan},
  date = {2020},
  volume = {33},
  pages = {512--523},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2020/hash/0607f4c705595b911a4f3e7a127b44e0-Abstract.html},
  urldate = {2023-02-27},
  abstract = {One desired capability for machines is the ability to transfer their understanding of one domain to another domain where data is (usually) scarce. Despite ample adaptation of transfer learning in many deep learning applications, we yet do not understand what enables a successful transfer and which part of the network is responsible for that. In this paper, we provide new tools and analysis to address these fundamental questions. Through a series of analysis on transferring to block-shuffled images, we separate the effect of feature reuse from learning high-level statistics of data and show that some benefit of transfer learning comes from the latter. We present that when training from pre-trained weights, the model stays in the same basin in the loss landscape and different instances of such model are similar in feature space and close in parameter space.},
  file = {/Users/eragon/Zotero/storage/KQJ89APD/Neyshabur et al. - 2020 - What is being transferred in transfer learning.pdf}
}

@inproceedings{nguyenDeepNeuralNetworks2015,
  title = {Deep Neural Networks Are Easily Fooled: {{High}} Confidence Predictions for Unrecognizable Images},
  shorttitle = {Deep Neural Networks Are Easily Fooled},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
  date = {2015-06},
  pages = {427--436},
  publisher = {{IEEE}},
  location = {{Boston, MA, USA}},
  doi = {10.1109/CVPR.2015.7298640},
  url = {http://ieeexplore.ieee.org/document/7298640/},
  urldate = {2023-03-27},
  abstract = {Deep neural networks (DNNs) have recently been achieving state-of-the-art performance on a variety of pattern-recognition tasks, most notably visual classification problems. Given that DNNs are now able to classify objects in images with near-human-level performance, questions naturally arise as to what differences remain between computer and human vision. A recent study [30] revealed that changing an image (e.g. of a lion) in a way imperceptible to humans can cause a DNN to label the image as something else entirely (e.g. mislabeling a lion a library). Here we show a related result: it is easy to produce images that are completely unrecognizable to humans, but that state-of-theart DNNs believe to be recognizable objects with 99.99\% confidence (e.g. labeling with certainty that white noise static is a lion). Specifically, we take convolutional neural networks trained to perform well on either the ImageNet or MNIST datasets and then find images with evolutionary algorithms or gradient ascent that DNNs label with high confidence as belonging to each dataset class. It is possible to produce images totally unrecognizable to human eyes that DNNs believe with near certainty are familiar objects, which we call “fooling images” (more generally, fooling examples). Our results shed light on interesting differences between human vision and current DNNs, and raise questions about the generality of DNN computer vision.},
  eventtitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-4673-6964-0},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/SCNN7EAY/Nguyen et al. - 2015 - Deep neural networks are easily fooled High confi.pdf}
}

@video{nmhClassicalPlaylistWhen2023,
  entrysubtype = {video},
  title = {Classical Playlist for When You're on Deadline.},
  editor = {{nmh}},
  editortype = {director},
  date = {2023-04-25},
  url = {https://www.youtube.com/watch?v=4Ds-tmtvNC0},
  urldate = {2023-06-06},
  abstract = {Classical playlist when you're on deadline. When you’re a procrastinator and you need some music that makes you panic to the max. spotify: https://open.spotify.com/playlist/19K... 📚 Work \& Study with me: ~~~•~1~hour~Work~\&~Stu...~~ timestamp:  00:00 Dover Quartet - The Four Seasons Summer 3 Presto Vivaldi 02:34 Brooklyn Classical - Piano Sonata No 11 in A Major - Mvt 3 Turkish March Mozart 06:12 Dover Quartet - String Quartet No12 American- Mvt 4 Dvořák 11:42 Raviv Leibzirer - Scherzo Allegro 14:38 Raviv Leibzirer - Allegro assai 19:23 Jean-Miles Carter - Ghost Allegro 22:42 Dover Quartet - String Quartet No2 in A Minor - Mvt 1 Mendelssohn 30:39 Brooklyn Classical - Queen of the Night Mozart 33:44 Raviv Leibzirer - Allegro con brio ----------------------------------------------------------- - Video edited by  @nmh0413   - Camera i use: Canon Eos R,  24-105mm F4, 50mm F1.8.  - Việt Nam, freelancer, photography,... - Instagram: https://www.instagram.com/ngmhiep\_22/ - If you like my videos or want to support me:   + Buy me a coffee: buymeacoffee.com/ngmhiep1304   + Momo: 0335948121 (Nguyễn Minh Hiệp) ------------------------------------------------------------ Thank you! \#classicalmusic \#stopprocrastination \#musicfordeadline}
}

@incollection{nunnariOverlapGradCAMSaliency2021,
  title = {On the {{Overlap Between Grad-CAM Saliency Maps}} and {{Explainable Visual Features}} in {{Skin Cancer Images}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Extraction}}},
  author = {Nunnari, Fabrizio and Kadir, Md Abdul and Sonntag, Daniel},
  editor = {Holzinger, Andreas and Kieseberg, Peter and Tjoa, A Min and Weippl, Edgar},
  date = {2021},
  volume = {12844},
  pages = {241--253},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-84060-0_16},
  url = {https://link.springer.com/10.1007/978-3-030-84060-0_16},
  urldate = {2022-10-03},
  abstract = {Dermatologists recognize melanomas by inspecting images in which they identify human-comprehensible visual features. In this paper, we investigate to what extent such features correspond to the saliency areas identified on CNNs trained for classification. Our experiments, conducted on two neural architectures characterized by different depth and different resolution of the last convolutional layer, quantify to what extent thresholded Grad-CAM saliency maps can be used to identify visual features of skin cancer. We found that the best threshold value, i.e., the threshold at which we can measure the highest Jaccard index, varies significantly among features; ranging from 0.3 to 0.7. In addition, we measured Jaccard indices as high as 0.143, which is almost 50\% of the performance of state-of-the-art architectures specialized in feature mask prediction at pixel-level, such as U-Net. Finally, a breakdown test between malignancy and classification correctness shows that higher resolution saliency maps could help doctors in spotting wrong classifications.},
  isbn = {978-3-030-84059-4 978-3-030-84060-0},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/33H2VKZ5/Nunnari et al. - 2021 - On the Overlap Between Grad-CAM Saliency Maps and .pdf}
}

@article{ogaraComparingDataAugmentation,
  title = {Comparing {{Data Augmentation Strategies}} for {{Deep Image Classification}}},
  author = {O'Gara, Sarah and McGuinness, Kevin},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/8IJDDIUD/O'Gara and McGuinness - Comparing Data Augmentation Strategies for Deep Im.pdf}
}

@article{ogaraComparingDataAugmentationa,
  title = {Comparing {{Data Augmentation Strategies}} for {{Deep Image Classification}}},
  author = {O'Gara, Sarah and McGuinness, Kevin},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/WGEI9KX7/O'Gara and McGuinness - Comparing Data Augmentation Strategies for Deep Im.pdf}
}

@inproceedings{oquabObjectLocalizationFree2015,
  title = {Is Object Localization for Free? - {{Weakly-supervised}} Learning with Convolutional Neural Networks},
  shorttitle = {Is Object Localization for Free?},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Oquab, Maxime and Bottou, Leon and Laptev, Ivan and Sivic, Josef},
  date = {2015-06},
  pages = {685--694},
  publisher = {{IEEE}},
  location = {{Boston, MA, USA}},
  doi = {10.1109/CVPR.2015.7298668},
  url = {http://ieeexplore.ieee.org/document/7298668/},
  urldate = {2022-10-21},
  abstract = {Successful methods for visual object recognition typically rely on training datasets containing lots of richly annotated images. Detailed image annotation, e.g. by object bounding boxes, however, is both expensive and often subjective. We describe a weakly supervised convolutional neural network (CNN) for object classification that relies only on image-level labels, yet can learn from cluttered scenes containing multiple objects. We quantify its object classification and object location prediction performance on the Pascal VOC 2012 (20 object classes) and the much larger Microsoft COCO (80 object classes) datasets. We find that the network (i) outputs accurate image-level labels, (ii) predicts approximate locations (but not extents) of objects, and (iii) performs comparably to its fully-supervised counterparts using object bounding box annotation for training.},
  eventtitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-4673-6964-0},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/G38P7ZRA/Oquab et al. - 2015 - Is object localization for free - Weakly-supervis.pdf}
}

@article{oyamaInfluenceImageClassification2018,
  title = {Influence of Image Classification Accuracy on Saliency Map Estimation},
  author = {Oyama, Taiki and Yamanaka, Takao},
  date = {2018},
  journaltitle = {CAAI Transactions on Intelligence Technology},
  volume = {3},
  number = {3},
  pages = {140--152},
  issn = {2468-2322},
  doi = {10.1049/trit.2018.1012},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/trit.2018.1012},
  urldate = {2022-10-03},
  abstract = {Saliency map estimation in computer vision aims to estimate the locations where people gaze in images. Since people tend to look at objects in images, the parameters of the model pre-trained on ImageNet for image classification are useful for the saliency map estimation. However, there is no research on the relationship between the image classification accuracy and the performance of the saliency map estimation. In this study, it is shown that there is a strong correlation between image classification accuracy and saliency map estimation accuracy. The authors also investigated the effective architecture based on multi-scale images and the up-sampling layers to refine the saliency-map resolution. The model achieved the state-of-the-art accuracy on the PASCAL-S, OSIE, and MIT1003 datasets. In the MIT saliency benchmark, the model achieved the best performance in some metrics and competitive results in the other metrics.},
  langid = {english},
  keywords = {(B6135) Optical,(C5260B) Computer vision and image processing techniques,computer vision,image and video signal processing,image classification,ImageNet,MIT1003,multiscale images,OSIE,PASCAL-S,saliency-map resolution,up-sampling layer},
  file = {/Users/eragon/Zotero/storage/HHL5L9G9/Oyama and Yamanaka - 2018 - Influence of image classification accuracy on sali.pdf}
}

@inproceedings{pageShapeAnalysisAlgorithm2003,
  title = {Shape Analysis Algorithm Based on Information Theory},
  booktitle = {Proceedings 2003 {{International Conference}} on {{Image Processing}} ({{Cat}}. {{No}}. {{03CH37429}})},
  author = {Page, David L and Koschan, Andreas F and Sukumar, Sreenivas R and Roui-Abidi, Besma and Abidi, Mongi A},
  date = {2003},
  volume = {1},
  pages = {I--229},
  publisher = {{IEEE}}
}

@inproceedings{palacioWhatDeepNetworks2018,
  title = {What Do {{Deep Networks Like}} to {{See}}?},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Palacio, Sebastian and Folz, Joachim and Hees, Jorn and Raue, Federico and Borth, Damian and Dengel, Andreas},
  date = {2018-06},
  pages = {3108--3117},
  publisher = {{IEEE}},
  location = {{Salt Lake City, UT}},
  doi = {10.1109/CVPR.2018.00328},
  url = {https://ieeexplore.ieee.org/document/8578426/},
  urldate = {2023-05-25},
  abstract = {We propose a novel way to measure and understand convolutional neural networks by quantifying the amount of input signal they let in. To do this, an autoencoder (AE) was fine-tuned on gradients from a pre-trained classifier with fixed parameters. We compared the reconstructed samples from AEs that were fine-tuned on a set of image classifiers (AlexNet, VGG16, ResNet-50, and Inception v3) and found substantial differences. The AE learns which aspects of the input space to preserve and which ones to ignore, based on the information encoded in the backpropagated gradients. Measuring the changes in accuracy when the signal of one classifier is used by a second one, a relation of total order emerges. This order depends directly on each classifier’s input signal but it does not correlate with classification accuracy or network size. Further evidence of this phenomenon is provided by measuring the normalized mutual information between original images and auto-encoded reconstructions from different fine-tuned AEs. These findings break new ground in the area of neural network understanding, opening a new way to reason, debug, and interpret their results. We present four concrete examples in the literature where observations can now be explained in terms of the input signal that a model uses.},
  eventtitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-5386-6420-9},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/CQ3QR2C9/Palacio et al. - 2018 - What do Deep Networks Like to See.pdf}
}

@inproceedings{palacioXAIHandbookUnified2021,
  title = {{{XAI Handbook}}: {{Towards}} a {{Unified Framework}} for {{Explainable AI}}},
  shorttitle = {{{XAI Handbook}}},
  author = {Palacio, Sebastian and Lucieri, Adriano and Munir, Mohsin and Ahmed, Sheraz and Hees, Jörn and Dengel, Andreas},
  date = {2021},
  pages = {3766--3775},
  url = {https://openaccess.thecvf.com/content/ICCV2021W/RPRMI/html/Palacio_XAI_Handbook_Towards_a_Unified_Framework_for_Explainable_AI_ICCVW_2021_paper.html},
  urldate = {2023-05-25},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/BGYGGDUL/Palacio et al. - 2021 - XAI Handbook Towards a Unified Framework for Expl.pdf}
}

@unpublished{parmarImageTransformer2018,
  title = {Image {{Transformer}}},
  author = {Parmar, Niki and Vaswani, Ashish and Uszkoreit, Jakob and Kaiser, Łukasz and Shazeer, Noam and Ku, Alexander and Tran, Dustin},
  date = {2018-06-15},
  eprint = {1802.05751},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1802.05751},
  url = {http://arxiv.org/abs/1802.05751},
  urldate = {2022-05-24},
  abstract = {Image generation has been successfully cast as an autoregressive sequence generation or transformation problem. Recent work has shown that self-attention is an effective way of modeling textual sequences. In this work, we generalize a recently proposed model architecture based on self-attention, the Transformer, to a sequence modeling formulation of image generation with a tractable likelihood. By restricting the self-attention mechanism to attend to local neighborhoods we significantly increase the size of images the model can process in practice, despite maintaining significantly larger receptive fields per layer than typical convolutional neural networks. While conceptually simple, our generative models significantly outperform the current state of the art in image generation on ImageNet, improving the best published negative log-likelihood on ImageNet from 3.83 to 3.77. We also present results on image super-resolution with a large magnification ratio, applying an encoder-decoder configuration of our architecture. In a human evaluation study, we find that images generated by our super-resolution model fool human observers three times more often than the previous state of the art.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/eragon/Zotero/storage/P6NJDBGX/Parmar et al. - 2018 - Image Transformer.pdf;/Users/eragon/Zotero/storage/SLBCRG5Z/1802.html}
}

@article{patersonChildrenInterpretationAmbiguous2006,
  title = {Children's {{Interpretation}} of {{Ambiguous Focus}} in {{Sentences With}} "{{Only}}"},
  author = {Paterson, Kevin B. and Liversedge, Simon P. and White, Diane and Filik, Ruth and Jaz, Kristina},
  date = {2006-07},
  journaltitle = {Language Acquisition},
  shortjournal = {Language Acquisition},
  volume = {13},
  number = {3},
  pages = {253--284},
  issn = {1048-9223, 1532-7817},
  doi = {10.1207/s15327817la1303_4},
  url = {http://www.tandfonline.com/doi/abs/10.1207/s15327817la1303_4},
  urldate = {2022-10-24},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/8C7FZLYL/Paterson et al. - 2006 - Children's Interpretation of Ambiguous Focus in Se.pdf}
}

@inproceedings{pepik3dObjectClass2015,
  title = {3d Object Class Detection in the Wild},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}}},
  author = {Pepik, Bojan and Stark, Michael and Gehler, Peter and Ritschel, Tobias and Schiele, Bernt},
  date = {2015},
  pages = {1--10}
}

@online{petsiukRISERandomizedInput2018,
  title = {{{RISE}}: {{Randomized Input Sampling}} for {{Explanation}} of {{Black-box Models}}},
  shorttitle = {{{RISE}}},
  author = {Petsiuk, Vitali and Das, Abir and Saenko, Kate},
  date = {2018-09-25},
  eprint = {1806.07421},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1806.07421},
  urldate = {2023-02-20},
  abstract = {Deep neural networks are being used increasingly to automate data analysis and decision making, yet their decision-making process is largely unclear and is difficult to explain to the end users. In this paper, we address the problem of Explainable AI for deep neural networks that take images as input and output a class probability. We propose an approach called RISE that generates an importance map indicating how salient each pixel is for the model’s prediction. In contrast to white-box approaches that estimate pixel importance using gradients or other internal network state, RISE works on blackbox models. It estimates importance empirically by probing the model with randomly masked versions of the input image and obtaining the corresponding outputs. We compare our approach to state-of-the-art importance extraction methods using both an automatic deletion/insertion metric and a pointing metric based on human-annotated object segments. Extensive experiments on several benchmark datasets show that our approach matches or exceeds the performance of other methods, including white-box approaches.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/eragon/Zotero/storage/VD3KHYFH/Petsiuk et al. - 2018 - RISE Randomized Input Sampling for Explanation of.pdf}
}

@online{poliHyenaHierarchyLarger2023,
  title = {Hyena {{Hierarchy}}: {{Towards Larger Convolutional Language Models}}},
  shorttitle = {Hyena {{Hierarchy}}},
  author = {Poli, Michael and Massaroli, Stefano and Nguyen, Eric and Fu, Daniel Y. and Dao, Tri and Baccus, Stephen and Bengio, Yoshua and Ermon, Stefano and Ré, Christopher},
  date = {2023-04-19},
  eprint = {2302.10866},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2302.10866},
  url = {http://arxiv.org/abs/2302.10866},
  urldate = {2023-05-11},
  abstract = {Recent advances in deep learning have relied heavily on the use of large Transformers due to their ability to learn at scale. However, the core building block of Transformers, the attention operator, exhibits quadratic cost in sequence length, limiting the amount of context accessible. Existing subquadratic methods based on low-rank and sparse approximations need to be combined with dense attention layers to match Transformers, indicating a gap in capability. In this work, we propose Hyena, a subquadratic drop-in replacement for attention constructed by interleaving implicitly parametrized long convolutions and data-controlled gating. In recall and reasoning tasks on sequences of thousands to hundreds of thousands of tokens, Hyena improves accuracy by more than 50 points over operators relying on state-spaces and other implicit and explicit methods, matching attention-based models. We set a new state-of-the-art for dense-attention-free architectures on language modeling in standard datasets (WikiText103 and The Pile), reaching Transformer quality with a 20\% reduction in training compute required at sequence length 2K. Hyena operators are twice as fast as highly optimized attention at sequence length 8K, and 100x faster at sequence length 64K.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/eragon/Zotero/storage/3YM2F3FY/Poli et al. - 2023 - Hyena Hierarchy Towards Larger Convolutional Lang.pdf}
}

@article{polonskyWhatImage2005,
  title = {What’s in an Image?},
  author = {Polonsky, Oleg and Patané, Giuseppe and Biasotti, Silvia and Gotsman, Craig and Spagnuolo, Michela},
  date = {2005},
  journaltitle = {The Visual Computer},
  volume = {21},
  number = {8},
  pages = {840--847},
  publisher = {{Springer}}
}

@article{polynMemorySearchNeural2008,
  title = {Memory Search and the Neural Representation of Context},
  author = {Polyn, Sean M. and Kahana, Michael J.},
  date = {2008-01},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {12},
  number = {1},
  pages = {24--30},
  issn = {13646613},
  doi = {10.1016/j.tics.2007.10.010},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661307003038},
  urldate = {2022-05-27},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/VCPKC2M8/Polyn and Kahana - 2008 - Memory search and the neural representation of con.pdf}
}

@article{popelTrainingTipsTransformer2018,
  title = {Training {{Tips}} for the {{Transformer Model}}},
  author = {Popel, Martin and Bojar, Ondřej},
  date = {2018-04-01},
  journaltitle = {The Prague Bulletin of Mathematical Linguistics},
  volume = {110},
  number = {1},
  eprint = {1804.00247},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {43--70},
  issn = {1804-0462},
  doi = {10.2478/pralin-2018-0002},
  url = {http://arxiv.org/abs/1804.00247},
  urldate = {2022-05-24},
  abstract = {This article describes our experiments in neural machine translation using the recent Tensor2Tensor framework and the Transformer sequence-to-sequence model (Vaswani et al., 2017). We examine some of the critical parameters that affect the final translation quality, memory usage, training stability and training time, concluding each experiment with a set of recommendations for fellow researchers. In addition to confirming the general mantra "more data and larger models", we address scaling to multiple GPUs and provide practical tips for improved training regarding batch size, learning rate, warmup steps, maximum sentence length and checkpoint averaging. We hope that our observations will allow others to get better results given their particular hardware and data constraints.},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/eragon/Zotero/storage/RV85QH7H/Popel and Bojar - 2018 - Training Tips for the Transformer Model.pdf;/Users/eragon/Zotero/storage/XBK96DBJ/1804.html}
}

@article{portolesCharacterizingSynchronyPatterns2018,
  title = {Characterizing Synchrony Patterns across Cognitive Task Stages of Associative Recognition Memory},
  author = {Portoles, Oscar and Borst, Jelmer P. and family=Vugt, given=Marieke K., prefix=van, useprefix=true},
  date = {2018-10},
  journaltitle = {European Journal of Neuroscience},
  shortjournal = {Eur J Neurosci},
  volume = {48},
  number = {8},
  pages = {2759--2769},
  issn = {0953816X},
  doi = {10.1111/ejn.13817},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/ejn.13817},
  urldate = {2022-05-27},
  abstract = {Numerous studies seek to understand the role of oscillatory synchronization in cognition. This problem is particularly challenging in the context of complex cognitive behavior, which consists of a sequence of processing steps with uncertain duration. In this study, we analyzed oscillatory connectivity measures in time windows that previous computational models had associated with a specific sequence of processing steps in an associative memory recognition task (visual encoding, familiarity, memory retrieval, decision making, and motor response). The timing of these processing steps was estimated on a single-trial basis with a novel hidden semi-Markov model multivariate pattern analysis (HSMM-MVPA) method. We show that different processing stages are associated with specific patterns of oscillatory connectivity. Visual encoding is characterized by a dense network connecting frontal, posterior, and temporal areas as well as frontal and occipital phase locking in the 4–9 Hz theta band. Familiarity is associated with frontal phase locking in the 9–14 Hz alpha band. Decision making is associated with frontal and temporo-central interhemispheric connections in the alpha band. During decision making, a second network in the theta band that connects left-temporal, central, and occipital areas bears similarity to the neural signature for preparing a motor response. A similar theta band network is also present during the motor response, with additionally alpha band connectivity between righttemporal and posterior areas. This demonstrates that the processing stages discovered with the HSMM-MVPA method are indeed linked to distinct synchronization patterns, leading to a closer understanding of the functional role of oscillations in cognition.},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/PZEH5W6Q/Portoles et al. - 2018 - Characterizing synchrony patterns across cognitive.pdf}
}

@article{pyntaPowerSocialTelevision2014,
  title = {The {{Power}} of {{Social Television}}: {{Can Social Media Build Viewer Engagement}}?: {{A New Approach}} to {{Brain Imaging}} of {{Viewer Immersion}}},
  shorttitle = {The {{Power}} of {{Social Television}}},
  author = {Pynta, Peter and Seixas, Shaun A. S. and Nield, Geoffrey E. and Hier, James and Millward, Emelia and Silberstein, Richard B.},
  date = {2014-03-01},
  journaltitle = {Journal of Advertising Research},
  volume = {54},
  number = {1},
  pages = {71--80},
  publisher = {{Journal of Advertising Research}},
  issn = {0021-8499},
  doi = {10.2501/JAR-54-1-071-080},
  url = {http://www.journalofadvertisingresearch.com/content/54/1/71},
  urldate = {2022-06-15},
  abstract = {Marketers everywhere are paying close attention to radical changes in consumer behavior and engagement provoked by the rise of digital technology. In today's household, it is a common occurrence to share viewing experience across at least two screens: the television and secondary Internet-enabled devices. The current study used Steady State Topography (SST), a brain-activity recording methodology to explore this relationship. Participants' neural responses were recorded while they watched a live television broadcast and were allowed to freely interact on social-media platforms Twitter and Fango. The results indicate that engaging in social media while viewing television can significantly enhance neural indicators of viewer engagement in the television program.},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/9WAB2DS8/Pynta et al. - 2014 - The Power of Social Television Can Social Media B.pdf;/Users/eragon/Zotero/storage/ICVEB5W3/71.html}
}

@online{qinResizeMixMixingData2020,
  title = {{{ResizeMix}}: {{Mixing Data}} with {{Preserved Object Information}} and {{True Labels}}},
  shorttitle = {{{ResizeMix}}},
  author = {Qin, Jie and Fang, Jiemin and Zhang, Qian and Liu, Wenyu and Wang, Xingang and Wang, Xinggang},
  date = {2020-12-20},
  eprint = {2012.11101},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2012.11101},
  urldate = {2023-03-29},
  abstract = {Data augmentation is a powerful technique to increase the diversity of data, which can effectively improve the generalization ability of neural networks in image recognition tasks. Recent data mixing based augmentation strategies have achieved great success. Especially, CutMix uses a simple but effective method to improve the classifiers by randomly cropping a patch from one image and pasting it on another image. To further promote the performance of CutMix, a series of works explore to use the saliency information of the image to guide the mixing. We systematically study the importance of the saliency information for mixing data, and find that the saliency information is not so necessary for promoting the augmentation performance. Furthermore, we find that the cutting based data mixing methods carry two problems of label misallocation and object information missing, which cannot be resolved simultaneously. We propose a more effective but very easily implemented method, namely ResizeMix. We mix the data by directly resizing the source image to a small patch and paste it on another image. The obtained patch preserves more substantial object information compared with conventional cut-based methods. ResizeMix shows evident advantages over CutMix and the saliency-guided methods on both image classification and object detection tasks without additional computation cost, which even outperforms most costly search-based automatic augmentation methods.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/eragon/Zotero/storage/N5QGSZ8V/Qin et al. - 2020 - ResizeMix Mixing Data with Preserved Object Infor.pdf}
}

@inproceedings{qiVolumetricMultiviewCNNs2016,
  title = {Volumetric and Multi-View {{CNNs}} for Object Classification on {{3D}} Data},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Qi, Charles R and Su, Hao and Nießner, Matthias and Dai, Angela and Yan, Mengyuan and Guibas, Leonidas J},
  date = {2016},
  pages = {5648--5656}
}

@online{quSketchXAIFirstLook2023,
  title = {{{SketchXAI}}: {{A First Look}} at {{Explainability}} for {{Human Sketches}}},
  shorttitle = {{{SketchXAI}}},
  author = {Qu, Zhiyu and Gryaditskaya, Yulia and Li, Ke and Pang, Kaiyue and Xiang, Tao and Song, Yi-Zhe},
  date = {2023-04-23},
  eprint = {2304.11744},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2304.11744},
  url = {http://arxiv.org/abs/2304.11744},
  urldate = {2023-05-19},
  abstract = {This paper, for the very first time, introduces human sketches to the landscape of XAI (Explainable Artificial Intelligence). We argue that sketch as a ``human-centred'' data form, represents a natural interface to study explainability. We focus on cultivating sketch-specific explainability designs. This starts by identifying strokes as a unique building block that offers a degree of flexibility in object construction and manipulation impossible in photos. Following this, we design a simple explainability-friendly sketch encoder that accommodates the intrinsic properties of strokes: shape, location, and order. We then move on to define the first ever XAI task for sketch, that of stroke location inversion SLI. Just as we have heat maps for photos, and correlation matrices for text, SLI offers an explainability angle to sketch in terms of asking a network how well it can recover stroke locations of an unseen sketch. We offer qualitative results for readers to interpret as snapshots of the SLI process in the paper, and as GIFs on the project page. A minor but interesting note is that thanks to its sketch-specific design, our sketch encoder also yields the best sketch recognition accuracy to date while having the smallest number of parameters. The code is available at \textbackslash url\{https://sketchxai.github.io\}.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/eragon/Zotero/storage/PBSTIJQV/Qu et al. - 2023 - SketchXAI A First Look at Explainability for Huma.pdf}
}

@unpublished{radhakrishnanPatchnetInterpretableNeural2018,
  title = {Patchnet: {{Interpretable Neural Networks}} for {{Image Classification}}},
  shorttitle = {Patchnet},
  author = {Radhakrishnan, Adityanarayanan and Durham, Charles and Soylemezoglu, Ali and Uhler, Caroline},
  date = {2018-11-29},
  eprint = {1705.08078},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1705.08078},
  urldate = {2022-09-26},
  abstract = {Understanding how a complex machine learning model makes a classification decision is essential for its acceptance in sensitive areas such as health care. Towards this end, we present PatchNet, a method that provides the features indicative of each class in an image using a tradeoff between restricting global image context and classification error. We mathematically analyze this tradeoff, demonstrate Patchnet's ability to construct sharp visual heatmap representations of the learned features, and quantitatively compare these features with features selected by domain experts by applying PatchNet to the classification of benign/malignant skin lesions from the ISBI-ISIC 2017 melanoma classification challenge.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/eragon/Zotero/storage/FM4FT9BL/Radhakrishnan et al. - 2018 - Patchnet Interpretable Neural Networks for Image .pdf}
}

@article{rebuffiDataAugmentationCan,
  title = {Data {{Augmentation Can Improve Robustness}}},
  author = {Rebuffi, Sylvestre-Alvise and Gowal, Sven and Calian, Dan and Stimberg, Florian and Wiles, Olivia and Mann, Timothy},
  abstract = {Adversarial training suffers from robust overfitting, a phenomenon where the robust test accuracy starts to decrease during training. In this paper, we focus on reducing robust overfitting by using common data augmentation schemes. We demonstrate that, contrary to previous findings, when combined with model weight averaging, data augmentation can significantly boost robust accuracy. Furthermore, we compare various data augmentations techniques and observe that spatial composition techniques work best for adversarial training. Finally, we evaluate our approach on CIFAR-10 against ∞ and 2 norm-bounded perturbations of size = 8/255 and = 128/255, respectively. We show large absolute improvements of +2.93\% and +2.16\% in robust accuracy compared to previous state-of-the-art methods. In particular, against ∞ norm-bounded perturbations of size = 8/255, our model reaches 60.07\% robust accuracy without using any external data. We also achieve a significant performance boost with this approach while using other architectures and datasets such as CIFAR-100, SVHN and TINYIMAGENET.},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/DZUY663P/Rebufﬁ et al. - Data Augmentation Can Improve Robustness.pdf}
}

@online{rebuffiFixingDataAugmentation2021,
  title = {Fixing {{Data Augmentation}} to {{Improve Adversarial Robustness}}},
  author = {Rebuffi, Sylvestre-Alvise and Gowal, Sven and Calian, Dan A. and Stimberg, Florian and Wiles, Olivia and Mann, Timothy},
  date = {2021-10-18},
  eprint = {2103.01946},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2103.01946},
  urldate = {2023-01-16},
  abstract = {Adversarial training suffers from robust overfitting, a phenomenon where the robust test accuracy starts to decrease during training. In this paper, we focus on both heuristics-driven and data-driven augmentations as a means to reduce robust overfitting. First, we demonstrate that, contrary to previous findings, when combined with model weight averaging, data augmentation can significantly boost robust accuracy. Second, we explore how state-of-the-art generative models can be leveraged to artificially increase the size of the training set and further improve adversarial robustness. Finally, we evaluate our approach on CIFAR-10 against ∞ and 2 norm-bounded perturbations of size = 8/255 and = 128/255, respectively. We show large absolute improvements of +7.06\% and +5.88\% in robust accuracy compared to previous state-of-the-art methods. In particular, against ∞ norm-bounded perturbations of size = 8/255, our model reaches 64.20\% robust accuracy without using any external data, beating most prior works that use external data. Since its original publication (2 Mar 2021), this paper has been accepted to NeurIPS 2021 as two separate and updated papers (Rebuffi et al., 2021; Gowal et al., 2021). The new papers improve results and clarity.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/eragon/Zotero/storage/H2LK6PX4/Rebuffi et al. - 2021 - Fixing Data Augmentation to Improve Adversarial Ro.pdf}
}

@inproceedings{rebuffiThereBackAgain2020,
  title = {There and {{Back Again}}: {{Revisiting Backpropagation Saliency Methods}}},
  shorttitle = {There and {{Back Again}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Rebuffi, Sylvestre-Alvise and Fong, Ruth and Ji, Xu and Vedaldi, Andrea},
  date = {2020-06},
  pages = {8836--8845},
  publisher = {{IEEE}},
  location = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.00886},
  url = {https://ieeexplore.ieee.org/document/9157775/},
  urldate = {2023-02-22},
  abstract = {Saliency methods seek to explain the predictions of a model by producing an importance map across each input sample. A popular class of such methods is based on backpropagating a signal and analyzing the resulting gradient. Despite much research on such methods, relatively little work has been done to clarify the differences between such methods as well as the desiderata of these techniques. Thus, there is a need for rigorously understanding the relationships between different methods as well as their failure modes. In this work, we conduct a thorough analysis of backpropagation-based saliency methods and propose a single framework under which several such methods can be unified. As a result of our study, we make three additional contributions. First, we use our framework to propose NormGrad, a novel saliency method based on the spatial contribution of gradients of convolutional weights. Second, we combine saliency maps at different layers to test the ability of saliency methods to extract complementary information at different network levels (e.g. trading off spatial resolution and distinctiveness) and we explain why some methods fail at specific layers (e.g., Grad-CAM anywhere besides the last convolutional layer). Third, we introduce a classsensitivity metric and a meta-learning inspired paradigm applicable to any saliency method for improving sensitivity to the output class being explained.},
  eventtitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-72817-168-5},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/7FQCL9VF/Rebuffi et al. - 2020 - There and Back Again Revisiting Backpropagation S.pdf}
}

@article{rhodesDistinctionPerceivedDuration2018,
  title = {On the {{Distinction Between Perceived Duration}} and {{Event Timing}}: {{Towards}} a {{Unified Model}} of {{Time Perception}}},
  shorttitle = {On the {{Distinction Between Perceived Duration}} and {{Event Timing}}},
  author = {Rhodes, Darren},
  date = {2018-04-10},
  journaltitle = {Timing \& Time Perception},
  volume = {6},
  number = {1},
  pages = {90--123},
  publisher = {{Brill}},
  issn = {2213-445X, 2213-4468},
  doi = {10.1163/22134468-20181132},
  url = {https://brill.com/view/journals/time/6/1/article-p90_90.xml},
  urldate = {2023-01-25},
  abstract = {Time is a fundamental dimension of human perception, cognition and action, as the processing and cognition of temporal information is essential for everyday activities and survival. Innumerable studies have investigated the perception of time over the last 100 years, but the neural and computational bases for the processing of time remains unknown. Extant models of time perception are discussed before the proposition of a unified model of time perception that relates perceived event timing with perceived duration. The distinction between perceived event timing and perceived duration provides the current for navigating a river of contemporary approaches to time perception. Recent work has advocated a Bayesian approach to time perception. This framework has been applied to both duration and perceived timing, where prior expectations about when a stimulus might occur in the future (prior distribution) are combined with current sensory evidence (likelihood function) in order to generate the perception of temporal properties (posterior distribution). In general, these models predict that the brain uses temporal expectations to bias perception in a way that stimuli are ‘regularized’ i.e. stimuli look more like what has been seen before. As such, the synthesis of perceived timing and duration models is of theoretical importance for the field of timing and time perception.},
  langid = {english},
  keywords = {Bayesian time perception,central tendency,computational modeling,duration,Perceived timing,temporal processing},
  file = {/Users/eragon/Zotero/storage/77PCGDJA/Rhodes - 2018 - On the Distinction Between Perceived Duration and .pdf;/Users/eragon/Zotero/storage/ZQ4ZXWZB/Rhodes - 2018 - On the Distinction Between Perceived Duration and .pdf}
}

@online{ribeiroWhyShouldTrust2016,
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  shorttitle = {"{{Why Should I Trust You}}?},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  date = {2016-08-09},
  eprint = {1602.04938},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1602.04938},
  urldate = {2022-11-18},
  abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/eragon/Zotero/storage/J66379K9/Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf}
}

@online{ribeiroWhyShouldTrust2016a,
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  shorttitle = {"{{Why Should I Trust You}}?},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  date = {2016-08-09},
  eprint = {1602.04938},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1602.04938},
  urldate = {2023-02-20},
  abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/eragon/Zotero/storage/TTGBQICM/Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf}
}

@inproceedings{richterVarGradLowVarianceGradient2020,
  title = {{{VarGrad}}: {{A Low-Variance Gradient Estimator}} for {{Variational Inference}}},
  shorttitle = {{{VarGrad}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Richter, Lorenz and Boustati, Ayman and Nüsken, Nikolas and Ruiz, Francisco and Akyildiz, Omer Deniz},
  date = {2020},
  volume = {33},
  pages = {13481--13492},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2020/hash/9c22c0b51b3202246463e986c7e205df-Abstract.html},
  urldate = {2023-02-20},
  abstract = {We analyse the properties of an unbiased gradient estimator of the ELBO for variational inference, based on the score function method with leave-one-out control variates. We show that this gradient estimator can be obtained using a new loss, defined as the variance of the log-ratio between the exact posterior and the variational approximation, which we call the log-variance loss. Under certain conditions, the gradient of the log-variance loss equals the gradient of the (negative) ELBO. We show theoretically that this gradient estimator, which we call VarGrad due to its connection to the log-variance loss, exhibits lower variance than the score function method in certain settings, and that the leave-one-out control variate coefficients are close to the optimal ones. We empirically demonstrate that VarGrad offers a favourable variance versus computation trade-off compared to other state-of-the-art estimators on a discrete VAE.},
  file = {/Users/eragon/Zotero/storage/TSF66X63/Richter et al. - 2020 - VarGrad A Low-Variance Gradient Estimator for Var.pdf}
}

@unpublished{rombachHighResolutionImageSynthesis2022,
  title = {High-{{Resolution Image Synthesis}} with {{Latent Diffusion Models}}},
  author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
  date = {2022-04-13},
  eprint = {2112.10752},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2112.10752},
  urldate = {2022-09-26},
  abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/eragon/Zotero/storage/TU5QQTQU/Rombach et al. - 2022 - High-Resolution Image Synthesis with Latent Diffus.pdf}
}

@article{sahakyanExplainableArtificialIntelligence2021,
  title = {Explainable {{Artificial Intelligence}} for {{Tabular Data}}: {{A Survey}}},
  shorttitle = {Explainable {{Artificial Intelligence}} for {{Tabular Data}}},
  author = {Sahakyan, Maria and Aung, Zeyar and Rahwan, Talal},
  date = {2021},
  journaltitle = {IEEE Access},
  volume = {9},
  pages = {135392--135422},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3116481},
  abstract = {Machine learning techniques are increasingly gaining attention due to their widespread use in various disciplines across academia and industry. Despite their tremendous success, many such techniques suffer from the “black-box” problem, which refers to situations where the data analyst is unable to explain why such techniques arrive at certain decisions. This problem has fuelled interest in Explainable Artificial Intelligence (XAI), which refers to techniques that can easily be interpreted by humans. Unfortunately, many of these techniques are not suitable for tabular data, which is surprising given the importance and widespread use of tabular data in critical applications such as finance, healthcare, and criminal justice. Also surprising is the fact that, despite the vast literature on XAI, there are still no survey articles to date that focus on tabular data. Consequently, despite the existing survey articles that cover a wide range of XAI techniques, it remains challenging for researchers working on tabular data to go through all of these surveys and extract the techniques that are suitable for their analysis. Our article fills this gap by providing a comprehensive and up-to-date survey of the XAI techniques that are relevant to tabular data. Furthermore, we categorize the references covered in our survey, indicating the type of the model being explained, the approach being used to provide the explanation, and the XAI problem being addressed. Our article is the first to provide researchers with a map that helps them navigate the XAI literature in the context of tabular data.},
  eventtitle = {{{IEEE Access}}},
  keywords = {Black-box models,Data models,explainable artificial intelligence,Inspection,machine learning,Medical services,model interpretability,Neural networks,Numerical models,Solid modeling},
  file = {/Users/eragon/Zotero/storage/XLEE8Z8K/Sahakyan et al. - 2021 - Explainable Artificial Intelligence for Tabular Da.pdf}
}

@inproceedings{sandlerMobilenetv2InvertedResiduals2018,
  title = {Mobilenetv2: {{Inverted}} Residuals and Linear Bottlenecks},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
  date = {2018},
  pages = {4510--4520}
}

@unpublished{santurkarHowDoesBatch2019,
  title = {How {{Does Batch Normalization Help Optimization}}?},
  author = {Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander},
  date = {2019-04-14},
  eprint = {1805.11604},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1805.11604},
  url = {http://arxiv.org/abs/1805.11604},
  urldate = {2022-08-25},
  abstract = {Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm's effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers' input distributions during training to reduce the so-called "internal covariate shift". In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training.},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/eragon/Zotero/storage/J63HZ9MM/Santurkar et al. - 2019 - How Does Batch Normalization Help Optimization.pdf}
}

@inproceedings{savarese3DGenericObject2007,
  title = {{{3D}} Generic Object Categorization, Localization and Pose Estimation},
  booktitle = {2007 {{IEEE}} 11th {{International Conference}} on {{Computer Vision}}},
  author = {Savarese, Silvio and Fei-Fei, Li},
  date = {2007},
  pages = {1--8},
  publisher = {{IEEE}}
}

@incollection{schmidhuberSelfReferentialWeightMatrix1993,
  title = {A ‘{{Self-Referential}}’ {{Weight Matrix}}},
  booktitle = {{{ICANN}} ’93},
  author = {Schmidhuber, J.},
  editor = {Gielen, Stan and Kappen, Bert},
  date = {1993},
  pages = {446--450},
  publisher = {{Springer London}},
  location = {{London}},
  doi = {10.1007/978-1-4471-2063-6_107},
  url = {http://link.springer.com/10.1007/978-1-4471-2063-6_107},
  urldate = {2023-04-26},
  abstract = {Weight modi cations in traditional neural nets are computed by hard-wired algorithms. Without exception, all previous weight change algorithms have many speci c limitations. Is it (in principle) possible to overcome limitations of hard-wired algorithms by allowing neural nets to run and improve their own weight change algorithms? This paper constructively demonstrates that the answer (in principle) is `yes'. I derive an initial gradientbased sequence learning algorithm for a `self-referential' recurrent network that can `speak' about its own weight matrix in terms of activations. It uses some of its input and output units for observing its own errors and for explicitly analyzing and modifying its own weight matrix, including those parts of the weight matrix responsible for analyzing and modifying the weight matrix. The result is the rst `introspective' neural net with explicit potential control over all of its own adaptive parameters. A disadvantage of the algorithm is its high computational complexity per time step which is independent of the sequence length and equals O(nconnlognconn), where nconn is the number of connections. Another disadvantage is the high number of local minima of the unusually complex error surface. The purpose of this paper, however, is not to come up with the most e cient `introspective' or `self-referential' weight change algorithm, but to show that such algorithms are possible at all.},
  isbn = {978-3-540-19839-0 978-1-4471-2063-6},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/N7UZ25QU/Schmidhuber - 1993 - A ‘Self-Referential’ Weight Matrix.pdf}
}

@article{selvarajuGradCAMVisualExplanations,
  title = {Grad-{{CAM}}: {{Visual Explanations From Deep Networks}} via {{Gradient-Based Localization}}},
  author = {Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  pages = {9},
  abstract = {We propose a technique for producing ‘visual explanations’ for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent. Our approach – Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say logits for ‘dog’ or even a caption), flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, GradCAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in tasks with multi-modal inputs (e.g. visual question answering) or reinforcement learning, without architectural changes or re-training. We combine Grad-CAM with existing fine-grained visualizations to create a high-resolution class-discriminative visualization, Guided Grad-CAM, and apply it to image classification, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classification models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) outperform previous methods on the ILSVRC-15 weakly-supervised localization task, (c) are more faithful to the underlying model, and (d) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visualizations show even non-attention based models can localize inputs. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that Grad-CAM helps untrained users successfully discern a ‘stronger’ deep network from a ‘weaker’ one even when both make identical predictions. Our code is available at https: //github.com/ramprs/grad-cam/ along with a demo on CloudCV [2]1 and video at youtu.be/COjUB9Izk6E.},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/EDVF5SE9/Selvaraju et al. - Grad-CAM Visual Explanations From Deep Networks v.pdf}
}

@online{selvarajuGradCAMWhyDid2017,
  title = {Grad-{{CAM}}: {{Why}} Did You Say That?},
  shorttitle = {Grad-{{CAM}}},
  author = {Selvaraju, Ramprasaath R. and Das, Abhishek and Vedantam, Ramakrishna and Cogswell, Michael and Parikh, Devi and Batra, Dhruv},
  date = {2017-01-25},
  eprint = {1611.07450},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1611.07450},
  urldate = {2023-02-20},
  abstract = {We propose a technique for making Convolutional Neural Network (CNN)-based models more transparent by visualizing input regions that are 'important' for predictions -- or visual explanations. Our approach, called Gradient-weighted Class Activation Mapping (Grad-CAM), uses class-specific gradient information to localize important regions. These localizations are combined with existing pixel-space visualizations to create a novel high-resolution and class-discriminative visualization called Guided Grad-CAM. These methods help better understand CNN-based models, including image captioning and visual question answering (VQA) models. We evaluate our visual explanations by measuring their ability to discriminate between classes, to inspire trust in humans, and their correlation with occlusion maps. Grad-CAM provides a new way to understand CNN-based models. We have released code, an online demo hosted on CloudCV, and a full version of this extended abstract.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/eragon/Zotero/storage/JF959R5S/Selvaraju et al. - 2017 - Grad-CAM Why did you say that.pdf}
}

@unpublished{shankarOperationalizingMachineLearning2022,
  title = {Operationalizing {{Machine Learning}}: {{An Interview Study}}},
  shorttitle = {Operationalizing {{Machine Learning}}},
  author = {Shankar, Shreya and Garcia, Rolando and Hellerstein, Joseph M. and Parameswaran, Aditya G.},
  date = {2022-09-16},
  eprint = {2209.09125},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2209.09125},
  urldate = {2022-09-26},
  abstract = {Organizations rely on machine learning engineers (MLEs) to operationalize ML, i.e., deploy and maintain ML pipelines in production. The process of operationalizing ML, or MLOps, consists of a continual loop of (i) data collection and labeling, (ii) experimentation to improve ML performance, (iii) evaluation throughout a multi-staged deployment process, and (iv) monitoring of performance drops in production. When considered together, these responsibilities seem staggering -- how does anyone do MLOps, what are the unaddressed challenges, and what are the implications for tool builders? We conducted semi-structured ethnographic interviews with 18 MLEs working across many applications, including chatbots, autonomous vehicles, and finance. Our interviews expose three variables that govern success for a production ML deployment: Velocity, Validation, and Versioning. We summarize common practices for successful ML experimentation, deployment, and sustaining production performance. Finally, we discuss interviewees' pain points and anti-patterns, with implications for tool design.},
  keywords = {Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Computer Science - Software Engineering},
  file = {/Users/eragon/Zotero/storage/4BKYWSFA/Shankar et al. - 2022 - Operationalizing Machine Learning An Interview St.pdf}
}

@article{shermanPowerAdolescence2016,
  title = {The {{Power}} of the {{Like}} in {{Adolescence}}},
  author = {Sherman, Lauren E. and Payton, Ashley A. and Hernandez, Leanna M. and Greenfield, Patricia M. and Dapretto, Mirella},
  date = {2016-07},
  journaltitle = {Psychological Science},
  shortjournal = {Psychol Sci},
  volume = {27},
  number = {7},
  eprint = {27247125},
  eprinttype = {pmid},
  pages = {1027--1035},
  issn = {0956-7976},
  doi = {10.1177/0956797616645673},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5387999/},
  urldate = {2022-06-15},
  abstract = {We investigated a unique way in which adolescent peer influence occurs on social media. We developed a novel functional MRI (fMRI) paradigm to simulate Instagram, a popular social photo-sharing tool, and measured adolescents’ behavioral and neural responses to likes, a quantifiable form of social endorsement and potential source of peer influence. Adolescents underwent fMRI while viewing photos ostensibly submitted to Instagram. They were more likely to like photos depicted with many likes than photos with few likes; this finding showed the influence of virtual peer endorsement and held for both neutral photos and photos of risky behaviors (e.g., drinking, smoking). Viewing photos with many (compared with few) likes was associated with greater activity in neural regions implicated in reward processing, social cognition, imitation, and attention. Furthermore, when adolescents viewed risky photos (as opposed to neutral photos), activation in the cognitive-control network decreased. These findings highlight possible mechanisms underlying peer influence during adolescence.},
  pmcid = {PMC5387999},
  file = {/Users/eragon/Zotero/storage/CANDS4CV/Sherman et al. - 2016 - The Power of the Like in Adolescence.pdf}
}

@article{shortenSurveyImageData2019,
  title = {A Survey on {{Image Data Augmentation}} for {{Deep Learning}}},
  author = {Shorten, Connor and Khoshgoftaar, Taghi M.},
  date = {2019-12},
  journaltitle = {Journal of Big Data},
  shortjournal = {J Big Data},
  volume = {6},
  number = {1},
  pages = {60},
  issn = {2196-1115},
  doi = {10.1186/s40537-019-0197-0},
  url = {https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0197-0},
  urldate = {2023-05-08},
  abstract = {Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/5A6GH22Q/Shorten and Khoshgoftaar - 2019 - A survey on Image Data Augmentation for Deep Learn.pdf}
}

@online{shrikumarComputationallyEfficientMeasures2018,
  title = {Computationally {{Efficient Measures}} of {{Internal Neuron Importance}}},
  author = {Shrikumar, Avanti and Su, Jocelin and Kundaje, Anshul},
  date = {2018-07-25},
  eprint = {1807.09946},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1807.09946},
  url = {http://arxiv.org/abs/1807.09946},
  urldate = {2022-11-28},
  abstract = {The challenge of assigning importance to individual neurons in a network is of interest when interpreting deep learning models. In recent work, Dhamdhere et al. proposed Total Conductance, a "natural refinement of Integrated Gradients" for attributing importance to internal neurons. Unfortunately, the authors found that calculating conductance in tensorflow required the addition of several custom gradient operators and did not scale well. In this work, we show that the formula for Total Conductance is mathematically equivalent to Path Integrated Gradients computed on a hidden layer in the network. We provide a scalable implementation of Total Conductance using standard tensorflow gradient operators that we call Neuron Integrated Gradients. We compare Neuron Integrated Gradients to DeepLIFT, a pre-existing computationally efficient approach that is applicable to calculating internal neuron importance. We find that DeepLIFT produces strong empirical results and is faster to compute, but because it lacks the theoretical properties of Neuron Integrated Gradients, it may not always be preferred in practice. Colab notebook reproducing results: http://bit.ly/neuronintegratedgradients},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/eragon/Zotero/storage/MZVID3Y2/Shrikumar et al. - 2018 - Computationally Efficient Measures of Internal Neu.pdf}
}

@online{shrikumarLearningImportantFeatures2019,
  title = {Learning {{Important Features Through Propagating Activation Differences}}},
  author = {Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
  date = {2019-10-12},
  eprint = {1704.02685},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1704.02685},
  url = {http://arxiv.org/abs/1704.02685},
  urldate = {2022-11-28},
  abstract = {The purported "black box" nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. Video tutorial: http://goo.gl/qKb7pL, ICML slides: bit.ly/deeplifticmlslides, ICML talk: https://vimeo.com/238275076, code: http://goo.gl/RM8jvH.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/eragon/Zotero/storage/CSAZYN2B/Shrikumar et al. - 2019 - Learning Important Features Through Propagating Ac.pdf}
}

@article{sigihaleAtypicalBrainActivation2007,
  title = {Atypical {{Brain Activation During Simple}} \& {{Complex Levels}} of {{Processing}} in {{Adult ADHD}}: {{An fMRI Study}}},
  shorttitle = {Atypical {{Brain Activation During Simple}} \& {{Complex Levels}} of {{Processing}} in {{Adult ADHD}}},
  author = {Sigi Hale, T. and Bookheimer, Susan and McGough, James J. and Phillips, Joseph M. and McCracken, James T.},
  date = {2007-09},
  journaltitle = {Journal of Attention Disorders},
  shortjournal = {J Atten Disord},
  volume = {11},
  number = {2},
  pages = {125--139},
  issn = {1087-0547, 1557-1246},
  doi = {10.1177/1087054706294101},
  url = {http://journals.sagepub.com/doi/10.1177/1087054706294101},
  urldate = {2022-06-15},
  abstract = {Objective: Executive dysfunction in ADHD is well supported. However, recent studies suggest that more fundamental impairments may be contributing. We assessed brain function in adults with ADHD during simple and complex forms of processing. Method: We used functional magnetic resonance imaging with forward and backward digit spans to investigate number repetitions and complex working memory function. If pathology is limited to higher cognitive operations, group differences should be confined to the backward condition. Results: During the forward digit span, ADHD participants exhibited greater activation of LH linguistic processing areas and increased activation of right frontal and parietal cortices. During the backward digit span, they exhibited greater activation of LH linguistic processing areas and failed to activate bilateral parietal regions important for the complex executive operations. Conclusion: Abnormal brain function among adult ADHD participants was not limited to complex executive functions. Abnormal processing of numeric stimuli was indicated during both simple and complex cognitive operations. (J. of Att. Dis. 2007;11(2) 125-140)},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/LIJMTZ5N/Sigi Hale et al. - 2007 - Atypical Brain Activation During Simple & Complex .pdf}
}

@online{simonyanDeepConvolutionalNetworks2014,
  title = {Deep {{Inside Convolutional Networks}}: {{Visualising Image Classification Models}} and {{Saliency Maps}}},
  shorttitle = {Deep {{Inside Convolutional Networks}}},
  author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  date = {2014-04-19},
  eprint = {1312.6034},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1312.6034},
  urldate = {2022-11-18},
  abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [5], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [13].},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/eragon/Zotero/storage/3TNFZE58/Simonyan et al. - 2014 - Deep Inside Convolutional Networks Visualising Im.pdf}
}

@online{simonyanDeepConvolutionalNetworks2014a,
  title = {Deep {{Inside Convolutional Networks}}: {{Visualising Image Classification Models}} and {{Saliency Maps}}},
  shorttitle = {Deep {{Inside Convolutional Networks}}},
  author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  date = {2014-04-19},
  eprint = {1312.6034},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1312.6034},
  urldate = {2022-11-18},
  abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [5], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [13].},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/eragon/Zotero/storage/EXEY4ASY/Simonyan et al. - 2014 - Deep Inside Convolutional Networks Visualising Im.pdf}
}

@online{simonyanDeepConvolutionalNetworks2014b,
  title = {Deep {{Inside Convolutional Networks}}: {{Visualising Image Classification Models}} and {{Saliency Maps}}},
  shorttitle = {Deep {{Inside Convolutional Networks}}},
  author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  date = {2014-04-19},
  eprint = {1312.6034},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1312.6034},
  url = {http://arxiv.org/abs/1312.6034},
  urldate = {2022-11-28},
  abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/eragon/Zotero/storage/PXVDTL4R/Simonyan et al. - 2014 - Deep Inside Convolutional Networks Visualising Im.pdf}
}

@unpublished{simonyanVeryDeepConvolutional2014,
  title = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  author = {Simonyan, Karen and Zisserman, Andrew},
  date = {2014},
  eprint = {1409.1556},
  eprinttype = {arxiv}
}

@online{singhHideandSeekDataAugmentation2018,
  title = {Hide-and-{{Seek}}: {{A Data Augmentation Technique}} for {{Weakly-Supervised Localization}} and {{Beyond}}},
  shorttitle = {Hide-and-{{Seek}}},
  author = {Singh, Krishna Kumar and Yu, Hao and Sarmasi, Aron and Pradeep, Gautam and Lee, Yong Jae},
  date = {2018-11-06},
  eprint = {1811.02545},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1811.02545},
  url = {http://arxiv.org/abs/1811.02545},
  urldate = {2023-03-27},
  abstract = {We propose 'Hide-and-Seek' a general purpose data augmentation technique, which is complementary to existing data augmentation techniques and is beneficial for various visual recognition tasks. The key idea is to hide patches in a training image randomly, in order to force the network to seek other relevant content when the most discriminative content is hidden. Our approach only needs to modify the input image and can work with any network to improve its performance. During testing, it does not need to hide any patches. The main advantage of Hide-and-Seek over existing data augmentation techniques is its ability to improve object localization accuracy in the weakly-supervised setting, and we therefore use this task to motivate the approach. However, Hide-and-Seek is not tied only to the image localization task, and can generalize to other forms of visual input like videos, as well as other recognition tasks like image classification, temporal action localization, semantic segmentation, emotion recognition, age/gender estimation, and person re-identification. We perform extensive experiments to showcase the advantage of Hide-and-Seek on these various visual recognition problems.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/eragon/Zotero/storage/LL5XKNP3/Singh et al. - 2018 - Hide-and-Seek A Data Augmentation Technique for W.pdf}
}

@online{smilkovSmoothGradRemovingNoise2017,
  title = {{{SmoothGrad}}: Removing Noise by Adding Noise},
  shorttitle = {{{SmoothGrad}}},
  author = {Smilkov, Daniel and Thorat, Nikhil and Kim, Been and Viégas, Fernanda and Wattenberg, Martin},
  date = {2017-06-12},
  eprint = {1706.03825},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1706.03825},
  url = {http://arxiv.org/abs/1706.03825},
  urldate = {2022-11-28},
  abstract = {Explaining the output of a deep network remains a challenge. In the case of an image classifier, one type of explanation is to identify pixels that strongly influence the final decision. A starting point for this strategy is the gradient of the class score function with respect to the input image. This gradient can be interpreted as a sensitivity map, and there are several techniques that elaborate on this basic idea. This paper makes two contributions: it introduces SmoothGrad, a simple method that can help visually sharpen gradient-based sensitivity maps, and it discusses lessons in the visualization of these maps. We publish the code for our experiments and a website with our results.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/eragon/Zotero/storage/YCR6ABCK/Smilkov et al. - 2017 - SmoothGrad removing noise by adding noise.pdf}
}

@online{smithSuperConvergenceVeryFast2018,
  title = {Super-{{Convergence}}: {{Very Fast Training}} of {{Neural Networks Using Large Learning Rates}}},
  shorttitle = {Super-{{Convergence}}},
  author = {Smith, Leslie N. and Topin, Nicholay},
  date = {2018-05-17},
  eprint = {1708.07120},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1708.07120},
  url = {http://arxiv.org/abs/1708.07120},
  urldate = {2023-02-28},
  abstract = {In this paper, we describe a phenomenon, which we named "super-convergence", where neural networks can be trained an order of magnitude faster than with standard training methods. The existence of super-convergence is relevant to understanding why deep networks generalize well. One of the key elements of super-convergence is training with one learning rate cycle and a large maximum learning rate. A primary insight that allows super-convergence training is that large learning rates regularize the training, hence requiring a reduction of all other forms of regularization in order to preserve an optimal regularization balance. We also derive a simplification of the Hessian Free optimization method to compute an estimate of the optimal learning rate. Experiments demonstrate super-convergence for Cifar-10/100, MNIST and Imagenet datasets, and resnet, wide-resnet, densenet, and inception architectures. In addition, we show that super-convergence provides a greater boost in performance relative to standard training when the amount of labeled training data is limited. The architectures and code to replicate the figures in this paper are available at github.com/lnsmith54/super-convergence. See http://www.fast.ai/2018/04/30/dawnbench-fastai/ for an application of super-convergence to win the DAWNBench challenge (see https://dawn.cs.stanford.edu/benchmark/).},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/eragon/Zotero/storage/E8KLW756/Smith and Topin - 2018 - Super-Convergence Very Fast Training of Neural Ne.pdf}
}

@online{SMOTESyntheticMinority,
  title = {{{SMOTE}}: {{Synthetic Minority Over-sampling Technique}} | {{Journal}} of {{Artificial Intelligence Research}}},
  url = {https://www.jair.org/index.php/jair/article/view/10302},
  urldate = {2023-03-31}
}

@inproceedings{sockMultiView6DObject2017,
  title = {Multi-{{View 6D Object Pose Estimation}} and {{Camera Motion Planning Using RGBD Images}}},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}}) {{Workshops}}},
  author = {Sock, Juil and Hamidreza Kasaei, S. and Seabra Lopes, Luis and Kim, Tae-Kyun},
  date = {2017}
}

@article{spinnerExplAInerVisualAnalytics2019,
  title = {{{explAIner}}: {{A Visual Analytics Framework}} for {{Interactive}} and {{Explainable Machine Learning}}},
  shorttitle = {{{explAIner}}},
  author = {Spinner, Thilo and Schlegel, Udo and Schäfer, Hanna and El-Assady, Mennatallah},
  date = {2019},
  journaltitle = {IEEE Transactions on Visualization and Computer Graphics},
  shortjournal = {IEEE Trans. Visual. Comput. Graphics},
  eprint = {1908.00087},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {1--1},
  issn = {1077-2626, 1941-0506, 2160-9306},
  doi = {10.1109/TVCG.2019.2934629},
  url = {http://arxiv.org/abs/1908.00087},
  urldate = {2023-02-13},
  abstract = {We propose a framework for interactive and explainable machine learning that enables users to (1) understand machine learning models; (2) diagnose model limitations using different explainable AI methods; as well as (3) refine and optimize the models. Our framework combines an iterative XAI pipeline with eight global monitoring and steering mechanisms, including quality monitoring, provenance tracking, model comparison, and trust building. To operationalize the framework, we present explAIner, a visual analytics system for interactive and explainable machine learning that instantiates all phases of the suggested pipeline within the commonly used TensorBoard environment. We performed a user-study with nine participants across different expertise levels to examine their perception of our workflow and to collect suggestions to fill the gap between our system and framework. The evaluation confirms that our tightly integrated system leads to an informed machine learning process while disclosing opportunities for further extensions.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  file = {/Users/eragon/Zotero/storage/KFI8V7HV/Spinner et al. - 2019 - explAIner A Visual Analytics Framework for Intera.pdf}
}

@online{springenbergStrivingSimplicityAll2015,
  title = {Striving for {{Simplicity}}: {{The All Convolutional Net}}},
  shorttitle = {Striving for {{Simplicity}}},
  author = {Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
  date = {2015-04-13},
  eprint = {1412.6806},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1412.6806},
  url = {http://arxiv.org/abs/1412.6806},
  urldate = {2022-11-18},
  abstract = {Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the "deconvolution approach" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/eragon/Zotero/storage/DQH9A4MX/Springenberg et al. - 2015 - Striving for Simplicity The All Convolutional Net.pdf}
}

@inproceedings{steedImageRepresentationsLearned2021,
  title = {Image {{Representations Learned With Unsupervised Pre-Training Contain Human-like Biases}}},
  booktitle = {Proceedings of the 2021 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Steed, Ryan and Caliskan, Aylin},
  date = {2021-03-03},
  series = {{{FAccT}} '21},
  pages = {701--713},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3442188.3445932},
  url = {https://doi.org/10.1145/3442188.3445932},
  urldate = {2022-09-06},
  abstract = {Recent advances in machine learning leverage massive datasets of unlabeled images from the web to learn general-purpose image representations for tasks from image classification to face recognition. But do unsupervised computer vision models automatically learn implicit patterns and embed social biases that could have harmful downstream effects? We develop a novel method for quantifying biased associations between representations of social concepts and attributes in images. We find that state-of-the-art unsupervised models trained on ImageNet, a popular benchmark image dataset curated from internet images, automatically learn racial, gender, and intersectional biases. We replicate 8 previously documented human biases from social psychology, from the innocuous, as with insects and flowers, to the potentially harmful, as with race and gender. Our results closely match three hypotheses about intersectional bias from social psychology. For the first time in unsupervised computer vision, we also quantify implicit human biases about weight, disabilities, and several ethnicities. When compared with statistical patterns in online image datasets, our findings suggest that machine learning models can automatically learn bias from the way people are stereotypically portrayed on the web.},
  isbn = {978-1-4503-8309-7},
  keywords = {computer vision,implicit bias,unsupervised learning},
  file = {/Users/eragon/Zotero/storage/8UGH43BN/Steed and Caliskan - 2021 - Image Representations Learned With Unsupervised Pr.pdf}
}

@article{steinbornSequentialEffectsShort2008,
  title = {Sequential Effects within a Short Foreperiod Context: {{Evidence}} for the Conditioning Account of Temporal Preparation},
  shorttitle = {Sequential Effects within a Short Foreperiod Context},
  author = {Steinborn, Michael B. and Rolke, Bettina and Bratzke, Daniel and Ulrich, Rolf},
  date = {2008-10},
  journaltitle = {Acta Psychologica},
  shortjournal = {Acta Psychologica},
  volume = {129},
  number = {2},
  pages = {297--307},
  issn = {00016918},
  doi = {10.1016/j.actpsy.2008.08.005},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S000169180800111X},
  urldate = {2023-01-27},
  abstract = {Responses to an imperative stimulus (IS) are especially fast when they are preceded by a warning signal (WS). When the interval between WS and IS (the foreperiod, FP) is variable, reaction time (RT) is not only influenced by the current FP but also by the FP of the preceding trial. These sequential effects have recently been proposed to originate from a trace conditioning process, in which the individuals learn the temporal WS–IS relationship in a trial-by-trial manner. Research has shown that trace conditioning is maximal when the temporal interval between the conditioned and unconditioned stimulus is between 0.25 and 0.60 s. Consequently, one would predict that sequential effects occur especially within short FP contexts. However, this prediction is contradicted by Karlin [Karlin, L. (1959). Reaction time as a function of foreperiod duration and variability. Journal of Experimental Psychology, 58, 185–191] who did not observe the typical sequential effects with short FPs. To investigate temporal preparation for short FPs, three experiments were conducted, examining the sequential FP effect comparably for short and long FP-sets (Experiment 1), assessing the influence of catch trials (Experiment 2) and the case of a very dense FP-range (Experiment 3) on sequential FP effects. The results provide strong evidence for sequential effects within a short FP context and thus support the trace conditioning account of temporal preparation.},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/WRPNP877/Steinborn et al. - 2008 - Sequential effects within a short foreperiod conte.pdf}
}

@online{steinerHowTrainYour2022,
  title = {How to Train Your {{ViT}}? {{Data}}, {{Augmentation}}, and {{Regularization}} in {{Vision Transformers}}},
  shorttitle = {How to Train Your {{ViT}}?},
  author = {Steiner, Andreas and Kolesnikov, Alexander and Zhai, Xiaohua and Wightman, Ross and Uszkoreit, Jakob and Beyer, Lucas},
  date = {2022-06-23},
  eprint = {2106.10270},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2106.10270},
  urldate = {2023-01-16},
  abstract = {Vision Transformers (ViT) have been shown to attain highly competitive performance for a wide range of vision applications, such as image classification, object detection and semantic image segmentation. In comparison to convolutional neural networks, the Vision Transformer’s weaker inductive bias is generally found to cause an increased reliance on model regularization or data augmentation (“AugReg” for short) when training on smaller training datasets. We conduct a systematic empirical study in order to better understand the interplay between the amount of training data, AugReg, model size and compute budget.1 As one result of this study we find that the combination of increased compute and AugReg can yield models with the same performance as models trained on an order of magnitude more training data: we train ViT models of various sizes on the public ImageNet-21k dataset which either match or outperform their counterparts trained on the larger, but not publicly available JFT-300M dataset.},
  langid = {english},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/eragon/Zotero/storage/U3YS3MH8/Steiner et al. - 2022 - How to train your ViT Data, Augmentation, and Reg.pdf}
}

@article{sturmfelsVisualizingImpactFeature2020,
  title = {Visualizing the {{Impact}} of {{Feature Attribution Baselines}}},
  author = {Sturmfels, Pascal and Lundberg, Scott and Lee, Su-In},
  date = {2020-01-10},
  journaltitle = {Distill},
  shortjournal = {Distill},
  volume = {5},
  number = {1},
  pages = {e22},
  issn = {2476-0757},
  doi = {10.23915/distill.00022},
  url = {https://distill.pub/2020/attribution-baselines},
  urldate = {2023-03-24},
  abstract = {Exploring the baseline input hyperparameter, and how it impacts interpretations of neural network behavior.},
  langid = {english}
}

@inproceedings{suMultiviewConvolutionalNeural2015,
  title = {Multi-View Convolutional Neural Networks for {{3D}} Shape Recognition},
  booktitle = {Proceedings of the {{IEEE}} International Conference on Computer Vision},
  author = {Su, Hang and Maji, Subhransu and Kalogerakis, Evangelos and Learned-Miller, Erik},
  date = {2015}
}

@inproceedings{suMultiviewConvolutionalNeural2015a,
  title = {Multi-View Convolutional Neural Networks for {{3D}} Shape Recognition},
  booktitle = {Proceedings of the {{IEEE}} International Conference on Computer Vision},
  author = {Su, Hang and Maji, Subhransu and Kalogerakis, Evangelos and Learned-Miller, Erik},
  date = {2015}
}

@online{sundararajanAxiomaticAttributionDeep2017,
  title = {Axiomatic {{Attribution}} for {{Deep Networks}}},
  author = {Sundararajan, Mukund and Taly, Ankur and Yan, Qiqi},
  date = {2017-06-12},
  eprint = {1703.01365},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1703.01365},
  url = {http://arxiv.org/abs/1703.01365},
  urldate = {2023-03-24},
  abstract = {We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms---Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/eragon/Zotero/storage/K595X6ZS/Sundararajan et al. - 2017 - Axiomatic Attribution for Deep Networks.pdf}
}

@unpublished{suttonAlbertaPlanAI2022,
  title = {The {{Alberta Plan}} for {{AI Research}}},
  author = {Sutton, Richard S. and Bowling, Michael H. and Pilarski, Patrick M.},
  date = {2022-08-23},
  eprint = {2208.11173},
  eprinttype = {arxiv},
  eprintclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2208.11173},
  urldate = {2022-09-20},
  abstract = {Herein we describe our approach to artificial intelligence research, which we call the Alberta Plan. The Alberta Plan is pursued within our research groups in Alberta and by others who are like minded throughout the world. We welcome all who would join us in this pursuit.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/eragon/Zotero/storage/ZZX9LMZX/Sutton et al. - 2022 - The Alberta Plan for AI Research.pdf}
}

@online{szegedyGoingDeeperConvolutions2014,
  title = {Going {{Deeper}} with {{Convolutions}}},
  author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  date = {2014-09-16},
  eprint = {1409.4842},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1409.4842},
  url = {http://arxiv.org/abs/1409.4842},
  urldate = {2023-04-29},
  abstract = {We propose a deep convolutional neural network architecture codenamed "Inception", which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC 2014). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC 2014 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/eragon/Zotero/storage/QXWJMEET/Szegedy et al. - 2014 - Going Deeper with Convolutions.pdf}
}

@article{taatgenTracesTimesRepresentations2011,
  title = {Traces of Times Past: {{Representations}} of Temporal Intervals in Memory},
  shorttitle = {Traces of Times Past},
  author = {Taatgen, Niels and Rijn, Hedderik},
  date = {2011-05-28},
  journaltitle = {Memory \& cognition},
  shortjournal = {Memory \& cognition},
  volume = {39},
  pages = {1546--60},
  doi = {10.3758/s13421-011-0113-0},
  abstract = {Theories of time perception typically assume that some sort of memory represents time intervals. This memory component is typically underdeveloped in theories of time perception. Following earlier work that suggested that representations of different time intervals contaminate each other (Grondin, 2005; Jazayeri \& Shadlen, 2010; Jones \& Wearden, 2004), an experiment was conducted in which subjects had to alternate in reproducing two intervals. In two conditions of the experiment, the duration of one of the intervals changed over the experiment, forcing subjects to adjust their representation of that interval, while keeping the other constant. The results show that the adjustment of one interval carried over to the other interval, indicating that subjects were not able to completely separate the two representations. We propose a temporal reference memory that is based on existing memory models (Anderson, 1990). Our model assumes that the representation of an interval is based on a pool of recent experiences. In a series of simulations, we show that our pool model fits the data, while two alternative models that have previously been proposed do not.},
  file = {/Users/eragon/Zotero/storage/PZECY956/Taatgen and Rijn - 2011 - Traces of times past Representations of temporal .pdf}
}

@article{takahashiDataAugmentationUsing2020,
  title = {Data {{Augmentation}} Using {{Random Image Cropping}} and {{Patching}} for {{Deep CNNs}}},
  author = {Takahashi, Ryo and Matsubara, Takashi and Uehara, Kuniaki},
  date = {2020-09},
  journaltitle = {IEEE Transactions on Circuits and Systems for Video Technology},
  shortjournal = {IEEE Trans. Circuits Syst. Video Technol.},
  volume = {30},
  number = {9},
  eprint = {1811.09030},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {2917--2931},
  issn = {1051-8215, 1558-2205},
  doi = {10.1109/TCSVT.2019.2935128},
  url = {http://arxiv.org/abs/1811.09030},
  urldate = {2023-03-30},
  abstract = {Deep convolutional neural networks (CNNs) have achieved remarkable results in image processing tasks. However, their high expression ability risks overfitting. Consequently, data augmentation techniques have been proposed to prevent overfitting while enriching datasets. Recent CNN architectures with more parameters are rendering traditional data augmentation techniques insufficient. In this study, we propose a new data augmentation technique called random image cropping and patching (RICAP) which randomly crops four images and patches them to create a new training image. Moreover, RICAP mixes the class labels of the four images, resulting in an advantage similar to label smoothing. We evaluated RICAP with current state-of-the-art CNNs (e.g., the shake-shake regularization model) by comparison with competitive data augmentation techniques such as cutout and mixup. RICAP achieves a new state-of-the-art test error of \$2.19\textbackslash\%\$ on CIFAR-10. We also confirmed that deep CNNs with RICAP achieve better results on classification tasks using CIFAR-100 and ImageNet and an image-caption retrieval task using Microsoft COCO.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/eragon/Zotero/storage/7KUF8FPR/Takahashi et al. - 2020 - Data Augmentation using Random Image Cropping and .pdf}
}

@article{tal-perrySpatiotemporalLinkTemporal2022,
  title = {The {{Spatiotemporal Link}} of {{Temporal Expectations}}: {{Contextual Temporal Expectation Is Independent}} of {{Spatial Attention}}},
  shorttitle = {The {{Spatiotemporal Link}} of {{Temporal Expectations}}},
  author = {Tal-Perry, Noam and Yuval-Greenberg, Shlomit},
  date = {2022-03-23},
  journaltitle = {The Journal of Neuroscience},
  shortjournal = {J. Neurosci.},
  volume = {42},
  number = {12},
  pages = {2516--2523},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.1555-21.2022},
  url = {https://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.1555-21.2022},
  urldate = {2023-01-25},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/2YJSVKUH/Tal-Perry and Yuval-Greenberg - 2022 - The Spatiotemporal Link of Temporal Expectations .pdf}
}

@inproceedings{tanEfficientnetRethinkingModel2019,
  title = {Efficientnet: {{Rethinking}} Model Scaling for Convolutional Neural Networks},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Tan, Mingxing and Le, Quoc},
  date = {2019},
  pages = {6105--6114},
  publisher = {{PMLR}}
}

@article{teneyMultiviewFeatureDistributions2014,
  title = {Multiview Feature Distributions for Object Detection and Continuous Pose Estimation},
  author = {Teney, Damien and Piater, Justus},
  date = {2014},
  journaltitle = {Computer Vision and Image Understanding},
  volume = {125},
  pages = {265--282},
  publisher = {{Elsevier}}
}

@article{theprecise4qconsortiumExplainabilityArtificialIntelligence2020,
  title = {Explainability for Artificial Intelligence in Healthcare: A Multidisciplinary Perspective},
  shorttitle = {Explainability for Artificial Intelligence in Healthcare},
  author = {{the Precise4Q consortium} and Amann, Julia and Blasimme, Alessandro and Vayena, Effy and Frey, Dietmar and Madai, Vince I.},
  date = {2020-12},
  journaltitle = {BMC Medical Informatics and Decision Making},
  shortjournal = {BMC Med Inform Decis Mak},
  volume = {20},
  number = {1},
  pages = {310},
  issn = {1472-6947},
  doi = {10.1186/s12911-020-01332-6},
  url = {https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-01332-6},
  urldate = {2023-01-17},
  abstract = {Abstract                            Background               Explainability is one of the most heavily debated topics when it comes to the application of artificial intelligence (AI) in healthcare. Even though AI-driven systems have been shown to outperform humans in certain analytical tasks, the lack of explainability continues to spark criticism. Yet, explainability is not a purely technological issue, instead it invokes a host of medical, legal, ethical, and societal questions that require thorough exploration. This paper provides a comprehensive assessment of the role of explainability in medical AI and makes an ethical evaluation of what explainability means for the adoption of AI-driven tools into clinical practice.                                         Methods               Taking AI-based clinical decision support systems as a case in point, we adopted a multidisciplinary approach to analyze the relevance of explainability for medical AI from the technological, legal, medical, and patient perspectives. Drawing on the findings of this conceptual analysis, we then conducted an ethical assessment using the “Principles of Biomedical Ethics” by Beauchamp and Childress (autonomy, beneficence, nonmaleficence, and justice) as an analytical framework to determine the need for explainability in medical AI.                                         Results               Each of the domains highlights a different set of core considerations and values that are relevant for understanding the role of explainability in clinical practice. From the technological point of view, explainability has to be considered both in terms how it can be achieved and what is beneficial from a development perspective. When looking at the legal perspective we identified informed consent, certification and approval as medical devices, and liability as core touchpoints for explainability. Both the medical and patient perspectives emphasize the importance of considering the interplay between human actors and medical AI. We conclude that omitting explainability in clinical decision support systems poses a threat to core ethical values in medicine and may have detrimental consequences for individual and public health.                                         Conclusions               To ensure that medical AI lives up to its promises, there is a need to sensitize developers, healthcare professionals, and legislators to the challenges and limitations of opaque algorithms in medical AI and to foster multidisciplinary collaboration moving forward.},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/867RVLHC/the Precise4Q consortium et al. - 2020 - Explainability for artificial intelligence in heal.pdf}
}

@article{thrunProbabilisticRobotics2002,
  title = {Probabilistic Robotics},
  author = {Thrun, Sebastian},
  date = {2002},
  journaltitle = {Communications of the ACM},
  volume = {45},
  number = {3},
  pages = {52--57},
  publisher = {{ACM New York, NY, USA}}
}

@incollection{tommasiDeeperLookDataset2017,
  title = {A {{Deeper Look}} at {{Dataset Bias}}},
  booktitle = {Domain {{Adaptation}} in {{Computer Vision Applications}}},
  author = {Tommasi, Tatiana and Patricia, Novi and Caputo, Barbara and Tuytelaars, Tinne},
  editor = {Csurka, Gabriela},
  date = {2017},
  series = {Advances in {{Computer Vision}} and {{Pattern Recognition}}},
  pages = {37--55},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-58347-1_2},
  url = {https://doi.org/10.1007/978-3-319-58347-1_2},
  urldate = {2022-09-06},
  abstract = {The presence of a bias in each image data collection has recently attracted a lot of attention in the computer vision community showing the limits in generalization of any learning method trained on a specific dataset. At the same time, with the rapid development of deep learning architectures, the activation values of Convolutional Neural Networks (CNN) are emerging as reliable and robust image descriptors. In this chapter we propose to verify the potential of the CNN features when facing the dataset biasDataset biasproblem. With this purpose we introduce a large testbed for cross-dataset analysis and we discuss the challenges faced to create two comprehensive experimental setups by aligning twelve existing image databases. We conduct a series of analyses looking at how the datasets differ among each other and verifying the performance of existing debiasing methods under different representations. We learn important lessons on which part of the dataset bias problem can be considered solved and which open questions still need to be tackled.},
  isbn = {978-3-319-58347-1},
  langid = {english},
  keywords = {Capture Bias,Cross-dataset Generalization,Dataset Bias,Robust Image Descriptors,Train Images}
}

@inproceedings{torralbaUnbiasedLookDataset2011,
  title = {Unbiased Look at Dataset Bias},
  booktitle = {{{CVPR}} 2011},
  author = {Torralba, Antonio and Efros, Alexei A.},
  date = {2011-06},
  pages = {1521--1528},
  publisher = {{IEEE}},
  location = {{Colorado Springs, CO, USA}},
  doi = {10.1109/CVPR.2011.5995347},
  url = {http://ieeexplore.ieee.org/document/5995347/},
  urldate = {2023-01-18},
  eventtitle = {2011 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-4577-0394-2},
  file = {/Users/eragon/Zotero/storage/V9N3FX2F/Torralba and Efros - 2011 - Unbiased look at dataset bias.pdf}
}

@article{turelTimeDistortionWhen2018,
  title = {Time Distortion When Users At-Risk for Social Media Addiction Engage in Non-Social Media Tasks},
  author = {Turel, Ofir and Brevers, Damien and Bechara, Antoine},
  date = {2018-02},
  journaltitle = {Journal of Psychiatric Research},
  shortjournal = {Journal of Psychiatric Research},
  volume = {97},
  pages = {84--88},
  issn = {00223956},
  doi = {10.1016/j.jpsychires.2017.11.014},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0022395617308750},
  urldate = {2022-06-15},
  abstract = {Background: There is a growing concern over the addictiveness of Social Media use. Additional representative indicators of impaired control are needed in order to distinguish presumed social media addiction from normal use. Aims: (1) To examine the existence of time distortion during non-social-media use tasks that involve social media cues among those who may be considered at-risk for social media addiction. (2) To examine the usefulness of this distortion for at-risk vs. low/no-risk classification. Method: We used a task that prevented Facebook use and invoked Facebook reflections (survey on self-control strategies) and subsequently measured estimated vs. actual task completion time. We captured the level of addiction using the Bergen Facebook Addiction Scale in the survey, and we used a common cutoff criterion to classify people as at-risk vs. low/no-risk of Facebook addiction. Results: The at-risk group presented significant upward time estimate bias and the low/no-risk group presented significant downward time estimate bias. The bias was positively correlated with Facebook addiction scores. It was efficacious, especially when combined with self-reported estimates of extent of Facebook use, in classifying people to the two categories. Conclusions: Our study points to a novel, easy to obtain, and useful marker of at-risk for social media addiction, which may be considered for inclusion in diagnosis tools and procedures.},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/BI8NBN4V/Turel et al. - 2018 - Time distortion when users at-risk for social medi.pdf}
}

@online{uddinSaliencyMixSaliencyGuided2021,
  title = {{{SaliencyMix}}: {{A Saliency Guided Data Augmentation Strategy}} for {{Better Regularization}}},
  shorttitle = {{{SaliencyMix}}},
  author = {Uddin, A. F. M. Shahab and Monira, Mst Sirazam and Shin, Wheemyung and Chung, TaeChoong and Bae, Sung-Ho},
  date = {2021-07-27},
  eprint = {2006.01791},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2006.01791},
  urldate = {2023-04-11},
  abstract = {Advanced data augmentation strategies have widely been studied to improve the generalization ability of deep learning models. Regional dropout is one of the popular solutions that guides the model to focus on less discriminative parts by randomly removing image regions, resulting in improved regularization. However, such information removal is undesirable. On the other hand, recent strategies suggest to randomly cut and mix patches and their labels among training images, to enjoy the advantages of regional dropout without having any pointless pixel in the augmented images. We argue that such random selection strategies of the patches may not necessarily represent sufficient information about the corresponding object and thereby mixing the labels according to that uninformative patch enables the model to learn unexpected feature representation. Therefore, we propose SaliencyMix that carefully selects a representative image patch with the help of a saliency map and mixes this indicative patch with the target image, thus leading the model to learn more appropriate feature representation. SaliencyMix achieves the best known top-1 error of 21.26\% and 20.09\% for ResNet-50 and ResNet-101 architectures on ImageNet classification, respectively, and also improves the model robustness against adversarial perturbations. Furthermore, models that are trained with SaliencyMix help to improve the object detection performance. Source code is available at https://github.com/SaliencyMix/SaliencyMix.},
  pubstate = {preprint},
  keywords = {68T07,Computer Science - Machine Learning,I.2,I.4,Statistics - Machine Learning},
  file = {/Users/eragon/Zotero/storage/9QAI8L7R/Uddin et al. - 2021 - SaliencyMix A Saliency Guided Data Augmentation S.pdf}
}

@online{ulrichInteractiveVisualFeature2022,
  title = {Interactive {{Visual Feature Search}}},
  author = {Ulrich, Devon and Fong, Ruth},
  date = {2022-11-27},
  eprint = {2211.15060},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2211.15060},
  url = {http://arxiv.org/abs/2211.15060},
  urldate = {2023-05-25},
  abstract = {Many visualization techniques have been created to help explain the behavior of convolutional neural networks (CNNs), but they largely consist of static diagrams that convey limited information. Interactive visualizations can provide more rich insights and allow users to more easily explore a model's behavior; however, they are typically not easily reusable and are specific to a particular model. We introduce Visual Feature Search, a novel interactive visualization that is generalizable to any CNN and can easily be incorporated into a researcher's workflow. Our tool allows a user to highlight an image region and search for images from a given dataset with the most similar CNN features. It supports searching through large image datasets with an efficient cache-based search implementation. We demonstrate how our tool elucidates different aspects of model behavior by performing experiments on supervised, self-supervised, and human-edited CNNs. We also release a portable Python library and several IPython notebooks to enable researchers to easily use our tool in their own experiments. Our code can be found at https://github.com/lookingglasslab/VisualFeatureSearch.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/eragon/Zotero/storage/MU6YGBS2/Ulrich and Fong - 2022 - Interactive Visual Feature Search.pdf}
}

@article{urbachQuantifiersAreIncrementally2015,
  title = {Quantifiers Are Incrementally Interpreted in Context, More than Less},
  author = {Urbach, Thomas P. and DeLong, Katherine A. and Kutas, Marta},
  date = {2015-08},
  journaltitle = {Journal of Memory and Language},
  shortjournal = {Journal of Memory and Language},
  volume = {83},
  pages = {79--96},
  issn = {0749596X},
  doi = {10.1016/j.jml.2015.03.010},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0749596X15000431},
  urldate = {2022-10-24},
  abstract = {Language interpretation is often assumed to be incremental. However, our studies of quantifier expressions in isolated sentences found N400 event-related brain potential (ERP) evidence for partial but not full immediate quantifier interpretation (Urbach \& Kutas, 2010). Here we tested similar quantifier expressions in pragmatically supporting discourse contexts (Alex was an unusual toddler. Most/Few kids prefer sweets/vegetables . . .) while participants made plausibility judgments (Experiment 1) or read for comprehension (Experiment 2). Control Experiments 3A (plausibility) and 3B (comprehension) removed the discourse contexts. Quantifiers always modulated typical and/or atypical word N400 amplitudes. However, the real-time N400 effects only in Experiment 2 mirrored offline quantifier and typicality crossover interaction effects for plausibility ratings and cloze probabilities. We conclude that quantifier expressions can be interpreted fully and immediately, though pragmatic and task variables appear to impact the speed and/or depth of quantifier interpretation.},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/IV6HBB9S/Urbach et al. - 2015 - Quantifiers are incrementally interpreted in conte.pdf}
}

@online{valdenegro-toroMachineLearningStudents2022,
  title = {Machine {{Learning Students Overfit}} to {{Overfitting}}},
  author = {Valdenegro-Toro, Matias and Sabatelli, Matthia},
  date = {2022-09-07},
  eprint = {2209.03032},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2209.03032},
  url = {http://arxiv.org/abs/2209.03032},
  urldate = {2023-02-07},
  abstract = {Overfitting and generalization is an important concept in Machine Learning as only models that generalize are interesting for general applications. Yet some students have trouble learning this important concept through lectures and exercises. In this paper we describe common examples of students misunderstanding overfitting, and provide recommendations for possible solutions. We cover student misconceptions about overfitting, about solutions to overfitting, and implementation mistakes that are commonly confused with overfitting issues. We expect that our paper can contribute to improving student understanding and lectures about this important topic.},
  pubstate = {preprint},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/Users/eragon/Zotero/storage/ISZEZYFS/Valdenegro-Toro and Sabatelli - 2022 - Machine Learning Students Overfit to Overfitting.pdf}
}

@article{vanderveldenExplainableArtificialIntelligence2022,
  title = {Explainable Artificial Intelligence ({{XAI}}) in Deep Learning-Based Medical Image Analysis},
  author = {family=Velden, given=Bas H. M., prefix=van der, useprefix=true and Kuijf, Hugo J. and Gilhuijs, Kenneth G. A. and Viergever, Max A.},
  date = {2022-07-01},
  journaltitle = {Medical Image Analysis},
  shortjournal = {Medical Image Analysis},
  volume = {79},
  pages = {102470},
  issn = {1361-8415},
  doi = {10.1016/j.media.2022.102470},
  url = {https://www.sciencedirect.com/science/article/pii/S1361841522001177},
  urldate = {2023-02-13},
  abstract = {With an increase in deep learning-based methods, the call for explainability of such methods grows, especially in high-stakes decision making areas such as medical image analysis. This survey presents an overview of explainable artificial intelligence (XAI) used in deep learning-based medical image analysis. A framework of XAI criteria is introduced to classify deep learning-based medical image analysis methods. Papers on XAI techniques in medical image analysis are then surveyed and categorized according to the framework and according to anatomical location. The paper concludes with an outlook of future opportunities for XAI in medical image analysis.},
  langid = {english},
  keywords = {Deep learning,Explainable artificial intelligence,Interpretable deep learning,Medical image analysis,Survey},
  file = {/Users/eragon/Zotero/storage/GPPCMD2T/van der Velden et al. - 2022 - Explainable artificial intelligence (XAI) in deep .pdf}
}

@article{vanlehnRelativeEffectivenessHuman2011,
  title = {The {{Relative Effectiveness}} of {{Human Tutoring}}, {{Intelligent Tutoring Systems}}, and {{Other Tutoring Systems}}},
  author = {VanLEHN, Kurt},
  date = {2011-10},
  journaltitle = {Educational Psychologist},
  shortjournal = {Educational Psychologist},
  volume = {46},
  number = {4},
  pages = {197--221},
  issn = {0046-1520, 1532-6985},
  doi = {10.1080/00461520.2011.611369},
  url = {http://www.tandfonline.com/doi/abs/10.1080/00461520.2011.611369},
  urldate = {2022-11-13},
  langid = {english}
}

@online{vaswaniAttentionAllYou2017,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  date = {2017-12-05},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1706.03762},
  url = {http://arxiv.org/abs/1706.03762},
  urldate = {2022-10-21},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/eragon/Zotero/storage/WIMZKX6P/Vaswani et al. - 2017 - Attention Is All You Need.pdf}
}

@inproceedings{vazquezViewpointSelectionUsing2001,
  title = {Viewpoint Selection Using Viewpoint Entropy.},
  booktitle = {{{VMV}}},
  author = {Vázquez, Pere-Pau and Feixas, Miquel and Sbert, Mateu and Heidrich, Wolfgang},
  date = {2001},
  volume = {1},
  pages = {273--280},
  publisher = {{Citeseer}}
}

@unpublished{venugopalanItEasyFool2020,
  title = {It's Easy to Fool Yourself: {{Case}} Studies on Identifying Bias and Confounding in Bio-Medical Datasets},
  shorttitle = {It's Easy to Fool Yourself},
  author = {Venugopalan, Subhashini and Narayanaswamy, Arunachalam and Yang, Samuel and Geraschenko, Anton and Lipnick, Scott and Makhortova, Nina and Hawrot, James and Marques, Christine and Pereira, Joao and Brenner, Michael and Rubin, Lee and Wainger, Brian and Berndl, Marc},
  date = {2020-04-06},
  eprint = {1912.07661},
  eprinttype = {arxiv},
  eprintclass = {cs, eess, q-bio, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1912.07661},
  url = {http://arxiv.org/abs/1912.07661},
  urldate = {2022-09-06},
  abstract = {Confounding variables are a well known source of nuisance in biomedical studies. They present an even greater challenge when we combine them with black-box machine learning techniques that operate on raw data. This work presents two case studies. In one, we discovered biases arising from systematic errors in the data generation process. In the other, we found a spurious source of signal unrelated to the prediction task at hand. In both cases, our prediction models performed well but under careful examination hidden confounders and biases were revealed. These are cautionary tales on the limits of using machine learning techniques on raw data from scientific experiments.},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,Quantitative Biology - Quantitative Methods,Statistics - Machine Learning},
  file = {/Users/eragon/Zotero/storage/JC4H8PF9/Venugopalan et al. - 2020 - It's easy to fool yourself Case studies on identi.pdf}
}

@online{VisualizingImpactFeature,
  title = {Visualizing the {{Impact}} of {{Feature Attribution Baselines}}},
  url = {https://distill.pub/2020/attribution-baselines/},
  urldate = {2022-07-31},
  file = {/Users/eragon/Zotero/storage/3PZKPGR2/attribution-baselines.html}
}

@online{walawalkarAttentiveCutMixEnhanced2020,
  title = {Attentive {{CutMix}}: {{An Enhanced Data Augmentation Approach}} for {{Deep Learning Based Image Classification}}},
  shorttitle = {Attentive {{CutMix}}},
  author = {Walawalkar, Devesh and Shen, Zhiqiang and Liu, Zechun and Savvides, Marios},
  date = {2020-04-05},
  eprint = {2003.13048},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2003.13048},
  url = {http://arxiv.org/abs/2003.13048},
  urldate = {2023-03-29},
  abstract = {Convolutional neural networks (CNN) are capable of learning robust representation with different regularization methods and activations as convolutional layers are spatially correlated. Based on this property, a large variety of regional dropout strategies have been proposed, such as Cutout, DropBlock, CutMix, etc. These methods aim to promote the network to generalize better by partially occluding the discriminative parts of objects. However, all of them perform this operation randomly, without capturing the most important region(s) within an object. In this paper, we propose Attentive CutMix, a naturally enhanced augmentation strategy based on CutMix. In each training iteration, we choose the most descriptive regions based on the intermediate attention maps from a feature extractor, which enables searching for the most discriminative parts in an image. Our proposed method is simple yet effective, easy to implement and can boost the baseline significantly. Extensive experiments on CIFAR-10/100, ImageNet datasets with various CNN architectures (in a unified setting) demonstrate the effectiveness of our proposed method, which consistently outperforms the baseline CutMix and other methods by a significant margin.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/eragon/Zotero/storage/5ZN5WF6N/Walawalkar et al. - 2020 - Attentive CutMix An Enhanced Data Augmentation Ap.pdf}
}

@online{wangEntailmentFewShotLearner2021,
  title = {Entailment as {{Few-Shot Learner}}},
  author = {Wang, Sinong and Fang, Han and Khabsa, Madian and Mao, Hanzi and Ma, Hao},
  date = {2021-04-29},
  eprint = {2104.14690},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2104.14690},
  urldate = {2023-04-18},
  abstract = {Large pre-trained language models (LMs) have demonstrated remarkable ability as few-shot learners. However, their success hinges largely on scaling model parameters to a degree that makes it challenging to train and serve. In this paper, we propose a new approach, named as EFL, that can turn small LMs into better few-shot learners. The key idea of this approach is to reformulate potential NLP task into an entailment one, and then fine-tune the model with as little as 8 examples. We further demonstrate our proposed method can be: (i) naturally combined with an unsupervised contrastive learning-based data augmentation method; (ii) easily extended to multilingual few-shot learning. A systematic evaluation on 18 standard NLP tasks demonstrates that this approach improves the various existing SOTA few-shot learning methods by 12\textbackslash\%, and yields competitive few-shot performance with 500 times larger models, such as GPT-3.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/eragon/Zotero/storage/MJ7RJDDW/Wang et al. - 2021 - Entailment as Few-Shot Learner.pdf}
}

@article{wangMedicalImageSegmentation2022,
  title = {Medical Image Segmentation Using Deep Learning: {{A}} Survey},
  shorttitle = {Medical Image Segmentation Using Deep Learning},
  author = {Wang, Risheng and Lei, Tao and Cui, Ruixia and Zhang, Bingtao and Meng, Hongying and Nandi, Asoke K.},
  date = {2022},
  journaltitle = {IET Image Processing},
  volume = {16},
  number = {5},
  pages = {1243--1267},
  issn = {1751-9667},
  doi = {10.1049/ipr2.12419},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/ipr2.12419},
  urldate = {2022-08-16},
  abstract = {Deep learning has been widely used for medical image segmentation and a large number of papers has been presented recording the success of deep learning in the field. A comprehensive thematic survey on medical image segmentation using deep learning techniques is presented. This paper makes two original contributions. Firstly, compared to traditional surveys that directly divide literatures of deep learning on medical image segmentation into many groups and introduce literatures in detail for each group, we classify currently popular literatures according to a multi-level structure from coarse to fine. Secondly, this paper focuses on supervised and weakly supervised learning approaches, without including unsupervised approaches since they have been introduced in many old surveys and they are not popular currently. For supervised learning approaches, we analyse literatures in three aspects: the selection of backbone networks, the design of network blocks, and the improvement of loss functions. For weakly supervised learning approaches, we investigate literature according to data augmentation, transfer learning, and interactive segmentation, separately. Compared to existing surveys, this survey classifies the literatures very differently from before and is more convenient for readers to understand the relevant rationale and will guide them to think of appropriate improvements in medical image segmentation based on deep learning approaches.},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/SLKBYZ23/Wang et al. - 2022 - Medical image segmentation using deep learning A .pdf;/Users/eragon/Zotero/storage/YC4XEHWA/ipr2.html}
}

@article{wangNormalNetVoxelbasedCNN2019,
  title = {{{NormalNet}}: {{A}} Voxel-Based {{CNN}} for {{3D}} Object Classification and Retrieval},
  shorttitle = {{{NormalNet}}},
  author = {Wang, Cheng and Cheng, Ming and Sohel, Ferdous and Bennamoun, Mohammed and Li, Jonathan},
  date = {2019-01},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {323},
  pages = {139--147},
  issn = {09252312},
  doi = {10.1016/j.neucom.2018.09.075},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0925231218311561},
  urldate = {2022-05-10},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/WM22HW55/Wang et al. - 2019 - NormalNet A voxel-based CNN for 3D object classif.pdf}
}

@online{wangScoreCAMScoreWeightedVisual2020,
  title = {Score-{{CAM}}: {{Score-Weighted Visual Explanations}} for {{Convolutional Neural Networks}}},
  shorttitle = {Score-{{CAM}}},
  author = {Wang, Haofan and Wang, Zifan and Du, Mengnan and Yang, Fan and Zhang, Zijian and Ding, Sirui and Mardziel, Piotr and Hu, Xia},
  date = {2020-04-13},
  eprint = {1910.01279},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1910.01279},
  urldate = {2023-02-16},
  abstract = {Recently, increasing attention has been drawn to the internal mechanisms of convolutional neural networks, and the reason why the network makes specific decisions. In this paper, we develop a novel post-hoc visual explanation method called Score-CAM based on class activation mapping. Unlike previous class activation mapping based approaches, Score-CAM gets rid of the dependence on gradients by obtaining the weight of each activation map through its forward passing score on target class, the final result is obtained by a linear combination of weights and activation maps. We demonstrate that Score-CAM achieves better visual performance and fairness for interpreting the decision making process. Our approach outperforms previous methods on both recognition and localization tasks, it also passes the sanity check. We also indicate its application as debugging tools. Official code has been released.},
  pubstate = {preprint},
  version = {2},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/eragon/Zotero/storage/AR2AG5TE/Wang et al. - 2020 - Score-CAM Score-Weighted Visual Explanations for .pdf}
}

@inproceedings{wangScoreCAMScoreWeightedVisual2020a,
  title = {Score-{{CAM}}: {{Score-Weighted Visual Explanations}} for {{Convolutional Neural Networks}}},
  shorttitle = {Score-{{CAM}}},
  author = {Wang, Haofan and Wang, Zifan and Du, Mengnan and Yang, Fan and Zhang, Zijian and Ding, Sirui and Mardziel, Piotr and Hu, Xia},
  date = {2020},
  pages = {24--25},
  url = {https://openaccess.thecvf.com/content_CVPRW_2020/html/w1/Wang_Score-CAM_Score-Weighted_Visual_Explanations_for_Convolutional_Neural_Networks_CVPRW_2020_paper.html},
  urldate = {2023-02-20},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition Workshops}}},
  file = {/Users/eragon/Zotero/storage/RBF7B8IJ/Wang et al. - 2020 - Score-CAM Score-Weighted Visual Explanations for .pdf}
}

@article{weilnhammerPredictiveCodingAccount2017,
  title = {A Predictive Coding Account of Bistable Perception - a Model-Based {{fMRI}} Study},
  author = {Weilnhammer, Veith and Stuke, Heiner and Hesselmann, Guido and Sterzer, Philipp and Schmack, Katharina},
  editor = {Daunizeau, Jean},
  date = {2017-05-15},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLoS Comput Biol},
  volume = {13},
  number = {5},
  pages = {e1005536},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1005536},
  url = {https://dx.plos.org/10.1371/journal.pcbi.1005536},
  urldate = {2022-05-27},
  abstract = {In bistable vision, subjective perception wavers between two interpretations of a constant ambiguous stimulus. This dissociation between conscious perception and sensory stimulation has motivated various empirical studies on the neural correlates of bistable perception, but the neurocomputational mechanism behind endogenous perceptual transitions has remained elusive. Here, we recurred to a generic Bayesian framework of predictive coding and devised a model that casts endogenous perceptual transitions as a consequence of prediction errors emerging from residual evidence for the suppressed percept. Data simulations revealed close similarities between the model’s predictions and key temporal characteristics of perceptual bistability, indicating that the model was able to reproduce bistable perception. Fitting the predictive coding model to behavioural data from an fMRI-experiment on bistable perception, we found a correlation across participants between the model parameter encoding perceptual stabilization and the behaviourally measured frequency of perceptual transitions, corroborating that the model successfully accounted for participants’ perception. Formal model comparison with established models of bistable perception based on mutual inhibition and adaptation, noise or a combination of adaptation and noise was used for the validation of the predictive coding model against the established models. Most importantly, model-based analyses of the fMRI data revealed that prediction error timecourses derived from the predictive coding model correlated with neural signal time-courses in bilateral inferior frontal gyri and anterior insulae. Voxel-wise model selection indicated a superiority of the predictive coding model over conventional analysis approaches in explaining neural activity in these frontal areas, suggesting that frontal cortex encodes prediction errors that mediate endogenous perceptual transitions in bistable perception. Taken together, our current work provides a theoretical framework that allows for the analysis of behavioural and neural data using a predictive coding perspective on bistable perception. In this, our approach posits a crucial role of prediction error signalling for the resolution of perceptual ambiguities.},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/ZLGXJBNB/Weilnhammer et al. - 2017 - A predictive coding account of bistable perception.pdf}
}

@online{weiTransferableAdversarialAttacks2022,
  title = {Towards {{Transferable Adversarial Attacks}} on {{Vision Transformers}}},
  author = {Wei, Zhipeng and Chen, Jingjing and Goldblum, Micah and Wu, Zuxuan and Goldstein, Tom and Jiang, Yu-Gang},
  date = {2022-01-02},
  eprint = {2109.04176},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2109.04176},
  url = {http://arxiv.org/abs/2109.04176},
  urldate = {2022-11-19},
  abstract = {Vision transformers (ViTs) have demonstrated impressive performance on a series of computer vision tasks, yet they still suffer from adversarial examples. \% crafted in a similar fashion as CNNs. In this paper, we posit that adversarial attacks on transformers should be specially tailored for their architecture, jointly considering both patches and self-attention, in order to achieve high transferability. More specifically, we introduce a dual attack framework, which contains a Pay No Attention (PNA) attack and a PatchOut attack, to improve the transferability of adversarial samples across different ViTs. We show that skipping the gradients of attention during backpropagation can generate adversarial examples with high transferability. In addition, adversarial perturbations generated by optimizing randomly sampled subsets of patches at each iteration achieve higher attack success rates than attacks using all patches. We evaluate the transferability of attacks on state-of-the-art ViTs, CNNs and robustly trained CNNs. The results of these experiments demonstrate that the proposed dual attack can greatly boost transferability between ViTs and from ViTs to CNNs. In addition, the proposed method can easily be combined with existing transfer methods to boost performance. Code is available at https://github.com/zhipeng-wei/PNA-PatchOut.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/eragon/Zotero/storage/27296AC5/Wei et al. - 2022 - Towards Transferable Adversarial Attacks on Vision.pdf}
}

@article{wickramanayakeExplanationbasedDataAugmentation,
  title = {Explanation-Based {{Data Augmentation}} for {{Image Classification}}},
  author = {Wickramanayake, Sandareka and Lee, Mong Li and Hsu, Wynne},
  pages = {12},
  abstract = {Existing works have generated explanations for deep neural network decisions to provide insights into model behavior. We observe that these explanations can also be used to identify concepts that caused misclassifications. This allows us to understand the possible limitations of the dataset used to train the model, particularly the under-represented regions in the dataset. This work proposes a framework that utilizes concept-based explanations to automatically augment the dataset with new images that can cover these under-represented regions to improve the model performance. The framework is able to use the explanations generated by both interpretable classifiers and post-hoc explanations from black-box classifiers. Experiment results demonstrate that the proposed approach improves the accuracy of classifiers compared to state-of-the-art augmentation strategies.},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/9H2FTFQ7/Wickramanayake et al. - Explanation-based Data Augmentation for Image Clas.pdf}
}

@article{wickramanayakeExplanationbasedDataAugmentationa,
  title = {Explanation-Based {{Data Augmentation}} for {{Image Classification}}},
  author = {Wickramanayake, Sandareka and Lee, Mong Li and Hsu, Wynne},
  abstract = {Existing works have generated explanations for deep neural network decisions to provide insights into model behavior. We observe that these explanations can also be used to identify concepts that caused misclassifications. This allows us to understand the possible limitations of the dataset used to train the model, particularly the under-represented regions in the dataset. This work proposes a framework that utilizes concept-based explanations to automatically augment the dataset with new images that can cover these under-represented regions to improve the model performance. The framework is able to use the explanations generated by both interpretable classifiers and post-hoc explanations from black-box classifiers. Experiment results demonstrate that the proposed approach improves the accuracy of classifiers compared to state-of-the-art augmentation strategies.},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/SSU2VK7T/Wickramanayake et al. - Explanation-based Data Augmentation for Image Clas.pdf}
}

@software{wightmanRwightmanPytorchimagemodelsV02023,
  title = {Rwightman/Pytorch-Image-Models: V0.8.10dev0 {{Release}}},
  shorttitle = {Rwightman/Pytorch-Image-Models},
  author = {Wightman, Ross and Raw, Nathan and Soare, Alexander and Arora, Aman and Ha, Chris and Reich, Christoph and Guan, Fredo and Kaczmarzyk, Jakub and {mrT23} and Mike and SeeFun and {contrastive} and Rizin, Mohammed and Kim, Hyeongchan and Kertész, Csaba and Mehta, Dushyant and Cucurull, Guillem and Singh, Kushajveer and {hankyul} and Tatsunami, Yuki and Lavin, Andrew and Zhuang, Juntang and Hollemans, Matthijs and Rashad, Mohamed and Sameni, Sepehr and Shults, Vyacheslav and Lucain and Wang, Xiao and Kwon, Yonghye and Uchida, Yusuke},
  date = {2023-02-07},
  doi = {10.5281/zenodo.7618837},
  url = {https://zenodo.org/record/7618837},
  urldate = {2023-05-07},
  abstract = {Feb 7, 2023 New inference benchmark numbers added in results folder. Add convnext LAION CLIP trained weights and initial set of in1k fine-tunes convnext\_base.clip\_laion2b\_augreg\_ft\_in1k - 86.2\% @ 256x256 convnext\_base.clip\_laiona\_augreg\_ft\_in1k\_384 - 86.5\% @ 384x384 convnext\_large\_mlp.clip\_laion2b\_augreg\_ft\_in1k - 87.3\% @ 256x256 convnext\_large\_mlp.clip\_laion2b\_augreg\_ft\_in1k\_384 - 87.9\% @ 384x384 Add DaViT models. Supports features\_only=True. Adapted from https://github.com/dingmyu/davit by Fredo. Use a common NormMlpClassifierHead across MaxViT, ConvNeXt, DaViT Add EfficientFormer-V2 model, update EfficientFormer, and refactor LeViT (closely related architectures). Weights on HF hub. New EfficientFormer-V2 arch, significant refactor from original at (https://github.com/snap-research/EfficientFormer). Supports features\_only=True. Minor updates to EfficientFormer. Refactor LeViT models to stages, add features\_only=True support to new conv variants, weight remap required. Move ImageNet meta-data (synsets, indices) from /results to timm/data/\_info. Add ImageNetInfo / DatasetInfo classes to provide labelling for various ImageNet classifier layouts in timm Update inference.py to use, try: python inference.py /folder/to/images --model convnext\_small.in12k --label-type detail --topk 5 Ready for 0.8.10 pypi pre-release (final testing). Jan 20, 2023 Add two convnext 12k -{$>$} 1k fine-tunes at 384x384 convnext\_tiny.in12k\_ft\_in1k\_384 - 85.1 @ 384 convnext\_small.in12k\_ft\_in1k\_384 - 86.2 @ 384 Push all MaxxViT weights to HF hub, and add new ImageNet-12k -{$>$} 1k fine-tunes for rw base MaxViT and CoAtNet 1/2 models model top1 top5 samples / sec Params (M) GMAC Act (M) maxvit\_xlarge\_tf\_512.in21k\_ft\_in1k 88.53 98.64 21.76 475.77 534.14 1413.22 maxvit\_xlarge\_tf\_384.in21k\_ft\_in1k 88.32 98.54 42.53 475.32 292.78 668.76 maxvit\_base\_tf\_512.in21k\_ft\_in1k 88.20 98.53 50.87 119.88 138.02 703.99 maxvit\_large\_tf\_512.in21k\_ft\_in1k 88.04 98.40 36.42 212.33 244.75 942.15 maxvit\_large\_tf\_384.in21k\_ft\_in1k 87.98 98.56 71.75 212.03 132.55 445.84 maxvit\_base\_tf\_384.in21k\_ft\_in1k 87.92 98.54 104.71 119.65 73.80 332.90 maxvit\_rmlp\_base\_rw\_384.sw\_in12k\_ft\_in1k 87.81 98.37 106.55 116.14 70.97 318.95 maxxvitv2\_rmlp\_base\_rw\_384.sw\_in12k\_ft\_in1k 87.47 98.37 149.49 116.09 72.98 213.74 coatnet\_rmlp\_2\_rw\_384.sw\_in12k\_ft\_in1k 87.39 98.31 160.80 73.88 47.69 209.43 maxvit\_rmlp\_base\_rw\_224.sw\_in12k\_ft\_in1k 86.89 98.02 375.86 116.14 23.15 92.64 maxxvitv2\_rmlp\_base\_rw\_224.sw\_in12k\_ft\_in1k 86.64 98.02 501.03 116.09 24.20 62.77 maxvit\_base\_tf\_512.in1k 86.60 97.92 50.75 119.88 138.02 703.99 coatnet\_2\_rw\_224.sw\_in12k\_ft\_in1k 86.57 97.89 631.88 73.87 15.09 49.22 maxvit\_large\_tf\_512.in1k 86.52 97.88 36.04 212.33 244.75 942.15 coatnet\_rmlp\_2\_rw\_224.sw\_in12k\_ft\_in1k 86.49 97.90 620.58 73.88 15.18 54.78 maxvit\_base\_tf\_384.in1k 86.29 97.80 101.09 119.65 73.80 332.90 maxvit\_large\_tf\_384.in1k 86.23 97.69 70.56 212.03 132.55 445.84 maxvit\_small\_tf\_512.in1k 86.10 97.76 88.63 69.13 67.26 383.77 maxvit\_tiny\_tf\_512.in1k 85.67 97.58 144.25 31.05 33.49 257.59 maxvit\_small\_tf\_384.in1k 85.54 97.46 188.35 69.02 35.87 183.65 maxvit\_tiny\_tf\_384.in1k 85.11 97.38 293.46 30.98 17.53 123.42 maxvit\_large\_tf\_224.in1k 84.93 96.97 247.71 211.79 43.68 127.35 coatnet\_rmlp\_1\_rw2\_224.sw\_in12k\_ft\_in1k 84.90 96.96 1025.45 41.72 8.11 40.13 maxvit\_base\_tf\_224.in1k 84.85 96.99 358.25 119.47 24.04 95.01 maxxvit\_rmlp\_small\_rw\_256.sw\_in1k 84.63 97.06 575.53 66.01 14.67 58.38 coatnet\_rmlp\_2\_rw\_224.sw\_in1k 84.61 96.74 625.81 73.88 15.18 54.78 maxvit\_rmlp\_small\_rw\_224.sw\_in1k 84.49 96.76 693.82 64.90 10.75 49.30 maxvit\_small\_tf\_224.in1k 84.43 96.83 647.96 68.93 11.66 53.17 maxvit\_rmlp\_tiny\_rw\_256.sw\_in1k 84.23 96.78 807.21 29.15 6.77 46.92 coatnet\_1\_rw\_224.sw\_in1k 83.62 96.38 989.59 41.72 8.04 34.60 maxvit\_tiny\_rw\_224.sw\_in1k 83.50 96.50 1100.53 29.06 5.11 33.11 maxvit\_tiny\_tf\_224.in1k 83.41 96.59 1004.94 30.92 5.60 35.78 coatnet\_rmlp\_1\_rw\_224.sw\_in1k 83.36 96.45 1093.03 41.69 7.85 35.47 maxxvitv2\_nano\_rw\_256.sw\_in1k 83.11 96.33 1276.88 23.70 6.26 23.05 maxxvit\_rmlp\_nano\_rw\_256.sw\_in1k 83.03 96.34 1341.24 16.78 4.37 26.05 maxvit\_rmlp\_nano\_rw\_256.sw\_in1k 82.96 96.26 1283.24 15.50 4.47 31.92 maxvit\_nano\_rw\_256.sw\_in1k 82.93 96.23 1218.17 15.45 4.46 30.28 coatnet\_bn\_0\_rw\_224.sw\_in1k 82.39 96.19 1600.14 27.44 4.67 22.04 coatnet\_0\_rw\_224.sw\_in1k 82.39 95.84 1831.21 27.44 4.43 18.73 coatnet\_rmlp\_nano\_rw\_224.sw\_in1k 82.05 95.87 2109.09 15.15 2.62 20.34 coatnext\_nano\_rw\_224.sw\_in1k 81.95 95.92 2525.52 14.70 2.47 12.80 coatnet\_nano\_rw\_224.sw\_in1k 81.70 95.64 2344.52 15.14 2.41 15.41 maxvit\_rmlp\_pico\_rw\_256.sw\_in1k 80.53 95.21 1594.71 7.52 1.85 24.86},
  organization = {{Zenodo}}
}

@inproceedings{wu3DShapenetsDeep2015,
  title = {{{3D}} Shapenets: {{A}} Deep Representation for Volumetric Shapes},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Wu, Zhirong and Song, Shuran and Khosla, Aditya and Yu, Fisher and Zhang, Linguang and Tang, Xiaoou and Xiao, Jianxiong},
  date = {2015},
  pages = {1912--1920}
}

@unpublished{wuLearningProbabilisticLatent2016,
  title = {Learning a Probabilistic Latent Space of Object Shapes via {{3D}} Generative-Adversarial Modeling},
  author = {Wu, Jiajun and Zhang, Chengkai and Xue, Tianfan and Freeman, William T and Tenenbaum, Joshua B},
  date = {2016},
  eprint = {1610.07584},
  eprinttype = {arxiv}
}

@online{xiaoOffsiteTuningTransferLearning2023,
  title = {Offsite-{{Tuning}}: {{Transfer Learning}} without {{Full Model}}},
  shorttitle = {Offsite-{{Tuning}}},
  author = {Xiao, Guangxuan and Lin, Ji and Han, Song},
  date = {2023-02-09},
  eprint = {2302.04870},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2302.04870},
  url = {http://arxiv.org/abs/2302.04870},
  urldate = {2023-02-13},
  abstract = {Transfer learning is important for foundation models to adapt to downstream tasks. However, many foundation models are proprietary, so users must share their data with model owners to fine-tune the models, which is costly and raise privacy concerns. Moreover, fine-tuning large foundation models is computation-intensive and impractical for most downstream users. In this paper, we propose Offsite-Tuning, a privacy-preserving and efficient transfer learning framework that can adapt billion-parameter foundation models to downstream data without access to the full model. In offsite-tuning, the model owner sends a light-weight adapter and a lossy compressed emulator to the data owner, who then fine-tunes the adapter on the downstream data with the emulator's assistance. The fine-tuned adapter is then returned to the model owner, who plugs it into the full model to create an adapted foundation model. Offsite-tuning preserves both parties' privacy and is computationally more efficient than the existing fine-tuning methods that require access to the full model weights. We demonstrate the effectiveness of offsite-tuning on various large language and vision foundation models. Offsite-tuning can achieve comparable accuracy as full model fine-tuning while being privacy-preserving and efficient, achieving 6.5x speedup and 5.6x memory reduction. Code is available at https://github.com/mit-han-lab/offsite-tuning.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/eragon/Zotero/storage/DGAWF3E2/Xiao et al. - 2023 - Offsite-Tuning Transfer Learning without Full Mod.pdf}
}

@article{xuanMVC3DSpatialCorrelated2019,
  title = {{{MV-C3D}}: {{A Spatial Correlated Multi-View 3D Convolutional Neural Networks}}},
  shorttitle = {{{MV-C3D}}},
  author = {Xuan, Qi and Li, Fuxian and Liu, Yi and Xiang, Yun},
  date = {2019},
  journaltitle = {IEEE Access},
  volume = {7},
  pages = {92528--92538},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2923022},
  abstract = {As the development of deep neural networks, 3D object recognition is becoming increasingly popular in the computer vision community. Many multi-view-based methods are proposed to improve the category recognition accuracy. These approaches mainly rely on multi-view images that are rendered with the whole circumference. In real-world applications, however, 3D objects are mostly observed from partial viewpoints in a less range. Therefore, we propose a multi-view-based 3D convolutional neural network that takes only part of contiguous multi-view images as input and can still maintain high accuracy. Moreover, our model takes these view images as a joint variable to better learn the spatially correlated features using 3D convolution and 3D max-pooling layers. The experimental results on ModelNet10 and ModelNet40 datasets show that our MV-C3D technique can achieve outstanding performance with multi-view images that are captured from partial angles with less range. The results on 3D-rotated real image dataset MIRO further demonstrate that MV-C3D is more adaptable in real-world scenarios. The classification accuracy can be further improved with the increasing number of view images.},
  eventtitle = {{{IEEE Access}}},
  keywords = {3D object classification,Convolution,convolutional neural network,Convolutional neural networks,Correlation,deep learning,Kernel,multi-view,Solid modeling,Three-dimensional displays,Two dimensional displays},
  file = {/Users/eragon/Zotero/storage/9538VQQ4/Xuan et al. - 2019 - MV-C3D A Spatial Correlated Multi-View 3D Convolu.pdf;/Users/eragon/Zotero/storage/IKV2NZUF/8736713.html}
}

@online{yamadaDoesRobustnessImageNet2022,
  title = {Does {{Robustness}} on {{ImageNet Transfer}} to {{Downstream Tasks}}?},
  author = {Yamada, Yutaro and Otani, Mayu},
  date = {2022-04-08},
  eprint = {2204.03934},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2204.03934},
  url = {http://arxiv.org/abs/2204.03934},
  urldate = {2022-10-23},
  abstract = {As clean ImageNet accuracy nears its ceiling, the research community is increasingly more concerned about robust accuracy under distributional shifts. While a variety of methods have been proposed to robustify neural networks, these techniques often target models trained on ImageNet classification. At the same time, it is a common practice to use ImageNet pretrained backbones for downstream tasks such as object detection, semantic segmentation, and image classification from different domains. This raises a question: Can these robust image classifiers transfer robustness to downstream tasks? For object detection and semantic segmentation, we find that a vanilla Swin Transformer, a variant of Vision Transformer tailored for dense prediction tasks, transfers robustness better than Convolutional Neural Networks that are trained to be robust to the corrupted version of ImageNet. For CIFAR10 classification, we find that models that are robustified for ImageNet do not retain robustness when fully fine-tuned. These findings suggest that current robustification techniques tend to emphasize ImageNet evaluations. Moreover, network architecture is a strong source of robustness when we consider transfer learning.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/eragon/Zotero/storage/9XAV52TC/Yamada and Otani - 2022 - Does Robustness on ImageNet Transfer to Downstream.pdf}
}

@inproceedings{yangFairerDatasetsFiltering2020,
  title = {Towards Fairer Datasets: Filtering and Balancing the Distribution of the People Subtree in the {{ImageNet}} Hierarchy},
  shorttitle = {Towards Fairer Datasets},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Yang, Kaiyu and Qinami, Klint and Fei-Fei, Li and Deng, Jia and Russakovsky, Olga},
  date = {2020-01-27},
  series = {{{FAT}}* '20},
  pages = {547--558},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3351095.3375709},
  url = {https://doi.org/10.1145/3351095.3375709},
  urldate = {2022-09-06},
  abstract = {Computer vision technology is being used by many but remains representative of only a few. People have reported misbehavior of computer vision models, including offensive prediction results and lower performance for underrepresented groups. Current computer vision models are typically developed using datasets consisting of manually annotated images or videos; the data and label distributions in these datasets are critical to the models' behavior. In this paper, we examine ImageNet, a large-scale ontology of images that has spurred the development of many modern computer vision methods. We consider three key factors within the person subtree of ImageNet that may lead to problematic behavior in downstream computer vision technology: (1) the stagnant concept vocabulary of WordNet, (2) the attempt at exhaustive illustration of all categories with images, and (3) the inequality of representation in the images within concepts. We seek to illuminate the root causes of these concerns and take the first steps to mitigate them constructively.},
  isbn = {978-1-4503-6936-7},
  keywords = {computer vision,dataset construction,fairness,representative datasets},
  file = {/Users/eragon/Zotero/storage/FVP8GKD7/Yang et al. - 2020 - Towards fairer datasets filtering and balancing t.pdf}
}

@online{yangImageDataAugmentation2022,
  title = {Image {{Data Augmentation}} for {{Deep Learning}}: {{A Survey}}},
  shorttitle = {Image {{Data Augmentation}} for {{Deep Learning}}},
  author = {Yang, Suorong and Xiao, Weikang and Zhang, Mengcheng and Guo, Suhan and Zhao, Jian and Shen, Furao},
  date = {2022-04-18},
  eprint = {2204.08610},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2204.08610},
  urldate = {2023-05-31},
  abstract = {Deep learning has achieved remarkable results in many computer vision tasks. Deep neural networks typically rely on large amounts of training data to avoid overfitting. However, labeled data for real-world applications may be limited. By improving the quantity and diversity of training data, data augmentation has become an inevitable part of deep learning model training with image data. As an effective way to improve the sufficiency and diversity of training data, data augmentation has become a necessary part of successful application of deep learning models on image data. In this paper, we systematically review different image data augmentation methods. We propose a taxonomy of reviewed methods and present the strengths and limitations of these methods. We also conduct extensive experiments with various data augmentation methods on three typical computer vision tasks, including semantic segmentation, image classification and object detection. Finally, we discuss current challenges faced by data augmentation and future research directions to put forward some useful research guidance.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/eragon/Zotero/storage/JXDWEKSE/Yang et al. - 2022 - Image Data Augmentation for Deep Learning A Surve.pdf}
}

@online{yangTransferLearningSelfsupervised2020,
  title = {Transfer {{Learning}} or {{Self-supervised Learning}}? {{A Tale}} of {{Two Pretraining Paradigms}}},
  shorttitle = {Transfer {{Learning}} or {{Self-supervised Learning}}?},
  author = {Yang, Xingyi and He, Xuehai and Liang, Yuxiao and Yang, Yue and Zhang, Shanghang and Xie, Pengtao},
  date = {2020-06-22},
  eprinttype = {TechRxiv},
  doi = {10.36227/techrxiv.12502298.v1},
  url = {https://www.techrxiv.org/articles/preprint/Transfer_Learning_or_Self-supervised_Learning_A_Tale_of_Two_Pretraining_Paradigms/12502298/1},
  urldate = {2023-03-27},
  abstract = {Pretraining has become a standard technique in computer vision and natural language processing, which usually helps to improve performance substantially. Previously, the most dominant pretraining method is transfer learning (TL), which uses labeled data to learn a good representation network. Recently, a new pretraining approach -- self-supervised learning (SSL) -- has demonstrated promising results on a wide range of applications. SSL does not require annotated labels. It is purely conducted on input data by solving auxiliary tasks defined on the input data examples. The current reported results show that in certain applications, SSL outperforms TL and the other way around in other applications. There has not been a clear understanding on what properties of data and tasks render one approach outperforms the other. Without an informed guideline, ML researchers have to try both methods to find out which one is better empirically. It is usually time-consuming to do so. In this work, we aim to address this problem. We perform a comprehensive comparative study between SSL and TL regarding which one works better under different properties of data and tasks, including domain difference between source and target tasks, the amount of pretraining data, class imbalance in source data, and usage of target data for additional pretraining, etc. The insights distilled from our comparative studies can help ML researchers decide which method to use based on the properties of their applications.},
  langid = {english},
  pubstate = {preprint},
  file = {/Users/eragon/Zotero/storage/GGQT8P4B/Yang et al. - 2020 - Transfer Learning or Self-supervised Learning A T.pdf}
}

@article{yeDuFeNetImproveAccuracy2022,
  title = {{{DuFeNet}}: {{Improve}} the {{Accuracy}} and {{Increase Shape Bias}} of {{Neural Network Models}}},
  shorttitle = {{{DuFeNet}}},
  author = {Ye, Zecong and Gao, Zhiqiang and Cui, Xiaolong and Wang, Yaojie and Shan, Nanliang},
  date = {2022-07},
  journaltitle = {Signal, Image and Video Processing},
  shortjournal = {SIViP},
  volume = {16},
  number = {5},
  pages = {1153--1160},
  issn = {1863-1703, 1863-1711},
  doi = {10.1007/s11760-021-02065-3},
  url = {https://link.springer.com/10.1007/s11760-021-02065-3},
  urldate = {2022-09-26},
  abstract = {In image classification field, existing work tends to modify the network structure to obtain higher accuracy or faster speed. However, some studies have found that the neural network usually has texture bias effect, which means that the neural network is more sensitive to the texture information than the shape information. Based on such phenomenon, we propose a new way to improve network performance by making full use of gradient information. The dual features network (DuFeNet) is proposed in this paper. In DuFeNet, one sub-network is used to learn the information of gradient features, and the other is a traditional neural network with texture bias. The structure of DuFeNet is easy to implement in the original neural network structure. The experimental results clearly show that DuFeNet can achieve better accuracy in image classification and detection. It can increase the shape bias of the network adapted to human visual perception. Besides, DuFeNet can be used without modifying the structure of the original network at lower additional parameters cost.},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/D2Y5RLEB/Ye et al. - 2022 - DuFeNet Improve the Accuracy and Increase Shape B.pdf}
}

@online{yehFidelitySensitivityExplanations2019,
  title = {On the ({{In}})Fidelity and {{Sensitivity}} for {{Explanations}}},
  author = {Yeh, Chih-Kuan and Hsieh, Cheng-Yu and Suggala, Arun Sai and Inouye, David I. and Ravikumar, Pradeep},
  date = {2019-11-03},
  eprint = {1901.09392},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1901.09392},
  url = {http://arxiv.org/abs/1901.09392},
  urldate = {2022-11-28},
  abstract = {We consider objective evaluation measures of saliency explanations for complex black-box machine learning models. We propose simple robust variants of two notions that have been considered in recent literature: (in)fidelity, and sensitivity. We analyze optimal explanations with respect to both these measures, and while the optimal explanation for sensitivity is a vacuous constant explanation, the optimal explanation for infidelity is a novel combination of two popular explanation methods. By varying the perturbation distribution that defines infidelity, we obtain novel explanations by optimizing infidelity, which we show to out-perform existing explanations in both quantitative and qualitative measurements. Another salient question given these measures is how to modify any given explanation to have better values with respect to these measures. We propose a simple modification based on lowering sensitivity, and moreover show that when done appropriately, we could simultaneously improve both sensitivity as well as fidelity.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/eragon/Zotero/storage/PEVB52S7/Yeh et al. - 2019 - On the (In)fidelity and Sensitivity for Explanatio.pdf}
}

@article{yeomPruningExplainingNovel2021,
  title = {Pruning by Explaining: {{A}} Novel Criterion for Deep Neural Network Pruning},
  shorttitle = {Pruning by Explaining},
  author = {Yeom, Seul-Ki and Seegerer, Philipp and Lapuschkin, Sebastian and Binder, Alexander and Wiedemann, Simon and Müller, Klaus-Robert and Samek, Wojciech},
  date = {2021-07-01},
  journaltitle = {Pattern Recognition},
  shortjournal = {Pattern Recognition},
  volume = {115},
  pages = {107899},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2021.107899},
  url = {https://www.sciencedirect.com/science/article/pii/S0031320321000868},
  urldate = {2023-02-16},
  abstract = {The success of convolutional neural networks (CNNs) in various applications is accompanied by a significant increase in computation and parameter storage costs. Recent efforts to reduce these overheads involve pruning and compressing the weights of various layers while at the same time aiming to not sacrifice performance. In this paper, we propose a novel criterion for CNN pruning inspired by neural network interpretability: The most relevant units, i.e. weights or filters, are automatically found using their relevance scores obtained from concepts of explainable AI (XAI). By exploring this idea, we connect the lines of interpretability and model compression research. We show that our proposed method can efficiently prune CNN models in transfer-learning setups in which networks pre-trained on large corpora are adapted to specialized tasks. The method is evaluated on a broad range of computer vision datasets. Notably, our novel criterion is not only competitive or better compared to state-of-the-art pruning criteria when successive retraining is performed, but clearly outperforms these previous criteria in the resource-constrained application scenario in which the data of the task to be transferred to is very scarce and one chooses to refrain from fine-tuning. Our method is able to compress the model iteratively while maintaining or even improving accuracy. At the same time, it has a computational cost in the order of gradient computation and is comparatively simple to apply without the need for tuning hyperparameters for pruning.},
  langid = {english},
  keywords = {Convolutional neural network (CNN),Explainable AI (XAI),Interpretation of models,Layer-wise relevance propagation (LRP),Pruning},
  file = {/Users/eragon/Zotero/storage/D6WAB2L3/Yeom et al. - 2021 - Pruning by explaining A novel criterion for deep .pdf}
}

@online{yuBuildingEthicsArtificial2018,
  title = {Building {{Ethics}} into {{Artificial Intelligence}}},
  author = {Yu, Han and Shen, Zhiqi and Miao, Chunyan and Leung, Cyril and Lesser, Victor R. and Yang, Qiang},
  date = {2018-12-07},
  eprint = {1812.02953},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1812.02953},
  url = {http://arxiv.org/abs/1812.02953},
  urldate = {2023-01-17},
  abstract = {As artificial intelligence (AI) systems become increasingly ubiquitous, the topic of AI governance for ethical decision-making by AI has captured public imagination. Within the AI research community, this topic remains less familiar to many researchers. In this paper, we complement existing surveys, which largely focused on the psychological, social and legal discussions of the topic, with an analysis of recent advances in technical solutions for AI governance. By reviewing publications in leading AI conferences including AAAI, AAMAS, ECAI and IJCAI, we propose a taxonomy which divides the field into four areas: 1) exploring ethical dilemmas; 2) individual ethical decision frameworks; 3) collective ethical decision frameworks; and 4) ethics in human-AI interactions. We highlight the intuitions and key techniques used in each approach, and discuss promising future research directions towards successful integration of ethical AI systems into human societies.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/eragon/Zotero/storage/XXVPFSPV/Yu et al. - 2018 - Building Ethics into Artificial Intelligence.pdf}
}

@inproceedings{yunCutMixRegularizationStrategy2019,
  title = {{{CutMix}}: {{Regularization Strategy}} to {{Train Strong Classifiers With Localizable Features}}},
  shorttitle = {{{CutMix}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Yun, Sangdoo and Han, Dongyoon and Chun, Sanghyuk and Oh, Seong Joon and Yoo, Youngjoon and Choe, Junsuk},
  date = {2019-10},
  pages = {6022--6031},
  publisher = {{IEEE}},
  location = {{Seoul, Korea (South)}},
  doi = {10.1109/ICCV.2019.00612},
  url = {https://ieeexplore.ieee.org/document/9008296/},
  urldate = {2023-02-20},
  abstract = {Regional dropout strategies have been proposed to enhance the performance of convolutional neural network classifiers. They have proved to be effective for guiding the model to attend on less discriminative parts of objects (e.g. leg as opposed to head of a person), thereby letting the network generalize better and have better object localization capabilities. On the other hand, current methods for regional dropout remove informative pixels on training images by overlaying a patch of either black pixels or random noise. Such removal is not desirable because it leads to information loss and inefficiency during training. We therefore propose the CutMix augmentation strategy: patches are cut and pasted among training images where the ground truth labels are also mixed proportionally to the area of the patches. By making efficient use of training pixels and retaining the regularization effect of regional dropout, CutMix consistently outperforms the state-of-the-art augmentation strategies on CIFAR and ImageNet classification tasks, as well as on the ImageNet weakly-supervised localization task. Moreover, unlike previous augmentation methods, our CutMix-trained ImageNet classifier, when used as a pretrained model, results in consistent performance gains in Pascal detection and MS-COCO image captioning benchmarks. We also show that CutMix improves the model robustness against input corruptions and its out-of-distribution detection performances. Source code and pretrained models are available at https://github.com/clovaai/CutMix-PyTorch.},
  eventtitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-72814-803-8},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/RI2AY5CU/Yun et al. - 2019 - CutMix Regularization Strategy to Train Strong Cl.pdf}
}

@online{yunCutMixRegularizationStrategy2019a,
  title = {{{CutMix}}: {{Regularization Strategy}} to {{Train Strong Classifiers}} with {{Localizable Features}}},
  shorttitle = {{{CutMix}}},
  author = {Yun, Sangdoo and Han, Dongyoon and Oh, Seong Joon and Chun, Sanghyuk and Choe, Junsuk and Yoo, Youngjoon},
  date = {2019-08-07},
  eprint = {1905.04899},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1905.04899},
  url = {http://arxiv.org/abs/1905.04899},
  urldate = {2023-03-27},
  abstract = {Regional dropout strategies have been proposed to enhance the performance of convolutional neural network classifiers. They have proved to be effective for guiding the model to attend on less discriminative parts of objects (e.g. leg as opposed to head of a person), thereby letting the network generalize better and have better object localization capabilities. On the other hand, current methods for regional dropout remove informative pixels on training images by overlaying a patch of either black pixels or random noise. Such removal is not desirable because it leads to information loss and inefficiency during training. We therefore propose the CutMix augmentation strategy: patches are cut and pasted among training images where the ground truth labels are also mixed proportionally to the area of the patches. By making efficient use of training pixels and retaining the regularization effect of regional dropout, CutMix consistently outperforms the state-of-the-art augmentation strategies on CIFAR and ImageNet classification tasks, as well as on the ImageNet weakly-supervised localization task. Moreover, unlike previous augmentation methods, our CutMix-trained ImageNet classifier, when used as a pretrained model, results in consistent performance gains in Pascal detection and MS-COCO image captioning benchmarks. We also show that CutMix improves the model robustness against input corruptions and its out-of-distribution detection performances. Source code and pretrained models are available at https://github.com/clovaai/CutMix-PyTorch .},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/eragon/Zotero/storage/PM6KETLB/Yun et al. - 2019 - CutMix Regularization Strategy to Train Strong Cl.pdf}
}

@inproceedings{zeilerAdaptiveDeconvolutionalNetworks2011,
  title = {Adaptive Deconvolutional Networks for Mid and High Level Feature Learning},
  booktitle = {2011 {{International Conference}} on {{Computer Vision}}},
  author = {Zeiler, Matthew D. and Taylor, Graham W. and Fergus, Rob},
  date = {2011-11},
  pages = {2018--2025},
  publisher = {{IEEE}},
  location = {{Barcelona, Spain}},
  doi = {10.1109/ICCV.2011.6126474},
  url = {http://ieeexplore.ieee.org/document/6126474/},
  urldate = {2023-02-20},
  abstract = {We present a hierarchical model that learns image decompositions via alternating layers of convolutional sparse coding and max pooling. When trained on natural images, the layers of our model capture image information in a variety of forms: low-level edges, mid-level edge junctions, high-level object parts and complete objects. To build our model we rely on a novel inference scheme that ensures each layer reconstructs the input, rather than just the output of the layer directly beneath, as is common with existing hierarchical approaches. This makes it possible to learn multiple layers of representation and we show models with 4 layers, trained on images from the Caltech-101 and 256 datasets. When combined with a standard classifier, features extracted from these models outperform SIFT, as well as representations from other feature learning methods.},
  eventtitle = {2011 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-4577-1102-2 978-1-4577-1101-5 978-1-4577-1100-8},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/JDIY4ZZB/Zeiler et al. - 2011 - Adaptive deconvolutional networks for mid and high.pdf}
}

@online{zeilerVisualizingUnderstandingConvolutional2013,
  title = {Visualizing and {{Understanding Convolutional Networks}}},
  author = {Zeiler, Matthew D. and Fergus, Rob},
  date = {2013-11-28},
  eprint = {1311.2901},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1311.2901},
  url = {http://arxiv.org/abs/1311.2901},
  urldate = {2022-11-28},
  abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky \textbackslash etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/eragon/Zotero/storage/G7Z6NFNF/Zeiler and Fergus - 2013 - Visualizing and Understanding Convolutional Networ.pdf;/Users/eragon/Zotero/storage/TCYYQIJB/Zeiler and Fergus - 2013 - Visualizing and Understanding Convolutional Networ.pdf}
}

@online{zeilerVisualizingUnderstandingConvolutional2013a,
  title = {Visualizing and {{Understanding Convolutional Networks}}},
  author = {Zeiler, Matthew D. and Fergus, Rob},
  date = {2013-11-28},
  eprint = {1311.2901},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1311.2901},
  url = {http://arxiv.org/abs/1311.2901},
  urldate = {2022-12-07},
  abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky \textbackslash etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/eragon/Zotero/storage/IWCBV29B/Zeiler and Fergus - 2013 - Visualizing and Understanding Convolutional Networ.pdf}
}

@inproceedings{zhangDebiasedCAMMitigateImage2022,
  title = {Debiased-{{CAM}} to Mitigate Image Perturbations with Faithful Visual Explanations of Machine Learning},
  booktitle = {{{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Zhang, Wencan and Dimiccoli, Mariella and Lim, Brian Y},
  date = {2022-04-29},
  pages = {1--32},
  publisher = {{ACM}},
  location = {{New Orleans LA USA}},
  doi = {10.1145/3491102.3517522},
  url = {https://dl.acm.org/doi/10.1145/3491102.3517522},
  urldate = {2023-02-13},
  abstract = {Model explanations such as saliency maps can improve user trust in AI by highlighting important features for a prediction. However, these become distorted and misleading when explaining predictions of images that are subject to systematic error (bias) by perturbations and corruptions. Furthermore, the distortions persist despite model fine-tuning on images biased by different factors (blur, color temperature, day/night). We present Debiased-CAM to recover explanation faithfulness across various bias types and levels by training a multi-input, multi-task model with auxiliary tasks for explanation and bias level predictions. In simulation studies, the approach not only enhanced prediction accuracy, but also generated highly faithful explanations about these predictions as if the images were unbiased. In user studies, debiased explanations improved user task performance, perceived truthfulness and perceived helpfulness. Debiased training can provide a versatile platform for robust performance and explanation faithfulness for a wide range of applications with data biases.},
  eventtitle = {{{CHI}} '22: {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  isbn = {978-1-4503-9157-3},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/UMCH2X2D/Zhang et al. - 2022 - Debiased-CAM to mitigate image perturbations with .pdf}
}

@article{zhangExaminingCNNRepresentations2018,
  title = {Examining {{CNN Representations With Respect}} to {{Dataset Bias}}},
  author = {Zhang, Quanshi and Wang, Wenguan and Zhu, Song-Chun},
  date = {2018-04-29},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {32},
  number = {1},
  issn = {2374-3468},
  doi = {10.1609/aaai.v32i1.11833},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/11833},
  urldate = {2022-09-06},
  abstract = {Given a pre-trained CNN without any testing samples, this paper proposes a simple yet effective method to diagnose feature representations of the CNN. We aim to discover representation flaws caused by potential dataset bias. More specifically, when the CNN is trained to estimate image attributes, we mine latent relationships between representations of different attributes inside the CNN. Then, we compare the mined attribute relationships with ground-truth attribute relationships to discover the CNN's blind spots and failure modes due to dataset bias. In fact, representation flaws caused by dataset bias cannot be examined by conventional evaluation strategies based on testing images, because testing images may also have a similar bias. Experiments have demonstrated the effectiveness of our method.},
  issue = {1},
  langid = {english},
  keywords = {Knowledge representation},
  file = {/Users/eragon/Zotero/storage/6GCLCZLB/Zhang et al. - 2018 - Examining CNN Representations With Respect to Data.pdf}
}

@inproceedings{zhangHowDoesMixup2021,
  title = {How {{Does Mixup Help With Robustness}} and {{Generalization}}?},
  author = {Zhang, Linjun and Deng, Zhun and Kawaguchi, Kenji and Ghorbani, Amirata and Zou, James},
  date = {2021-03-17},
  url = {https://openreview.net/forum?id=8yKEo06dKNo},
  urldate = {2022-11-20},
  abstract = {Mixup is a popular data augmentation technique based on on convex combinations of pairs of examples and their labels. This simple technique has shown to substantially improve both the model's robustness as well as the generalization of the trained model. However, it is not well-understood why such improvement occurs. In this paper, we provide theoretical analysis to demonstrate how using Mixup in training helps model robustness and generalization. For robustness, we show that minimizing the Mixup loss corresponds to approximately minimizing an upper bound of the adversarial loss. This explains why models obtained by Mixup training exhibits robustness to several kinds of adversarial attacks such as Fast Gradient Sign Method (FGSM). For generalization, we prove that Mixup augmentation corresponds to a specific type of data-adaptive regularization which reduces overfitting. Our analysis provides new insights and a framework to understand Mixup.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/67D6JV6N/Zhang et al. - 2021 - How Does Mixup Help With Robustness and Generaliza.pdf}
}

@article{zhangInductiveMultiHypergraphLearning2018,
  title = {Inductive {{Multi-Hypergraph Learning}} and {{Its Application}} on {{View-Based 3D Object Classification}}},
  author = {Zhang, Zizhao and Lin, Haojie and Zhao, Xibin and Ji, Rongrong and Gao, Yue},
  date = {2018-12},
  journaltitle = {IEEE Transactions on Image Processing},
  shortjournal = {IEEE Trans. on Image Process.},
  volume = {27},
  number = {12},
  pages = {5957--5968},
  issn = {1057-7149, 1941-0042},
  doi = {10.1109/TIP.2018.2862625},
  url = {https://ieeexplore.ieee.org/document/8424480/},
  urldate = {2022-05-10},
  abstract = {The wide 3D applications have led to increasing amount of 3D object data, and thus effective 3D object classification technique has become urgent requirement. One important and challenging task for 3D object classification is how to formulate the 3D data correlation and exploit it. Most of previous works focus on learning optimal pairwise distance metric for object comparison, which may lose the global correlation among 3D objects. Recently, transductive hypergraph learning has been investigated for classification, which can jointly explore the correlation among multiple objects, including both the labeled and unlabeled data. Although these methods have shown better performance, they are still limited due to 1) a considerable amount of testing data may not be available in practice and 2) the high computational cost to test new coming data. To handle this problem, considering the multimodal representations of 3D objects in practice, we propose an inductive multi-hypergraph learning algorithm, which targets on learning an optimal projection for the multi-modal training data. In this method, all the training data are formulated in multihypergraph based on the features, and the inductive learning is conducted to learn the projection matrices and the optimal multi-hypergraph combination weights simultaneously. Different from the transductive learning on hypergraph, the high cost training process is off-line, and the testing process is very efficient for the inductive learning on hypergraph. We have conducted experiments on two 3D benchmarks, i.e., the NTU and the ModelNet40 datasets, and compared the proposed algorithm with the state-of-the-art methods and traditional transductive multi-hypergraph learning methods. Experimental results have demonstrated that the proposed method can achieve effective and efficient classification performance. We also note that the proposed method is a general framework and has the potential to be applied in other applications in practice.},
  langid = {english},
  file = {/home/erago/Zotero/storage/T8PNMNJI/inductive-multihypergraph-learning-and-its-application-on-viewba-2018.pdf}
}

@article{zhangInductiveMultihypergraphLearning2018,
  title = {Inductive Multi-Hypergraph Learning and Its Application on View-Based {{3D}} Object Classification},
  author = {Zhang, Zizhao and Lin, Haojie and Zhao, Xibin and Ji, Rongrong and Gao, Yue},
  date = {2018},
  journaltitle = {IEEE Transactions on Image Processing},
  volume = {27},
  number = {12},
  pages = {5957--5968},
  publisher = {{IEEE}}
}

@inproceedings{zhangIntraClassPartSwapping2021,
  title = {Intra-{{Class Part Swapping}} for {{Fine-Grained Image Classification}}},
  author = {Zhang, Lianbo and Huang, Shaoli and Liu, Wei},
  date = {2021},
  pages = {3209--3218},
  url = {https://openaccess.thecvf.com/content/WACV2021/html/Zhang_Intra-Class_Part_Swapping_for_Fine-Grained_Image_Classification_WACV_2021_paper.html},
  urldate = {2023-03-30},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Winter Conference}} on {{Applications}} of {{Computer Vision}}},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/HQTIPLU4/Zhang et al. - 2021 - Intra-Class Part Swapping for Fine-Grained Image C.pdf}
}

@inproceedings{zhangJointObjectPose2013,
  title = {Joint Object and Pose Recognition Using Homeomorphic Manifold Analysis},
  booktitle = {Proceedings of the {{AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Zhang, Haopeng and El-Gaaly, Tarek and Elgammal, Ahmed and Jiang, Zhiguo},
  date = {2013}
}

@unpublished{zhangLinkedDynamicGraph2019,
  title = {Linked {{Dynamic Graph CNN}}: {{Learning}} on {{Point Cloud}} via {{Linking Hierarchical Features}}},
  shorttitle = {Linked {{Dynamic Graph CNN}}},
  author = {Zhang, Kuangen and Hao, Ming and Wang, Jing and family=Silva, given=Clarence W., prefix=de, useprefix=true and Fu, Chenglong},
  date = {2019-08-05},
  eprint = {1904.10014},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/1904.10014},
  urldate = {2022-05-10},
  abstract = {Learning on point cloud is eagerly in demand because the point cloud is a common type of geometric data and can aid robots to understand environments robustly. However, the point cloud is sparse, unstructured, and unordered, which cannot be recognized accurately by a traditional convolutional neural network (CNN) nor a recurrent neural network (RNN). Fortunately, a graph convolutional neural network (Graph CNN) can process sparse and unordered data. Hence, we propose a linked dynamic graph CNN (LDGCNN) to classify and segment point cloud directly in this paper. We remove the transformation network, link hierarchical features from dynamic graphs, freeze feature extractor, and retrain the classifier to increase the performance of LDGCNN. We explain our network using theoretical analysis and visualization. Through experiments, we show that the proposed LDGCNN achieves state-of-art performance on two standard datasets: ModelNet40 and ShapeNet.},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/eragon/Zotero/storage/VE87R4GD/Zhang et al. - 2019 - Linked Dynamic Graph CNN Learning on Point Cloud .pdf;/Users/eragon/Zotero/storage/XBNQ44BK/1904.html}
}

@article{zhangMentalizingInformationPropagation2016,
  title = {Mentalizing and {{Information Propagation}} through {{Social Network}}: {{Evidence}} from a {{Resting-State-fMRI Study}}},
  shorttitle = {Mentalizing and {{Information Propagation}} through {{Social Network}}},
  author = {Zhang, Huijun and Mo, Lei},
  date = {2016},
  journaltitle = {Frontiers in Psychology},
  volume = {7},
  issn = {1664-1078},
  url = {https://www.frontiersin.org/article/10.3389/fpsyg.2016.01716},
  urldate = {2022-06-15},
  abstract = {Microblogs is one of the main social networking channels by which information is spread. Among them, Sina Weibo is one of the largest social networking channels in China. Millions of users repost information from Sina Weibo and share embedded emotion at the same time. The present study investigated participants’ propensity to repost microblog messages of positive, negative, or neutral valence, and studied the neural correlates during resting state with the reposting rate of each type microblog messages. Participants preferred to repost negative messages relative to positive and neutral messages. Reposting rate of negative messages was positively correlated to the functional connectivity of temporoparietal junction (TPJ) with insula, and TPJ with dorsolateral prefrontal cortex. These results indicate that reposting negative messages is related to conflict resolution between the feeling of pain/disgust and the intention to repost significant information. Thus, resposting emotional microblog messages might be attributed to participants’ appraisal of personal and recipient’s interest, as well as their cognitive process for decision making.},
  file = {/Users/eragon/Zotero/storage/6FISZQMC/Zhang and Mo - 2016 - Mentalizing and Information Propagation through So.pdf}
}

@online{zhangMixupEmpiricalRisk2018,
  title = {Mixup: {{Beyond Empirical Risk Minimization}}},
  shorttitle = {Mixup},
  author = {Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N. and Lopez-Paz, David},
  date = {2018-04-27},
  eprint = {1710.09412},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1710.09412},
  url = {http://arxiv.org/abs/1710.09412},
  urldate = {2023-03-27},
  abstract = {Large deep neural networks are powerful, but exhibit undesirable behaviors such as memorization and sensitivity to adversarial examples. In this work, we propose mixup, a simple learning principle to alleviate these issues. In essence, mixup trains a neural network on convex combinations of pairs of examples and their labels. By doing so, mixup regularizes the neural network to favor simple linear behavior in-between training examples. Our experiments on the ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show that mixup improves the generalization of state-of-the-art neural network architectures. We also find that mixup reduces the memorization of corrupt labels, increases the robustness to adversarial examples, and stabilizes the training of generative adversarial networks.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/eragon/Zotero/storage/K2UFNUHC/Zhang et al. - 2018 - mixup Beyond Empirical Risk Minimization.pdf}
}

@article{zhangSurveyNeuralNetwork2021,
  title = {A {{Survey}} on {{Neural Network Interpretability}}},
  author = {Zhang, Yu and Tiňo, Peter and Leonardis, Aleš and Tang, Ke},
  date = {2021-10},
  journaltitle = {IEEE Transactions on Emerging Topics in Computational Intelligence},
  shortjournal = {IEEE Trans. Emerg. Top. Comput. Intell.},
  volume = {5},
  number = {5},
  eprint = {2012.14261},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {726--742},
  issn = {2471-285X},
  doi = {10.1109/TETCI.2021.3100641},
  url = {http://arxiv.org/abs/2012.14261},
  urldate = {2022-12-07},
  abstract = {Along with the great success of deep neural networks, there is also growing concern about their black-box nature. The interpretability issue affects people's trust on deep learning systems. It is also related to many ethical problems, e.g., algorithmic discrimination. Moreover, interpretability is a desired property for deep networks to become powerful tools in other research fields, e.g., drug discovery and genomics. In this survey, we conduct a comprehensive review of the neural network interpretability research. We first clarify the definition of interpretability as it has been used in many different contexts. Then we elaborate on the importance of interpretability and propose a novel taxonomy organized along three dimensions: type of engagement (passive vs. active interpretation approaches), the type of explanation, and the focus (from local to global interpretability). This taxonomy provides a meaningful 3D view of distribution of papers from the relevant literature as two of the dimensions are not simply categorical but allow ordinal subcategories. Finally, we summarize the existing interpretability evaluation methods and suggest possible research directions inspired by our new taxonomy.},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/eragon/Zotero/storage/AF42PZ4M/Zhang et al. - 2021 - A Survey on Neural Network Interpretability.pdf}
}

@inproceedings{zhaoExploitingExplanationsModel2021,
  title = {Exploiting {{Explanations}} for {{Model Inversion Attacks}}},
  booktitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Zhao, Xuejun and Zhang, Wencan and Xiao, Xiaokui and Lim, Brian},
  date = {2021-10},
  pages = {662--672},
  publisher = {{IEEE}},
  location = {{Montreal, QC, Canada}},
  doi = {10.1109/ICCV48922.2021.00072},
  url = {https://ieeexplore.ieee.org/document/9709977/},
  urldate = {2023-02-13},
  abstract = {The successful deployment of artificial intelligence (AI) in many domains from healthcare to hiring requires their responsible use, particularly in model explanations and privacy. Explainable artificial intelligence (XAI) provides more information to help users to understand model decisions, yet this additional knowledge exposes additional risks for privacy attacks. Hence, providing explanation harms privacy. We study this risk for image-based model inversion attacks and identified several attack architectures with increasing performance to reconstruct private image data from model explanations. We have developed several multi-modal transposed CNN architectures that achieve significantly higher inversion performance than using the target model prediction only. These XAI-aware inversion models were designed to exploit the spatial knowledge in image explanations. To understand which explanations have higher privacy risk, we analyzed how various explanation types and factors influence inversion performance. In spite of some models not providing explanations, we further demonstrate increased inversion performance even for nonexplainable target models by exploiting explanations of surrogate models through attention transfer. This method first inverts an explanation from the target prediction, then reconstructs the target image. These threats highlight the urgent and significant privacy risks of explanations and calls attention for new privacy preservation techniques that balance the dual-requirement for AI explainability and privacy.},
  eventtitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-66542-812-5},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/E9AVXLRH/Zhao et al. - 2021 - Exploiting Explanations for Model Inversion Attack.pdf}
}

@article{zhaoM2DetSingleShotObject2019,
  title = {{{M2Det}}: {{A Single-Shot Object Detector Based}} on {{Multi-Level Feature Pyramid Network}}},
  shorttitle = {{{M2Det}}},
  author = {Zhao, Qijie and Sheng, Tao and Wang, Yongtao and Tang, Zhi and Chen, Ying and Cai, Ling and Ling, Haibin},
  date = {2019-07-17},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {33},
  number = {01},
  pages = {9259--9266},
  issn = {2374-3468},
  doi = {10.1609/aaai.v33i01.33019259},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/4962},
  urldate = {2023-05-08},
  abstract = {Feature pyramids are widely exploited by both the state-of-the-art one-stage object detectors (e.g., DSSD, RetinaNet, RefineDet) and the two-stage object detectors (e.g., Mask RCNN, DetNet) to alleviate the problem arising from scale variation across object instances. Although these object detectors with feature pyramids achieve encouraging results, they have some limitations due to that they only simply construct the feature pyramid according to the inherent multiscale, pyramidal architecture of the backbones which are originally designed for object classification task. Newly, in this work, we present Multi-Level Feature Pyramid Network (MLFPN) to construct more effective feature pyramids for detecting objects of different scales. First, we fuse multi-level features (i.e. multiple layers) extracted by backbone as the base feature. Second, we feed the base feature into a block of alternating joint Thinned U-shape Modules and Feature Fusion Modules and exploit the decoder layers of each Ushape module as the features for detecting objects. Finally, we gather up the decoder layers with equivalent scales (sizes) to construct a feature pyramid for object detection, in which every feature map consists of the layers (features) from multiple levels. To evaluate the effectiveness of the proposed MLFPN, we design and train a powerful end-to-end one-stage object detector we call M2Det by integrating it into the architecture of SSD, and achieve better detection performance than state-of-the-art one-stage detectors. Specifically, on MSCOCO benchmark, M2Det achieves AP of 41.0 at speed of 11.8 FPS with single-scale inference strategy and AP of 44.2 with multi-scale inference strategy, which are the new stateof-the-art results among one-stage detectors. The code will be made available on https://github.com/qijiezhao/M2Det.},
  issue = {01},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/MD5X57ZP/Zhao et al. - 2019 - M2Det A Single-Shot Object Detector Based on Mult.pdf}
}

@inproceedings{zhaoUnderstandingEvaluatingRacial2021,
  title = {Understanding and {{Evaluating Racial Biases}} in {{Image Captioning}}},
  author = {Zhao, Dora and Wang, Angelina and Russakovsky, Olga},
  date = {2021},
  pages = {14830--14840},
  url = {https://openaccess.thecvf.com/content/ICCV2021/html/Zhao_Understanding_and_Evaluating_Racial_Biases_in_Image_Captioning_ICCV_2021_paper.html},
  urldate = {2022-09-06},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/8EQS5EK2/Zhao et al. - 2021 - Understanding and Evaluating Racial Biases in Imag.pdf}
}

@article{zhongRandomErasingData2020,
  title = {Random {{Erasing Data Augmentation}}},
  author = {Zhong, Zhun and Zheng, Liang and Kang, Guoliang and Li, Shaozi and Yang, Yi},
  date = {2020-04-03},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI},
  volume = {34},
  number = {07},
  pages = {13001--13008},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v34i07.7000},
  url = {https://aaai.org/ojs/index.php/AAAI/article/view/7000},
  urldate = {2022-10-21},
  abstract = {In this paper, we introduce Random Erasing, a new data augmentation method for training the convolutional neural network (CNN). In training, Random Erasing randomly selects a rectangle region in an image and erases its pixels with random values. In this process, training images with various levels of occlusion are generated, which reduces the risk of over-fitting and makes the model robust to occlusion. Random Erasing is parameter learning free, easy to implement, and can be integrated with most of the CNN-based recognition models. Albeit simple, Random Erasing is complementary to commonly used data augmentation techniques such as random cropping and flipping, and yields consistent improvement over strong baselines in image classification, object detection and person re-identification. Code is available at: https://github.com/zhunzhong07/Random-Erasing.},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/H2HNNUFX/Zhong et al. - 2020 - Random Erasing Data Augmentation.pdf}
}

@article{zhongRandomErasingData2020a,
  title = {Random {{Erasing Data Augmentation}}},
  author = {Zhong, Zhun and Zheng, Liang and Kang, Guoliang and Li, Shaozi and Yang, Yi},
  date = {2020-04-03},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {07},
  pages = {13001--13008},
  issn = {2374-3468},
  doi = {10.1609/aaai.v34i07.7000},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/7000},
  urldate = {2023-03-29},
  abstract = {In this paper, we introduce Random Erasing, a new data augmentation method for training the convolutional neural network (CNN). In training, Random Erasing randomly selects a rectangle region in an image and erases its pixels with random values. In this process, training images with various levels of occlusion are generated, which reduces the risk of over-fitting and makes the model robust to occlusion. Random Erasing is parameter learning free, easy to implement, and can be integrated with most of the CNN-based recognition models. Albeit simple, Random Erasing is complementary to commonly used data augmentation techniques such as random cropping and flipping, and yields consistent improvement over strong baselines in image classification, object detection and person re-identification. Code is available at: https://github.com/zhunzhong07/Random-Erasing.},
  issue = {07},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/ICST65SL/Zhong et al. - 2020 - Random Erasing Data Augmentation.pdf}
}

@online{zhouConvNetsVsTransformers2021,
  title = {{{ConvNets}} vs. {{Transformers}}: {{Whose Visual Representations}} Are {{More Transferable}}?},
  shorttitle = {{{ConvNets}} vs. {{Transformers}}},
  author = {Zhou, Hong-Yu and Lu, Chixiang and Yang, Sibei and Yu, Yizhou},
  date = {2021-08-17},
  eprint = {2108.05305},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2108.05305},
  url = {http://arxiv.org/abs/2108.05305},
  urldate = {2022-10-23},
  abstract = {Vision transformers have attracted much attention from computer vision researchers as they are not restricted to the spatial inductive bias of ConvNets. However, although Transformer-based backbones have achieved much progress on ImageNet classification, it is still unclear whether the learned representations are as transferable as or even more transferable than ConvNets' features. To address this point, we systematically investigate the transfer learning ability of ConvNets and vision transformers in 15 single-task and multi-task performance evaluations. Given the strong correlation between the performance of pre-trained models and transfer learning, we include 2 residual ConvNets (i.e., R-101x3 and R-152x4) and 3 Transformer-based visual backbones (i.e., ViT-B, ViT-L and Swin-B), which have close error rates on ImageNet, that indicate similar transfer learning performance on downstream datasets. We observe consistent advantages of Transformer-based backbones on 13 downstream tasks (out of 15), including but not limited to fine-grained classification, scene recognition (classification, segmentation and depth estimation), open-domain classification, face recognition, etc. More specifically, we find that two ViT models heavily rely on whole network fine-tuning to achieve performance gains while Swin Transformer does not have such a requirement. Moreover, vision transformers behave more robustly in multi-task learning, i.e., bringing more improvements when managing mutually beneficial tasks and reducing performance losses when tackling irrelevant tasks. We hope our discoveries can facilitate the exploration and exploitation of vision transformers in the future.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/eragon/Zotero/storage/RNH3MUUD/Zhou et al. - 2021 - ConvNets vs. Transformers Whose Visual Representa.pdf}
}

@inproceedings{zhouLearningDeepFeatures2016,
  title = {Learning {{Deep Features}} for {{Discriminative Localization}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
  date = {2016-06},
  pages = {2921--2929},
  publisher = {{IEEE}},
  location = {{Las Vegas, NV, USA}},
  doi = {10.1109/CVPR.2016.319},
  url = {http://ieeexplore.ieee.org/document/7780688/},
  urldate = {2023-02-20},
  abstract = {In this work, we revisit the global average pooling layer proposed in [13], and shed light on how it explicitly enables the convolutional neural network (CNN) to have remarkable localization ability despite being trained on imagelevel labels. While this technique was previously proposed as a means for regularizing training, we find that it actually builds a generic localizable deep representation that exposes the implicit attention of CNNs on an image. Despite the apparent simplicity of global average pooling, we are able to achieve 37.1\% top-5 error for object localization on ILSVRC 2014 without training on any bounding box annotation.We demonstrate in a variety of experiments that our network is able to localize the discriminative image regions despite just being trained for solving classification task1.},
  eventtitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  isbn = {978-1-4673-8851-1},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/D46H4UWB/Zhou et al. - 2016 - Learning Deep Features for Discriminative Localiza.pdf}
}

@unpublished{zhouOpen3DModernLibrary2018,
  title = {{{Open3D}}: {{A Modern Library}} for {{3D Data Processing}}},
  author = {Zhou, Qian-Yi and Park, Jaesik and Koltun, Vladlen},
  date = {2018},
  eprint = {1801.09847},
  eprinttype = {arxiv}
}

@article{zhouPlaces10Million2018,
  title = {Places: {{A}} 10 {{Million Image Database}} for {{Scene Recognition}}},
  shorttitle = {Places},
  author = {Zhou, Bolei and Lapedriza, Agata and Khosla, Aditya and Oliva, Aude and Torralba, Antonio},
  date = {2018-06-01},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  volume = {40},
  number = {6},
  pages = {1452--1464},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2017.2723009},
  url = {https://ieeexplore.ieee.org/document/7968387/},
  urldate = {2023-05-21},
  abstract = {The rise of multi-million-item dataset initiatives has enabled data-hungry machine learning algorithms to reach nearhuman semantic classification performance at tasks such as visual object and scene recognition. Here we describe the Places Database, a repository of 10 million scene photographs, labeled with scene semantic categories, comprising a large and diverse list of the types of environments encountered in the world. Using the state-of-the-art Convolutional Neural Networks (CNNs), we provide scene classification CNNs (Places-CNNs) as baselines, that significantly outperform the previous approaches. Visualization of the CNNs trained on Places shows that object detectors emerge as an intermediate representation of scene classification. With its high-coverage and high-diversity of exemplars, the Places Database along with the Places-CNNs offer a novel resource to guide future progress on scene recognition problems.},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/A93KGJUX/Zhou et al. - 2018 - Places A 10 Million Image Database for Scene Reco.pdf}
}

@inproceedings{zhouSurveyEthicalPrinciples2020,
  title = {A {{Survey}} on {{Ethical Principles}} of {{AI}} and {{Implementations}}},
  booktitle = {2020 {{IEEE Symposium Series}} on {{Computational Intelligence}} ({{SSCI}})},
  author = {Zhou, Jianlong and Chen, Fang and Berry, Adam and Reed, Mike and Zhang, Shujia and Savage, Siobhan},
  date = {2020-12-01},
  pages = {3010--3017},
  publisher = {{IEEE}},
  location = {{Canberra, ACT, Australia}},
  doi = {10.1109/SSCI47803.2020.9308437},
  url = {https://ieeexplore.ieee.org/document/9308437/},
  urldate = {2023-01-17},
  abstract = {AI has powerful capabilities in prediction, automation, planning, targeting, and personalisation. Generally, it is assumed that AI can enable machines to exhibit human-like intelligence, and is claimed to benefit to different areas of our lives. Since AI is fueled by data and is a distinct form of autonomous and self-learning agency, we are seeing increasing ethical concerns related to AI uses. In order to mitigate various ethical concerns, national and international organisations including governmental organisations, private sectors as well as research institutes have made extensive efforts by drafting ethical principles of AI, and having active discussions on ethics of AI within and beyond the AI community. This paper investigates these efforts with a focus on the identification of fundamental ethical principles of AI and their implementations. The review found that there is a convergence around limited principles and the most prevalent principles are transparency, justice and fairness, responsibility, non-maleficence, and privacy. The investigation suggests that ethical principles need to be combined with every stages of the AI lifecycle in the implementation to ensure that the AI system is designed, implemented and deployed in an ethical manner. Similar to ethical framework used in biomedical and clinical research, this paper suggests checklist-style questionnaires as benchmarks for the implementation of ethical principles of AI.},
  eventtitle = {2020 {{IEEE Symposium Series}} on {{Computational Intelligence}} ({{SSCI}})},
  isbn = {978-1-72812-547-3},
  langid = {english},
  file = {/Users/eragon/Zotero/storage/T2RRCCHF/Zhou et al. - 2020 - A Survey on Ethical Principles of AI and Implement.pdf}
}

@online{zhuangComprehensiveSurveyTransfer2020,
  title = {A {{Comprehensive Survey}} on {{Transfer Learning}}},
  author = {Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
  date = {2020-06-23},
  eprint = {1911.02685},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1911.02685},
  urldate = {2023-02-27},
  abstract = {Transfer learning aims at improving the performance of target learners on target domains by transferring the knowledge contained in different but related source domains. In this way, the dependence on a large number of target domain data can be reduced for constructing target learners. Due to the wide application prospects, transfer learning has become a popular and promising area in machine learning. Although there are already some valuable and impressive surveys on transfer learning, these surveys introduce approaches in a relatively isolated way and lack the recent advances in transfer learning. Due to the rapid expansion of the transfer learning area, it is both necessary and challenging to comprehensively review the relevant studies. This survey attempts to connect and systematize the existing transfer learning researches, as well as to summarize and interpret the mechanisms and the strategies of transfer learning in a comprehensive way, which may help readers have a better understanding of the current research status and ideas. Unlike previous surveys, this survey paper reviews more than forty representative transfer learning approaches, especially homogeneous transfer learning approaches, from the perspectives of data and model. The applications of transfer learning are also briefly introduced. In order to show the performance of different transfer learning models, over twenty representative transfer learning models are used for experiments. The models are performed on three different datasets, i.e., Amazon Reviews, Reuters-21578, and Office-31. And the experimental results demonstrate the importance of selecting appropriate transfer learning models for different applications in practice.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/eragon/Zotero/storage/348SQIR3/Zhuang et al. - 2020 - A Comprehensive Survey on Transfer Learning.pdf}
}

@unpublished{zophNeuralArchitectureSearch2016,
  title = {Neural Architecture Search with Reinforcement Learning},
  author = {Zoph, Barret and Le, Quoc V},
  date = {2016},
  eprint = {1611.01578},
  eprinttype = {arxiv}
}

@article{zouNewDatasetDog2020,
  title = {A New Dataset of Dog Breed Images and a Benchmark for Finegrained Classification},
  author = {Zou, Ding-Nan and Zhang, Song-Hai and Mu, Tai-Jiang and Zhang, Min},
  date = {2020-12-01},
  journaltitle = {Computational Visual Media},
  shortjournal = {Comp. Visual Media},
  volume = {6},
  number = {4},
  pages = {477--487},
  issn = {2096-0662},
  doi = {10.1007/s41095-020-0184-6},
  url = {https://doi.org/10.1007/s41095-020-0184-6},
  urldate = {2023-05-21},
  abstract = {In this paper, we introduce an image dataset for fine-grained classification of dog breeds: the Tsinghua Dogs Dataset. It is currently the largest dataset for fine-grained classification of dogs, including 130 dog breeds and 70,428 real-world images. It has only one dog in each image and provides annotated bounding boxes for the whole body and head. In comparison to previous similar datasets, it contains more breeds and more carefully chosen images for each breed. The diversity within each breed is greater, with between 200 and 7000+ images for each breed. Annotation of the whole body and head makes the dataset not only suitable for the improvement of finegrained image classification models based on overall features, but also for those locating local informative parts. We show that dataset provides a tough challenge by benchmarking several state-of-the-art deep neural models. The dataset is available for academic purposes at https://cg.cs.tsinghua.edu.cn/ThuDogs/.},
  langid = {english},
  keywords = {benchmark,dataset,dog,fine-grained classification},
  file = {/Users/eragon/Zotero/storage/RGULJY7P/Zou et al. - 2020 - A new dataset of dog breed images and a benchmark .pdf}
}

@article{ullman1988attention,
  title={Attention, Selective Visual},
  author={Ullman, Shimon and Koch, Christof},
  journal={Sensory System I: Vision and Visual Systems},
  volume={1},
  pages={3},
  year={1988},
  publisher={Springer}
}