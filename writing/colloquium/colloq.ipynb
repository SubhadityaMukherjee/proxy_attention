{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ae5124e-8a2c-4112-bed9-b6df06593e51",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5a179be-6d2b-47a3-9ed3-9cb4b365ef8f",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4f28b3e-654c-46b2-a279-66d3596b8c85",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "paste files  tsing_results.pdf places256_results.pdf \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<img src=\"./images/tsing_results.pdf\" alt= “” width=\"1000px\">\n",
      "\n",
      "<img src=\"./images/places256_results.pdf\" alt= “” width=\"1000px\">\n",
      "\n",
      "<img src=\"./images/\" alt= “” width=\"1000px\">\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_string = \"\"\n",
    "for img in input(\"paste files \").split(\" \"):\n",
    "    # if \"pdf\" in img:\n",
    "    #     final_string += f'IFrame(\"{img}\", width = \"1152px\", height = \"580px\")\\n\\n'\n",
    "    # else:\n",
    "    final_string += f'<img src=\"./images/{img}\" alt= “” width=\"1000px\">\\n\\n'\n",
    "print(final_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a78e2a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Masters Thesis Colloquium\n",
    "Proxy Attention : Approximating Attention in CNNs using Gradient Based Techniques\n",
    "\n",
    "\n",
    "Subhaditya Mukherjee\n",
    "\n",
    "Supervisors: S.H. Mohades Kasaei and Matias Valdenegro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fde76ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffdc8ed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c231af-a5a6-43cd-98cb-7ded59641fcb",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<img src=\"./images/cmuff.jpg\" alt= “” width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfca1359-fd78-4ea1-81ea-4a9fcdeef7c4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<img src=\"./images/class2.png\" alt= “” width=\"1000px\">\n",
    "<!---\n",
    "lung cancer\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35be219c-8579-4889-b8de-f0b9a1d9e696",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<img src=\"./images/class3.jpg\" alt= “” width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b97b9e8-454a-45e8-930e-1717be80cf29",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<img src=\"./images/class4.png\" alt= “” width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d58698-14e5-4935-94b3-3f76aec19938",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## How?\n",
    "<img src=\"./images/nn.gif\" alt= “” width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b75c50",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Quantifying Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cab5f23-7a3a-4daa-a154-ba6f9d3a5470",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Accuracy\n",
    "<img src=\"./images/chall1.jpg\" alt= “” width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6e87ab-8322-494f-af91-328d7f6e18e3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Explainability\n",
    "<img src=\"./images/challcam.jpg\" alt= “” width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9308ac0-0e8f-4326-b629-2c90c5325524",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c3bba8-ad4b-4127-a0b5-cd9346ecd305",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Parameters\n",
    "<img src=\"./images/params.png\" alt= “” width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a8eef0-8dd5-4ae0-9f72-8ea79b808649",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Dataset sizes\n",
    "<img src=\"./images/dssize.png\" alt= “” width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5149360-a733-45fc-b53c-a10db776c1c3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Consequences \n",
    "- More labelled data\n",
    "- Vastly more energy consumption\n",
    "- Funds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0d73ab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Objective\n",
    "- Create a method to improve accuracy and explanations for image classification\n",
    "- No extra labels, reduced compute time, no modification to the architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d37c9c3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Previous Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e0b9a7-62a3-4b43-b9ea-4783a239e744",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5cd105-89ea-4aad-a358-99f782d445db",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<img src=\"./images/aug.png\" alt= “” width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a968b27-a0d4-4063-94c4-691cd92a5840",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<img src=\"./images/data_aug_categories.png\" alt= “” width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21daab23-6624-4bda-ad08-3122bab0b827",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Gradient Based Explanations\n",
    "<img src=\"./images/cams.png\" alt= “” width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f41bc9b-cdb6-4276-831a-25e0435eaddf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Proxy Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12302cba-da73-4afc-b6fa-1085df83bd3a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Research Questions\n",
    "1. Is it possible to create an augmentation technique that uses Attention maps?\n",
    "2. Is it possible to approximate the effects of Attention from ViTs in a CNN without changing the architecture?\n",
    "3. Is it possible to make a network converge faster and consequently require fewer data using the outputs from XAI techniques?\n",
    "4. Does using Proxy Attention impact the explainability positively?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ce7ca8-6d0f-4245-9fbd-d35122403080",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Intuition\n",
    "![img](./images/clutter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2dee862-e279-451c-8a63-ec5c58f3b94e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "![img](./images/attention.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60db65a-a74f-4499-8589-91db9c071f2c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "![img](./images/vitarch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e8e879-8ccd-40be-ae68-cd44e4130612",
   "metadata": {},
   "source": [
    "- insert gradcam example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46edb87-0730-489f-ac5c-5270fe14b851",
   "metadata": {},
   "source": [
    "- insert pipeline image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08910878-923a-4e2c-8b95-d00d68ee2ac2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Extensive Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788f6972-0ea9-4ffd-aa2b-93f376fc2493",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f24932-cb71-445a-abe7-1ea1c918aab0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### CIFAR100\n",
    "<img src=\"./images/cifar100.pdf.png\" alt= “” width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb78849f-ae83-45e2-96bd-c4da24ad10e8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Caltech101\n",
    "<img src=\"./images/caltech101.pdf.png\" alt= “” width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5dccef-1d41-4fde-82cf-840268214d39",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Places\n",
    "<img src=\"./images/places256.pdf.png\" alt= “” width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd14ff86-f247-47f5-90f3-107e234b2952",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Stanford Dogs\n",
    "<img src=\"./images/dogs.pdf.png\" alt= “” width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b61ad2-f810-4526-b855-f3e66b84055f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Tsinghua Dogs\n",
    "<img src=\"./images/tsing.png\" alt= “” width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0e415e-8bc9-4e9f-9a0c-5fcd5bc8aa31",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14235014-3c46-49cf-be31-9450afe0de04",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### VGG\n",
    "<img src=\"./images/vggarch.png\" alt= “” width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7efcc3-d879-452f-9b26-9c8204aaebaa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### ResNet\n",
    "<img src=\"./images/resnetarch.png\" alt= “” width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5b93cc-0abd-4a76-ab43-bffc287f8f77",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### EfficientNet\n",
    "<img src=\"./images/effnetarch.png\" alt= “” width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd28ced-8a77-43c1-b733-46af4eb4a1d9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Vision Transformer\n",
    "<img src=\"./images/vitarch.png\" alt= “” width=\"1000px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c735cb2-8343-40e3-a9e2-438011640d87",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Proxy Image Threshold\n",
    "<img src=\"./images/grad_threshold-crop.pdf.png\" alt= “” width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe02ee6b-801b-45cd-9007-464f2d170bb4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Proxy Image Weight\n",
    "<img src=\"./images/multiply_threshold-crop.pdf.png\" alt= “” width=\"500px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65dc7fe-6660-4b72-838f-338e237c67f7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Pixel Replacement Types\n",
    "<img src=\"./images/replacementtypes.pdf.png\" alt= “” width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a44c8d-6dd3-475e-9d54-83880e4ac3a5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Proxy Step Schedule\n",
    "- [20, p,19]\n",
    "- [5, p, 9, p,9, p,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa89912-5589-44c9-9df2-b4113be743e5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Subset of Wrongly Classified Images\n",
    "- 0.1\n",
    "- 0.2\n",
    "- 0.4\n",
    "- 0.8\n",
    "- 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3662ef6f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02df7687",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0341855",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c444ca4-1928-4f9d-937d-5e863e380ef2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# References\n",
    "- https://lih-verma.medium.com/query-key-and-value-in-attention-mechanism-3c3c6a2d4085\n",
    "- @dosovitskiyImageWorth16x162021\n",
    "- https://epochai.org/blog/trends-in-training-dataset-sizes"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "rise": {
   "async_timeout": 250,
   "auto_select": "code",
   "auto_select_timeout": 450,
   "autolaunch": false,
   "backimage": false,
   "center": true,
   "controls": true,
   "enable_chalkboard": true,
   "footer": "",
   "header": "",
   "height": "100%",
   "history": true,
   "overlay": "",
   "progress": true,
   "restore_timeout": 500,
   "scroll": true,
   "show_buttons_on_startup": true,
   "slideNumber": true,
   "start_slideshow_at": "selected",
   "theme": "simple",
   "transition": "fade",
   "width": "100%"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
