\chapter{Discussion} \label{ch:discussion}
\section{Research Questions}

 
The section discusses our results with respect to the research questions proposed in the study.
\begin{itemize}
\item \textbf{Is it possible to create an augmentation technique that uses Attention maps :} From our experiments, we do find that it is possible to use gradient based techniques as an augmentation step to create a proxy of the attention mechanism. Using proxy attention improves performance with a minimal increase in computation. Thus we can say that this research objective was completely fulfilled.
\item \textbf{Is it possible to approximate the effects of Attention from ViTs in a CNN without changing the architecture :} While proxy attention does incorporate the results of gradient based techniques while training to help guide the networks attention better, we do see that the type of attention exhibited is not the same as that of a vision transformer. Since the latter uses patches of a fixed size to feed into the network generating an attention map is related to the size of the patch as well. In the case of CNNs, the attention map is generated per pixel in the input image. To some extent it seems to be more useful to look at the attention map generated by CNN as compared to that of a vision transformer. We do also see that the CNN takes a lot less computational resources and time to train as compared to the vision transformer while also having a slightly more useful attention map. 

This being the case, this objective can also said to be completed, but the results were slightly different from what was initially expected. Future work could tackle this discrepancy by including research from quantifying attention flows in transformers. \cite{abnarQuantifyingAttentionFlow2020}
\item \textbf{Is it possible to make a network converge faster and consequently require less data using the outputs from XAI techniques :} from our results we do indeed see that using the outputs of gradient based techniques as part of proxy attention does have the network converge faster. As for using less data, although the need for data is not replaced, proxy attention does improves performance without the addition of new data or any modifications to the architecture. Thus we can say that this objective was also fulfilled.
\item \textbf{Does using Proxy Attention impact the explainability positively :} from comparing the attention maps, generated by networks and without proxy attention, we can see that the former does have better explanations. In most cases, using proxy attention does help guide the network to the important parts of the image. It is to be noted that in some cases using our method does make the network focus on the wrong part of the image, but this happens in frequently, and is not an inherent flaw of the method.

Although this does not impact the final performance, some possible reasons of this happening are discussed below. That being the case, we can also say that this objective was mostly fulfilled, except for the occasional mistake.
\end{itemize}


\section{Discussion of Results}
The following observations were made based on the results of both the explainability, and the accuracy obtained for different models trained with and without proxy attention on a multitude of datasets.
\begin{itemize}
 \item In general, using proxy attention to training model improves performance, regardless of the dataset and model used.
 \item Applying the proxy attention step multiple times in the training process does seem to improve performance, depending on how many times it was applied. But the rate of change of performance is not linear, and of course applying the proxy step multiple times does increase amount of compute required.
 \item The proxy attention step also seems to improve the results of the vision transformer and helps it learn to focus on the right regions of the image faster.
 \item From the results, we can see that it is not always beneficial to apply the proxy attention step to every image that the model predicted wrong. One of the hyper parameters was the subset of images that were passed to proxy attention. It was observed that if a network already has learnt a good representation of the data, giving it too many images might negatively affect performance. But if optimal performance has not been reached yet, then varying the Proxy Image subset parameter seems to be beneficial.
 \item Tuning Proxy Weight and Proxy Threshold is not really necessary but can help slightly improve performance. Using the values we suggested is enough in most cases.
 \item For easier datasets, using Proxy Attention sometimes degrades the performance and explainability of the network. For harder tasks and networks that do not initially learn well though, applying the method boosts accuracy.
 \item In most cases results indicate that models trained with Proxy Attention have improved explainability, for both CNNs and the Vision Transformer.
\end{itemize}



\section{Limitations of Proxy Attention}
\begin{itemize}
\item \textbf{Hyperparameters :} The research in this paper discusses quite a few hyper parameters. While optimal values of most of the hyper parameters have been found and discussed, some of them such as the Proxy Weight and Proxy Threshold seem to have more subtle effects. Although some recommended values for these parameters have been discussed, they seem to be reliant on the dataset used as well. That being the case, it would be required to further test these hyper parameters to improve performance on a case to case basis. This further testing is a very common part of most deep learning algorithms, but since Proxy Attention was created with the intent of reducing the computational cost of training a neural network, these additional tests do not support the objective fully. 

But on the other hand, we noticed that it was possible to get an improvement in performance even with the values we suggested for these hyperparameters. This means that having to do these additional tests is an optional part of the algorithm, and depending on the use case and the amount of performance tuning required, it is possible to use them to further improve performance if necessary.
\item \textbf{Attention :} One of the motives of Proxy Attention was to be able to imitate the effects of the attention mechanism to further improve performance by guiding a CNN based on its mistakes. From our results, we do indeed see that using our method does improve the attention of the network and helps it to better figure out the important parts of the image. But, in some cases using Proxy Attention does influence the network negatively. Although this is not extremely common, and for the most part does not really impact performance all that much it is worth noting that further research is needed in this particular case. 

One of the possible reasons that this happens is because the network does learn different representations at different parts of the training process. In the case where the network initially gets the attention map correct and then later gets it wrong, it might be because at that particular time step, the network has not fully developed a representation for that class yet. To support the statement, it is valuable to see that even though at the time steps considered, some of the attention map seem to be wrong, in the long run the performance of the network does not seem to be affected. For almost all our tests, we did see an increase in performance when using Proxy Attention.
\item \textbf{Better Scheduling :} Unlike data augmentation, the Proxy Attention step is not applied for every epoch. Since our method relies on the understanding of the network itself, applying it for every epoch seems counterintuitive. It does take any network time to learn, and if the step was applied for every epoch, it might actually end up destabilising training. That being the case, we did test multiple schedules with varying intervals of applying Proxy Attention. From our experiment, we did see that applying our method multiple times does seem to improve performance but there does not seem to be a linear increase in performance with increasing number of applications of Proxy Attention. 

In the initial stages of training neural networks, the network of course has not had time to learn better representation. While in the later stages of training, the network does have a better understanding of the data distribution and in turn can be used to generate better attention maps. That being the case, having a more semi automatic schedule would be beneficial. This schedule was not tested in this research, but future work could look at implementing a scheduler based on the performance of the network across training. An algorithm similar to that of a learning rate scheduler could be used.
\end{itemize}

\section{General Discussion} 
\subsection{Data Augmentation}
\begin{itemize}
    \item \textbf{Why not provide annotations and object position: }Datasets that contain annotations and positions are not easy to compile. For custom tasks, it is quite difficult to obtain such datasets. That being the case, Proxy Attention is a good alternative as it does not require any extra information.

    \item \textbf{Why not apply it every epoch like augmentation: } Applying Proxy Attention every epoch comes with certain caveats. Since gradient maps are being computed for multiple images, this process is slightly more computationally expensive than standard training. Doing so might also lead to overfitting as the network is given too much feedback. Since Proxy Attention relies on the model's predictions, giving the network time to learn is a good idea.

    \item \textbf{Isn't it the same as giving more images to the model: } This is somewhat true. Proxy Attention provides similar benefits to augmentation but with more specific information. In the models tested, an equal number of images were given to each model. Models trained with Proxy Attention and those trained without it received the same number of images. For example, if the modified image was present, the original image was not passed to the network to maintain fairness.
\end{itemize}

\subsection{Other Domains}
\begin{itemize}
\item \textbf{Why is it not SSL:} SSL involves using generating pseudo-labels in the case of real labels not being available and Proxy Attention simply uses Gradient based methods to guide the network to the correct parts of the image. Future research could also explore using Proxy Attention together with SSL.
\item \textbf{Why not use distillation:} Proxy Attention was created with the hope of speeding up training with minimal increase in the usage of computational resources. Distillation refers to using a model that was already trained on a similar task to improve the learning process of the current model. While distillation works very well and is useful in many regards, it goes against the aims of Proxy Attention as training the original network would have used up even more computational resources. That is not to say that distillation is not advantageous, but just that it is just not in the scope of this study.
\end{itemize}

\subsection{Model Architecture and Attention Modules}
\begin{itemize}
    \item \textbf{Why not just use a Transformer: }The use of Transformers was indeed tested, but they are more computationally expensive and not as easy to train as Convolutional Neural Networks (CNNs). The objective of the study was to explore whether the effects of the attention module could be approximated using gradient-based techniques. As a side note though, Proxy Attention, which also works with Transformers and also seems to improve the results of the model.
    \item \textbf{Why not modify the network architecture to include attention modules: }Proxy Attention was developed as a general technique that can be applied to any network architecture, not limited to CNNs. Many of the surveyed papers required specialized architectures, which can be counterintuitive and not always feasible to implement. Thus, Proxy Attention offers a more versatile approach for improving performance by only modifying the training process.
\end{itemize}

\subsection{Gradient Based Techniques}
\begin{itemize}
    \item \textbf{Why use a different gradient-based technique for the results: } To ensure fairness in the evaluation process, it is important to use a different gradient-based technique for the results. Neural networks excel at approximating transformations, so if the same technique is used for both training and evaluation, the network may end up learning to approximate the technique itself instead of learning where to look, which would defeat the whole point of Proxy Attention. Testing with a different technique ensures a fair comparison and avoids bias in the evaluation.

    \item \textbf{Combining gradient-based techniques with training is a bad idea, right: } This study was conducted specifically to investigate the feasibility of combining gradient-based techniques with training. While further rigorous testing is necessary to gain a deeper understanding of why and to what extent this approach works, our findings indicate that it is a promising starting point. Combining gradient-based techniques with training can lead to improved performance, but more research is needed to fully comprehend the underlying mechanisms and limitations of this approach.
\end{itemize}

\subsection{Hyperparameters}
\begin{itemize}
    \item \textbf{Why so many hyperparameters: } Since Proxy Attention is a novel technique, it necessitated testing multiple hyperparameters to determine which ones yield the best results. The extensive exploration of hyperparameters enabled the author to narrow down the search space for future experiments and focus only on the most effective ones. Varying the hyperparameters also allowed the author to examine the impact of different components of the pipeline, leading to a better understanding of the overall effects of using Proxy Attention.

    \item \textbf{Why do some of the hyperparameters not seem to be very sensitive: } In certain datasets, the hyperparameters may not exhibit significant effects. This could be attributed to the network already performing well enough on those datasets, rendering the hyperparameter variations less impactful. While there are theoretical expectations of certain effects, such as faster convergence, the practical results may not always demonstrate a substantial improvement in our studies. But although the effects may not be pronounced, they still contribute to enhancing the overall results to a certain degree. If nothing else, they do not seem to have a negative impact on the results, so they are still worth exploring.
\end{itemize}

\subsection{Stability and Training Effects}
\begin{itemize}
    \item \textbf{Will this destabilize training: } Some predictions that were previously classified correctly may become incorrect when using Proxy Attention. It is also to be noted that this does not seem to occur too frequently, so it is not a major concern. While this phenomenon does not appear to destabilize the training process, it does make the network more sensitive to applying Proxy Attention too frequently. The results indicate that applying Proxy Attention improves performance regardless, but it is not recommended to apply it every epoch. 

    \item \textbf{In later iterations, what happens if a correctly predicted image is wrongly classified: } Since the images generated through Proxy Attention are not persistent, the network should be capable of recovering from such misclassifications. In the worst-case scenario, the network will be trained on a slightly modified image, which is not necessarily detrimental. Moreover, the network has previously encountered the original image, so it should be able to learn a better representation of the image through subsequent iterations.

\end{itemize}

\subsection{Challenges with External Libraries} \label{sec:challenges_with_external_libraries}
Some of the challenges that were faced while using external libraries are as follows:
\begin{enumerate}
    \item \textbf{GPU cache}: While Ray Tune and Optuna manage resources efficiently, they did not clear the GPU cache effectively. PyTorch, by default, holds on to the GPU cache and does not release it until the program is closed for efficiency. This would not be a problem for a single training run, but if many trials were being run, the cache would quickly fill up and cause the training to crash. This does not imply that using Proxy Attention makes it impossible to use such libraries but that it was easier to implement a custom solution.
    \item \textbf{Cluster} : Both libraries were written to enable running large-scale experiments over multiple machines. While this would be useful for a large-scale project, it added unnecessary complexity to this project as all the experiments were run on a single machine.
    \item \textbf{Grid Search} : Both libraries mentioned above were designed for hyperparameter tuning and to implement multiple grid search variants. While this would be useful, it would stop many trials that would eventually be useful to analyze. In this project, it was important to have results for each of the trials, and since the author could not find a way to disable the default Early Stopping behaviour as part of the grid search, a simple trial generation algorithm was created instead.
\end{enumerate}