\chapter{Conclusion}
\section{Contributions}
- Novel augmentation technique that uses attention maps
    - Better explainability , better performance
    - Faster convergence, less data
    - Easy to implement, no change in architecture
- Experiments with a large number of hyperparameters and models to test the robustness of the method and find the best configuration
- Open source callback code that can be used to easily add Proxy Attention to any existing code base
- Script to easily parse tensorboard logs to a unified DataFrame for easy analysis
- Scripts to reproduce all the results in this thesis along with training logs

\section{Lessons Learned}
- Combining research from different domains to create a novel method
- Importance of hyperparameter tuning
- Understading memory leaks and debugging them, how to optimize memory usage 
- Importance of writing functional (instead of class oriented) code that can be easily reused and modified 
- Better understanding of Augmentation, XAI techniques
- Better understanding of configuring the training loop

\section{Future Work}
% - Schedule both the number of Proxy Steps, number of images for Proxy step based on validation performance
% - Experiment with more XAI methods
% - Experiment with smoothing the attention maps before using them for the Proxy step
While the results of this thesis are promising, there is still a lot of room for improvement. The following are some of the possible future directions for this work:
\begin{itemize}
    \item \textbf{Schedules} Currently, the number of Proxy Steps and the number of images used for the Proxy Step are fixed. It would be interesting to schedule both of these based on the validation performance. For example, if the validation performance is not improving, we can increase the number of Proxy Steps and the number of images used for the Proxy Step.
    \item \textbf{More XAI methods} We have only used a tiny subset of XAI methods for this thesis. It would be interesting to experiment with more XAI methods (eg: other methods from the literature survey) and see if they can be used to improve the performance of Proxy Attention.
    \item \textbf{Smoothing Attention Maps} The attention maps generated by the XAI methods are noisy. While no extra smoothing was used in this thesis, it would be useful to experiment with smoothing the attention maps before using them for the Proxy step. An example of a potentially suitable smoothing method is Eigen Smoothing \cite{jacobPyTorchLibraryCAM2021}.
\end{itemize}
