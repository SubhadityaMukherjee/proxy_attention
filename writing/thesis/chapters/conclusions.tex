\chapter{Conclusion} \label{ch:conclusion}

\section{Lessons Learned}
The lessons learned from this thesis are as follows:
\begin{itemize}
    \item \textbf{Combining research from different domains to create a novel method: } This thesis taught me how to combine research from different domains to create a novel method. In this case, we combined research from the domains of XAI and Augmentation to create a novel augmentation technique.
    \item \textbf{Hyperparameter Tuning: } We performed a large number of experiments with different hyperparameters and models to test the robustness of the method and find the best configuration. Doing so taught me the importance of hyperparameter tuning.
    \item \textbf{Memory Leaks: }We encountered a lot of memory leaks while working on the code for this thesis, and in the process learned how to debug and fix them. 
    \item \textbf{Function based code: }We wrote the code for this thesis in a functional style instead of an object oriented style as a personal experiment (inspired by the success of the Julia programming language). This made it easy to reuse certain parts of the code and modify others. Doing so taught me the importance of writing functional code.
    \item \textbf{Augmentation: }We learned a lot about augmentation while working on this thesis. We learned about the different types of augmentations, how to implement them, and how to use them to improve the performance of CNNs.
    \item \textbf{XAI: }We also learned a lot about XAI while working on this thesis. 
    \item \textbf{Training Loop: } Previous to this thesis, the author had only used the training loop provided by higher level libraries. However, for this thesis, we had to implement the training loop from scratch. Doing so taught the author a lot about the different components of the training loop and how to configure them for optimal performance and modify them to suit the needs of the project.
\end{itemize}

\section{Future Work}
While the results of this thesis are promising, there is still a lot of room for improvement. The following are some of the possible future directions for this work:
\begin{itemize}
    \item \textbf{Schedules:} Currently, the number of Proxy Steps and the number of images used for the Proxy Step are fixed. It would be interesting to schedule both of these based on the validation performance. For example, if the validation performance is not improving, we can increase the number of Proxy Steps and the number of images used for the Proxy Step.
    \item \textbf{More XAI methods:} We have only used a tiny subset of XAI methods for this thesis. It would be interesting to experiment with more XAI methods (eg: other methods from the literature survey) and see if they can be used to improve the performance of Proxy Attention.
    \item \textbf{Smoothing Attention Maps:} The attention maps generated by the XAI methods are noisy. While no extra smoothing was used in this thesis, it would be useful to experiment with smoothing the attention maps before using them for the Proxy step. An example of a potentially suitable smoothing method is Eigen Smoothing \cite{jacobPyTorchLibraryCAM2021}.
    \item \textbf{Better Attention Maps for ViT:} This research used the base vision transformer model but Abnar et al. \cite{abnarQuantifyingAttentionFlow2020} in their paper, find that the attention maps generated by a vision transformer are pretty unreliable due to self attention, combining different representations across layers of the transformer. While using self attention does lead to massive improvements and performance for Transformers, using these attention weights is an unreliable method of generating proper explanations. Thus future work could take their work into account to have a better comparison between CNNs and Transformers.
\end{itemize}
