
\chapter{Implementation}
\section{Overview}
\section{Hyper parameters}
\subsection{Clear Every Step}

\subsection{Gradient Method}

\subsection{Gradient Threshold Considered}

\subsection{Multiply Weight }

\subsection{Proxy Steps}

\subsection{Subset Of Wrongly Classified}

\section{Data Loading and Pre Processing}
\subsection{Directory structure}

\subsection{Label function}

\subsection{Clearing proxy images}

\subsection{Encode, Stratify, Kfold}

\subsection{train and test, val separate}

\subsection{Augmentations}
Imagenet Normalize
Tensor
Num workers

\section{Training Details}

\section{Grid Search}

\section{Optimizations}
\subsection{Mixed Precision}
\subsection{Gradient Scaling}
\subsection{No grad}
\subsection{Batched Proxy step}
\subsection{Trial Resumption}


\subsection{Models}
TIMM

\section{Gradient Based Methods}
\section{Proxy Attention}

\begin{algorithm}
    \caption{Single Batch Proxy Attention}
    \label{alg:proxy_attention_single_batch}
    \begin{algorithmic}
        \REQUIRE $input\_wrong$
        \REQUIRE $CAM$
        \REQUIRE $proxy\_threshold$
        \REQUIRE $proxy\_image\_weight$
        \STATE $grads \leftarrow CAM(input\_wrong)$
        \STATE $inversed\_normalized\_inputs \leftarrow inverse\_normalize(input\_wrong)$

        \STATE $output \leftarrow REPLACE(grads \geq proxy\_threshold, ((1- proxy\_image\_weight) * grads) * inversed\_normalized\_inputs, inversed\_normalized\_inputs)$
    \end{algorithmic}
\end{algorithm}


\begin{algorithm}
    \caption{Batch Proxy Attention}
    \label{alg:proxy_attention_batch}
    \begin{algorithmic}
        \REQUIRE $input\_wrong$
        \REQUIRE $label\_wrong$
        \REQUIRE $subset\_chosen$

        \STATE $chosen\_inds = CEIL(subset\_chosen * LENGTH(input\_wrong))$
        \STATE $input\_wrong\_subset = input\_wrong[:chosen_inds]$
        \STATE $label\_wrong\_subset = label\_wrong[:chosen_inds]$

        \STATE $processed\_labels \leftarrow [], processed\_thresholds \leftarrow []$
        \FOR{$i \leftarrow 0$ \TO $LENGTH(label\_wrong\_subset)$}
        \STATE $pass$
        \ENDFOR

    \end{algorithmic}
\end{algorithm}

\subsection{Callback Mechanism}
\section{Tensorboard}
\section{Transfer learning}
\section{Optimizer}
\section{LR scheduler}
\section{Loss function}
\section{Batch Size Finder}
To maximize training performance, a batch size finder \ref{alg:batch_size_finder} is used to find the optimal batch size for each of the models.

\begin{algorithm}
    \caption{Batch Size Finder Algorithm}
    \label{alg:batch_size_finder}
    \begin{algorithmic}
        \REQUIRE $dataset\_size$
        \REQUIRE $max\_batch\_size$
        \STATE $batch\_size \leftarrow 2$
        \WHILE{TRUE}
        \IF{$max\_batch\_size$ is not $None$ $\And$ $batch\_size \geq max\_batch\_size$}
        \STATE $batch\_size \leftarrow max\_batch\_size$
        \ENDIF
        \IF{$batch\_size \geq dataset\_size$}
        \STATE $batch\_size \leftarrow batch\_size // 2$
        \ENDIF

        \IF{$failed$ is $False$}
        \LOOP
        \STATE $inputs \leftarrow random((batch\_size,input\_shape))$
        \STATE $targets \leftarrow random((batch\_size,output\_shape))$
        \STATE $outputs \leftarrow model(inputs)$
        \STATE $loss \leftarrow MSE(outputs, targets)$
        \STATE $loss.backward()$
        \STATE $optimizer.step()$
        \STATE $optimizer.zero\_grad()$
        \STATE $failed \leftarrow True$
        \STATE $batch\_size \leftarrow batch\_size * 2$
        \ENDLOOP
        \ELSIF{$failed$ is $True$}
        \STATE $failed \leftarrow False$
        \STATE $batch\_size \leftarrow batch\_size // 2$
        \ENDIF

        \ENDWHILE

    \end{algorithmic}
\end{algorithm}

\section{Result Aggregation}

\section{Inference}