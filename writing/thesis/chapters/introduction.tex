
\chapter{Introduction}

\section{Problem Statement}
The problem statement of this study is creating a novel augmentation technique \textbf{Proxy Attention}, that uses attention maps to improve the performance of any model by guiding its attention away from the regions that are not important for the classification task.
In turn, this method should also improve the explainability of the model while maintaining the same architecture and hyperparameters.

\section{Motivation}
Over the past few years, Transformers have slowly become the SOTA in a majority of NLP tasks. Recently, they have also started taking over the CV world. Dosovitskiy et al. \cite{dosovitskiyImageWorth16x162021}  modified the image pipiline to generate patches of images, hence mimicking the NLP pipiline required by the Transformer. Doing so led to the creation of the Vision Transfer (ViT), variants of which have also been used to achieve SOTA results in image classification. However, ViTs are computationally expensive, harder to train, and require more data. While transfer learning can be used to mitigate the data requirement, CNNs are still extremely useful and are the go-to choice for many computer vision tasks.\\
One of the biggest advantages of Transformers is Attention, which helps it learn where to look in an image/text \cite{vaswaniAttentionAllYou2017}. The caveat of the boost in performance that attention brings, is offset by it's computational cost. As Poli et al. \cite{poliHyenaHierarchyLarger2023} in their research on larger language models find, using attention is not always worth it. CNNs are still extremely useful, but they lack the inbuilt explainability of Transformers. A vast majority of CNNs that we use today rely on older concepts, and upgrading the principles behind them to fit more mordern standards is not easy. An initial idea for this study was inspired by the work of Liu et al. \cite{liuConvNet2020s2022}, where the authors proposed a new architecture that uses many concepts from Transformers to improve the performance of CNNs. While this approach is promising, it requires a lot of changes to the architecture and hyperparameters, and does not generalize well to other models.\\
The motivation behind Proxy Attention was to combine the best of both worlds. Since it is not directly possible without specialized architectures, we use XAI techniques to approximate the effects of attention, and use it as a \textit{Proxy}. The idea behind it stemmed from the following intuition: the mistakes made by CNNs are often due to the model focusing on the wrong regions of the image. While CNNs eventually learn to understand the images better and choose the right regions, this requires quite some training time. Proxy Attention aims to slightly speed up this process, and eventually make the model converge faster by gently guiding its attention away from the regions that are not important for the classification task.\\
Proxy Attention uses what the model already knows to help guide it by helping it better understand its mistakes. Using XAI techniques, the regions that the model used to make its prediction can be identified. If the image was misclassified, we can say that the model probably focused on the wrong regions. Using this information, we can re-weight the image to minimize the effect of the regions that most strongly influenced the prediction. Since the model is already familiar with the image, showing it a modified version of the image should potentially help it generalize better.\\
This research further explores the idea of Proxy Attention, and tests its effect on the performance and explainability of standard models for classification tasks. No method is perfect, and Proxy Attention is no exception. That being the case, we also explore some of the limitations of Proxy Attention and discuss possible solutions to mitigate them.

\section{Context and Novelty}
The concept of Proxy Attention is relevant to any computer vision task, but we focus on image classification in this study. Using Proxy Attention, significant improvements in performance and explainability can be achieved without changing the architecture and with minimal changes to an existing code base.\\
The novelty of this study is the use of XAI techniques as an augmentation technique to approximate the effects of Attention in a CNN and guide the model's focus away from the regions that are preventing it from making the correct prediction. Proxy Attention was created in the hopes of showing that the explainability of CNNs can be improved by using the outputs from XAI techniques.\\
Combining these two concepts is a novel approach, and the author hopes that this study will inspire further research in this direction and motivate researchers to explore the possibilities of combining seemingly unrelated concepts to create novel solutions.

\section{Challenges}
The major challenges of this study were as follows:
\begin{itemize}
    \item Creating a novel augmentation technique that uses attention maps to improve the model's performance.
    \item Testing the effect of Proxy attention on the explainability of the model.
    \item Comparing many hyperparameters and models with limited computational resources.
    \item Optimizing the usage of XAI techniques to improve the computational efficiency of Proxy Attention.
\end{itemize}

\section{Research Questions} \label{section:researchq}
The main research questions that summarize the aims of this study are as follows.
\begin{enumerate}
    \item Is it possible to create an augmentation technique that uses Attention maps?
    \item Is it possible to approximate the effects of Attention from ViTs in a CNN without changing the architecture?
    \item Is it possible to make a network converge faster and consequently require less data using the outputs from XAI techniques?
    \item Does using Proxy Attention impact the explainability positively?
\end{enumerate}
\section{Thesis Outline}
This thesis follows the following structure:
\begin{itemize}
    \item \textbf{Chapter 2} provides the necessary background information about this study's relevant topics and datasets.
    \item \textbf{Chapter 3} provides a literature review of the relevant topics.
    \item \textbf{Chapter 4} describes the methodology used in this study and the implementation details.
    \item \textbf{Chapter 5} presents the results and answers the research questions.
    \item \textbf{Chapter 6} discusses the results in the context of the research questions.
    \item \textbf{Chapter 7} concludes the thesis and provides recommendations for future work.
    \item \textbf{Chapter 8} provides additional details and results.
\end{itemize}