
@misc{simonyan_deep_2014,
	title = {Deep {Inside} {Convolutional} {Networks}: {Visualising} {Image} {Classification} {Models} and {Saliency} {Maps}},
	shorttitle = {Deep {Inside} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1312.6034},
	abstract = {This paper addresses the visualisation of image classiﬁcation models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The ﬁrst one generates an image, which maximises the class score [5], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, speciﬁc to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classiﬁcation ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [13].},
	language = {en},
	urldate = {2022-11-18},
	publisher = {arXiv},
	author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
	month = apr,
	year = {2014},
	note = {arXiv:1312.6034 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Simonyan et al. - 2014 - Deep Inside Convolutional Networks Visualising Im.pdf:/Users/eragon/Zotero/storage/EXEY4ASY/Simonyan et al. - 2014 - Deep Inside Convolutional Networks Visualising Im.pdf:application/pdf},
}

@misc{ribeiro_why_2016,
	title = {"{Why} {Should} {I} {Trust} {You}?": {Explaining} the {Predictions} of {Any} {Classifier}},
	shorttitle = {"{Why} {Should} {I} {Trust} {You}?},
	url = {http://arxiv.org/abs/1602.04938},
	abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classiﬁer in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the ﬂexibility of these methods by explaining diﬀerent models for text (e.g. random forests) and image classiﬁcation (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classiﬁer, and identifying why a classiﬁer should not be trusted.},
	language = {en},
	urldate = {2022-11-18},
	publisher = {arXiv},
	author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
	month = aug,
	year = {2016},
	note = {arXiv:1602.04938 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf:/Users/eragon/Zotero/storage/J66379K9/Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf:application/pdf},
}

@misc{hinton_distilling_2015,
	title = {Distilling the {Knowledge} in a {Neural} {Network}},
	url = {http://arxiv.org/abs/1503.02531},
	abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions [3]. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators [1] have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can signiﬁcantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish ﬁne-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	language = {en},
	urldate = {2022-11-18},
	publisher = {arXiv},
	author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	month = mar,
	year = {2015},
	note = {arXiv:1503.02531 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf:/Users/eragon/Zotero/storage/TRICABTJ/Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf:application/pdf},
}

@misc{springenberg_striving_2015,
	title = {Striving for {Simplicity}: {The} {All} {Convolutional} {Net}},
	shorttitle = {Striving for {Simplicity}},
	url = {http://arxiv.org/abs/1412.6806},
	doi = {10.48550/arXiv.1412.6806},
	abstract = {Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the "deconvolution approach" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.},
	urldate = {2022-11-18},
	publisher = {arXiv},
	author = {Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
	month = apr,
	year = {2015},
	note = {arXiv:1412.6806 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/DQH9A4MX/Springenberg et al. - 2015 - Striving for Simplicity The All Convolutional Net.pdf:application/pdf},
}

@misc{ghorbani_interpretation_2018,
	title = {Interpretation of {Neural} {Networks} is {Fragile}},
	url = {http://arxiv.org/abs/1710.10547},
	doi = {10.48550/arXiv.1710.10547},
	abstract = {In order for machine learning to be deployed and trusted in many applications, it is crucial to be able to reliably explain why the machine learning algorithm makes certain predictions. For example, if an algorithm classifies a given pathology image to be a malignant tumor, then the doctor may need to know which parts of the image led the algorithm to this classification. How to interpret black-box predictors is thus an important and active area of research. A fundamental question is: how much can we trust the interpretation itself? In this paper, we show that interpretation of deep learning predictions is extremely fragile in the following sense: two perceptively indistinguishable inputs with the same predicted label can be assigned very different interpretations. We systematically characterize the fragility of several widely-used feature-importance interpretation methods (saliency maps, relevance propagation, and DeepLIFT) on ImageNet and CIFAR-10. Our experiments show that even small random perturbation can change the feature importance and new systematic perturbations can lead to dramatically different interpretations without changing the label. We extend these results to show that interpretations based on exemplars (e.g. influence functions) are similarly fragile. Our analysis of the geometry of the Hessian matrix gives insight on why fragility could be a fundamental challenge to the current interpretation approaches.},
	urldate = {2022-11-18},
	publisher = {arXiv},
	author = {Ghorbani, Amirata and Abid, Abubakar and Zou, James},
	month = nov,
	year = {2018},
	note = {arXiv:1710.10547 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/9YPQ3RXB/Ghorbani et al. - 2018 - Interpretation of Neural Networks is Fragile.pdf:application/pdf;Full Text:/Users/eragon/Zotero/storage/K7RINCR7/Ghorbani et al. - 2018 - Interpretation of Neural Networks is Fragile.pdf:application/pdf},
}

@misc{kindermans_reliability_2017,
	title = {The ({Un})reliability of saliency methods},
	url = {http://arxiv.org/abs/1711.00867},
	doi = {10.48550/arXiv.1711.00867},
	abstract = {Saliency methods aim to explain the predictions of deep neural networks. These methods lack reliability when the explanation is sensitive to factors that do not contribute to the model prediction. We use a simple and common pre-processing step ---adding a constant shift to the input data--- to show that a transformation with no effect on the model can cause numerous methods to incorrectly attribute. In order to guarantee reliability, we posit that methods should fulfill input invariance, the requirement that a saliency method mirror the sensitivity of the model with respect to transformations of the input. We show, through several examples, that saliency methods that do not satisfy input invariance result in misleading attribution.},
	urldate = {2022-11-18},
	publisher = {arXiv},
	author = {Kindermans, Pieter-Jan and Hooker, Sara and Adebayo, Julius and Alber, Maximilian and Schütt, Kristof T. and Dähne, Sven and Erhan, Dumitru and Kim, Been},
	month = nov,
	year = {2017},
	note = {arXiv:1711.00867 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/EBUDRT4E/Kindermans et al. - 2017 - The (Un)reliability of saliency methods.pdf:application/pdf},
}

@misc{lin_network_2014,
	title = {Network {In} {Network}},
	url = {http://arxiv.org/abs/1312.4400},
	doi = {10.48550/arXiv.1312.4400},
	abstract = {We propose a novel deep network structure called "Network In Network" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.},
	urldate = {2022-11-18},
	publisher = {arXiv},
	author = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
	month = mar,
	year = {2014},
	note = {arXiv:1312.4400 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/ZYBABWJK/Lin et al. - 2014 - Network In Network.pdf:application/pdf},
}

@misc{gan_vision-language_2022,
	title = {Vision-{Language} {Pre}-training: {Basics}, {Recent} {Advances}, and {Future} {Trends}},
	shorttitle = {Vision-{Language} {Pre}-training},
	url = {http://arxiv.org/abs/2210.09263},
	abstract = {This paper surveys vision-language pre-training (VLP) methods for multimodal intelligence that have been developed in the last few years. We group these approaches into three categories: (i) VLP for image-text tasks, such as image captioning, image-text retrieval, visual question answering, and visual grounding; (ii) VLP for core computer vision tasks, such as (open-set) image classiﬁcation, object detection, and segmentation; and (iii) VLP for video-text tasks, such as video captioning, video-text retrieval, and video question answering. For each category, we present a comprehensive review of state-of-the-art methods, and discuss the progress that has been made and challenges still being faced, using speciﬁc systems and models as case studies. In addition, for each category, we discuss advanced topics being actively explored in the research community, such as big foundation models, uniﬁed modeling, in-context few-shot learning, knowledge, robustness, and computer vision in the wild, to name a few.},
	language = {en},
	urldate = {2022-11-19},
	publisher = {arXiv},
	author = {Gan, Zhe and Li, Linjie and Li, Chunyuan and Wang, Lijuan and Liu, Zicheng and Gao, Jianfeng},
	month = oct,
	year = {2022},
	note = {arXiv:2210.09263 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language},
	file = {Gan et al. - 2022 - Vision-Language Pre-training Basics, Recent Advan.pdf:/Users/eragon/Zotero/storage/RZ5EPBRS/Gan et al. - 2022 - Vision-Language Pre-training Basics, Recent Advan.pdf:application/pdf},
}

@inproceedings{zhang_how_2021,
	title = {How {Does} {Mixup} {Help} {With} {Robustness} and {Generalization}?},
	url = {https://openreview.net/forum?id=8yKEo06dKNo},
	abstract = {Mixup is a popular data augmentation technique based on on convex combinations of pairs of examples and their labels. This simple technique has shown to substantially improve both the model's robustness as well as the generalization of the trained model. However, it is not well-understood why such improvement occurs. In this paper, we provide theoretical analysis to demonstrate how using Mixup in training helps model robustness and generalization. For robustness, we show that minimizing the Mixup loss corresponds to approximately minimizing an upper bound of the adversarial loss. This explains why models obtained by Mixup training exhibits robustness to several kinds of adversarial attacks such as Fast Gradient Sign Method (FGSM). For generalization, we prove that Mixup augmentation corresponds to a specific type of data-adaptive regularization which reduces overfitting. Our analysis provides new insights and a framework to understand Mixup.},
	language = {en},
	urldate = {2022-11-20},
	author = {Zhang, Linjun and Deng, Zhun and Kawaguchi, Kenji and Ghorbani, Amirata and Zou, James},
	month = mar,
	year = {2021},
	file = {Full Text PDF:/Users/eragon/Zotero/storage/67D6JV6N/Zhang et al. - 2021 - How Does Mixup Help With Robustness and Generaliza.pdf:application/pdf},
}

@misc{dabkowski_real_2017,
	title = {Real {Time} {Image} {Saliency} for {Black} {Box} {Classifiers}},
	url = {http://arxiv.org/abs/1705.07857},
	doi = {10.48550/arXiv.1705.07857},
	abstract = {In this work we develop a fast saliency detection method that can be applied to any differentiable image classifier. We train a masking model to manipulate the scores of the classifier by masking salient parts of the input image. Our model generalises well to unseen images and requires a single forward pass to perform saliency detection, therefore suitable for use in real-time systems. We test our approach on CIFAR-10 and ImageNet datasets and show that the produced saliency maps are easily interpretable, sharp, and free of artifacts. We suggest a new metric for saliency and test our method on the ImageNet object localisation task. We achieve results outperforming other weakly supervised methods.},
	urldate = {2022-11-24},
	publisher = {arXiv},
	author = {Dabkowski, Piotr and Gal, Yarin},
	month = may,
	year = {2017},
	note = {arXiv:1705.07857 [stat]},
	keywords = {Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/AFR2PAGY/Dabkowski and Gal - 2017 - Real Time Image Saliency for Black Box Classifiers.pdf:application/pdf},
}

@misc{bau_network_2017,
	title = {Network {Dissection}: {Quantifying} {Interpretability} of {Deep} {Visual} {Representations}},
	shorttitle = {Network {Dissection}},
	url = {http://arxiv.org/abs/1704.05796},
	doi = {10.48550/arXiv.1704.05796},
	abstract = {We propose a general framework called Network Dissection for quantifying the interpretability of latent representations of CNNs by evaluating the alignment between individual hidden units and a set of semantic concepts. Given any CNN model, the proposed method draws on a broad data set of visual concepts to score the semantics of hidden units at each intermediate convolutional layer. The units with semantics are given labels across a range of objects, parts, scenes, textures, materials, and colors. We use the proposed method to test the hypothesis that interpretability of units is equivalent to random linear combinations of units, then we apply our method to compare the latent representations of various networks when trained to solve different supervised and self-supervised training tasks. We further analyze the effect of training iterations, compare networks trained with different initializations, examine the impact of network depth and width, and measure the effect of dropout and batch normalization on the interpretability of deep visual representations. We demonstrate that the proposed method can shed light on characteristics of CNN models and training methods that go beyond measurements of their discriminative power.},
	urldate = {2022-11-24},
	publisher = {arXiv},
	author = {Bau, David and Zhou, Bolei and Khosla, Aditya and Oliva, Aude and Torralba, Antonio},
	month = apr,
	year = {2017},
	note = {arXiv:1704.05796 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, I.2.10},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/EDW6KRBC/Bau et al. - 2017 - Network Dissection Quantifying Interpretability o.pdf:application/pdf},
}

@misc{fong_understanding_2019,
	title = {Understanding {Deep} {Networks} via {Extremal} {Perturbations} and {Smooth} {Masks}},
	url = {http://arxiv.org/abs/1910.08485},
	doi = {10.48550/arXiv.1910.08485},
	abstract = {The problem of attribution is concerned with identifying the parts of an input that are responsible for a model's output. An important family of attribution methods is based on measuring the effect of perturbations applied to the input. In this paper, we discuss some of the shortcomings of existing approaches to perturbation analysis and address them by introducing the concept of extremal perturbations, which are theoretically grounded and interpretable. We also introduce a number of technical innovations to compute extremal perturbations, including a new area constraint and a parametric family of smooth perturbations, which allow us to remove all tunable hyper-parameters from the optimization problem. We analyze the effect of perturbations as a function of their area, demonstrating excellent sensitivity to the spatial properties of the deep neural network under stimulation. We also extend perturbation analysis to the intermediate layers of a network. This application allows us to identify the salient channels necessary for classification, which, when visualized using feature inversion, can be used to elucidate model behavior. Lastly, we introduce TorchRay, an interpretability library built on PyTorch.},
	urldate = {2022-11-24},
	publisher = {arXiv},
	author = {Fong, Ruth and Patrick, Mandela and Vedaldi, Andrea},
	month = oct,
	year = {2019},
	note = {arXiv:1910.08485 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/FDMYY4EH/Fong et al. - 2019 - Understanding Deep Networks via Extremal Perturbat.pdf:application/pdf},
}

@article{montavon_methods_2018,
	title = {Methods for {Interpreting} and {Understanding} {Deep} {Neural} {Networks}},
	volume = {73},
	issn = {10512004},
	url = {http://arxiv.org/abs/1706.07979},
	doi = {10.1016/j.dsp.2017.10.011},
	abstract = {This paper provides an entry point to the problem of interpreting a deep neural network model and explaining its predictions. It is based on a tutorial given at ICASSP 2017. It introduces some recently proposed techniques of interpretation, along with theory, tricks and recommendations, to make most efficient use of these techniques on real data. It also discusses a number of practical applications.},
	urldate = {2022-11-24},
	journal = {Digital Signal Processing},
	author = {Montavon, Grégoire and Samek, Wojciech and Müller, Klaus-Robert},
	month = feb,
	year = {2018},
	note = {arXiv:1706.07979 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {1--15},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/R96FQEWE/Montavon et al. - 2018 - Methods for Interpreting and Understanding Deep Ne.pdf:application/pdf},
}

@misc{miller_explainable_2017,
	title = {Explainable {AI}: {Beware} of {Inmates} {Running} the {Asylum} {Or}: {How} {I} {Learnt} to {Stop} {Worrying} and {Love} the {Social} and {Behavioural} {Sciences}},
	shorttitle = {Explainable {AI}},
	url = {http://arxiv.org/abs/1712.00547},
	doi = {10.48550/arXiv.1712.00547},
	abstract = {In his seminal book `The Inmates are Running the Asylum: Why High-Tech Products Drive Us Crazy And How To Restore The Sanity' [2004, Sams Indianapolis, IN, USA], Alan Cooper argues that a major reason why software is often poorly designed (from a user perspective) is that programmers are in charge of design decisions, rather than interaction designers. As a result, programmers design software for themselves, rather than for their target audience, a phenomenon he refers to as the `inmates running the asylum'. This paper argues that explainable AI risks a similar fate. While the re-emergence of explainable AI is positive, this paper argues most of us as AI researchers are building explanatory agents for ourselves, rather than for the intended users. But explainable AI is more likely to succeed if researchers and practitioners understand, adopt, implement, and improve models from the vast and valuable bodies of research in philosophy, psychology, and cognitive science, and if evaluation of these models is focused more on people than on technology. From a light scan of literature, we demonstrate that there is considerable scope to infuse more results from the social and behavioural sciences into explainable AI, and present some key results from these fields that are relevant to explainable AI.},
	urldate = {2022-11-24},
	publisher = {arXiv},
	author = {Miller, Tim and Howe, Piers and Sonenberg, Liz},
	month = dec,
	year = {2017},
	note = {arXiv:1712.00547 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/9W8R7Q8N/Miller et al. - 2017 - Explainable AI Beware of Inmates Running the Asyl.pdf:application/pdf},
}

@misc{adebayo_sanity_2020,
	title = {Sanity {Checks} for {Saliency} {Maps}},
	url = {http://arxiv.org/abs/1810.03292},
	doi = {10.48550/arXiv.1810.03292},
	abstract = {Saliency methods have emerged as a popular tool to highlight features in an input deemed relevant for the prediction of a learned model. Several saliency methods have been proposed, often guided by visual appeal on image data. In this work, we propose an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide. We find that reliance, solely, on visual assessment can be misleading. Through extensive experiments we show that some existing saliency methods are independent both of the model and of the data generating process. Consequently, methods that fail the proposed tests are inadequate for tasks that are sensitive to either data or model, such as, finding outliers in the data, explaining the relationship between inputs and outputs that the model learned, and debugging the model. We interpret our findings through an analogy with edge detection in images, a technique that requires neither training data nor model. Theory in the case of a linear model and a single-layer convolutional neural network supports our experimental findings.},
	urldate = {2022-11-24},
	publisher = {arXiv},
	author = {Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian and Hardt, Moritz and Kim, Been},
	month = nov,
	year = {2020},
	note = {arXiv:1810.03292 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/6M6QZEGQ/Adebayo et al. - 2020 - Sanity Checks for Saliency Maps.pdf:application/pdf;Full Text:/Users/eragon/Zotero/storage/RSBEVLMW/Adebayo et al. - 2020 - Sanity Checks for Saliency Maps.pdf:application/pdf},
}

@misc{gilpin_explaining_2019,
	title = {Explaining {Explanations}: {An} {Overview} of {Interpretability} of {Machine} {Learning}},
	shorttitle = {Explaining {Explanations}},
	url = {http://arxiv.org/abs/1806.00069},
	doi = {10.48550/arXiv.1806.00069},
	abstract = {There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we provide our definition of explainability and show how it can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.},
	urldate = {2022-11-24},
	publisher = {arXiv},
	author = {Gilpin, Leilani H. and Bau, David and Yuan, Ben Z. and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
	month = feb,
	year = {2019},
	note = {arXiv:1806.00069 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/MZWDDSBU/Gilpin et al. - 2019 - Explaining Explanations An Overview of Interpreta.pdf:application/pdf},
}

@misc{arrieta_explainable_2019,
	title = {Explainable {Artificial} {Intelligence} ({XAI}): {Concepts}, {Taxonomies}, {Opportunities} and {Challenges} toward {Responsible} {AI}},
	shorttitle = {Explainable {Artificial} {Intelligence} ({XAI})},
	url = {http://arxiv.org/abs/1910.10045},
	doi = {10.48550/arXiv.1910.10045},
	abstract = {In the last years, Artificial Intelligence (AI) has achieved a notable momentum that may deliver the best of expectations over many application sectors across the field. For this to occur, the entire community stands in front of the barrier of explainability, an inherent problem of AI techniques brought by sub-symbolism (e.g. ensembles or Deep Neural Networks) that were not present in the last hype of AI. Paradigms underlying this problem fall within the so-called eXplainable AI (XAI) field, which is acknowledged as a crucial feature for the practical deployment of AI models. This overview examines the existing literature in the field of XAI, including a prospect toward what is yet to be reached. We summarize previous efforts to define explainability in Machine Learning, establishing a novel definition that covers prior conceptual propositions with a major focus on the audience for which explainability is sought. We then propose and discuss about a taxonomy of recent contributions related to the explainability of different Machine Learning models, including those aimed at Deep Learning methods for which a second taxonomy is built. This literature analysis serves as the background for a series of challenges faced by XAI, such as the crossroads between data fusion and explainability. Our prospects lead toward the concept of Responsible Artificial Intelligence, namely, a methodology for the large-scale implementation of AI methods in real organizations with fairness, model explainability and accountability at its core. Our ultimate goal is to provide newcomers to XAI with a reference material in order to stimulate future research advances, but also to encourage experts and professionals from other disciplines to embrace the benefits of AI in their activity sectors, without any prior bias for its lack of interpretability.},
	urldate = {2022-11-24},
	publisher = {arXiv},
	author = {Arrieta, Alejandro Barredo and Díaz-Rodríguez, Natalia and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and Barbado, Alberto and García, Salvador and Gil-López, Sergio and Molina, Daniel and Benjamins, Richard and Chatila, Raja and Herrera, Francisco},
	month = dec,
	year = {2019},
	note = {arXiv:1910.10045 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/24ZS9T3Q/Arrieta et al. - 2019 - Explainable Artificial Intelligence (XAI) Concept.pdf:application/pdf;Full Text:/Users/eragon/Zotero/storage/DPZTMKX5/Arrieta et al. - 2019 - Explainable Artificial Intelligence (XAI) Concept.pdf:application/pdf},
}

@misc{mueller_explanation_2019,
	title = {Explanation in {Human}-{AI} {Systems}: {A} {Literature} {Meta}-{Review}, {Synopsis} of {Key} {Ideas} and {Publications}, and {Bibliography} for {Explainable} {AI}},
	shorttitle = {Explanation in {Human}-{AI} {Systems}},
	url = {http://arxiv.org/abs/1902.01876},
	doi = {10.48550/arXiv.1902.01876},
	abstract = {This is an integrative review that address the question, "What makes for a good explanation?" with reference to AI systems. Pertinent literatures are vast. Thus, this review is necessarily selective. That said, most of the key concepts and issues are expressed in this Report. The Report encapsulates the history of computer science efforts to create systems that explain and instruct (intelligent tutoring systems and expert systems). The Report expresses the explainability issues and challenges in modern AI, and presents capsule views of the leading psychological theories of explanation. Certain articles stand out by virtue of their particular relevance to XAI, and their methods, results, and key points are highlighted. It is recommended that AI/XAI researchers be encouraged to include in their research reports fuller details on their empirical or experimental methods, in the fashion of experimental psychology research reports: details on Participants, Instructions, Procedures, Tasks, Dependent Variables (operational definitions of the measures and metrics), Independent Variables (conditions), and Control Conditions.},
	urldate = {2022-11-24},
	publisher = {arXiv},
	author = {Mueller, Shane T. and Hoffman, Robert R. and Clancey, William and Emrey, Abigail and Klein, Gary},
	month = feb,
	year = {2019},
	note = {arXiv:1902.01876 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/6X7WSPJM/Mueller et al. - 2019 - Explanation in Human-AI Systems A Literature Meta.pdf:application/pdf;Full Text:/Users/eragon/Zotero/storage/YS246RSU/Mueller et al. - 2019 - Explanation in Human-AI Systems A Literature Meta.pdf:application/pdf},
}

@misc{dombrowski_explanations_2019,
	title = {Explanations can be manipulated and geometry is to blame},
	url = {http://arxiv.org/abs/1906.07983},
	doi = {10.48550/arXiv.1906.07983},
	abstract = {Explanation methods aim to make neural networks more trustworthy and interpretable. In this paper, we demonstrate a property of explanation methods which is disconcerting for both of these purposes. Namely, we show that explanations can be manipulated arbitrarily by applying visually hardly perceptible perturbations to the input that keep the network's output approximately constant. We establish theoretically that this phenomenon can be related to certain geometrical properties of neural networks. This allows us to derive an upper bound on the susceptibility of explanations to manipulations. Based on this result, we propose effective mechanisms to enhance the robustness of explanations.},
	urldate = {2022-11-24},
	publisher = {arXiv},
	author = {Dombrowski, Ann-Kathrin and Alber, Maximilian and Anders, Christopher J. and Ackermann, Marcel and Müller, Klaus-Robert and Kessel, Pan},
	month = sep,
	year = {2019},
	note = {arXiv:1906.07983 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Cryptography and Security},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/9FUQAKVN/Dombrowski et al. - 2019 - Explanations can be manipulated and geometry is to.pdf:application/pdf;Full Text:/Users/eragon/Zotero/storage/YI5IAMS5/Dombrowski et al. - 2019 - Explanations can be manipulated and geometry is to.pdf:application/pdf},
}

@misc{mudrakarta_did_2018,
	title = {Did the {Model} {Understand} the {Question}?},
	url = {http://arxiv.org/abs/1805.05492},
	doi = {10.48550/arXiv.1805.05492},
	abstract = {We analyze state-of-the-art deep learning models for three tasks: question answering on (1) images, (2) tables, and (3) passages of text. Using the notion of {\textbackslash}emph\{attribution\} (word importance), we find that these deep networks often ignore important question terms. Leveraging such behavior, we perturb questions to craft a variety of adversarial examples. Our strongest attacks drop the accuracy of a visual question answering model from \$61.1{\textbackslash}\%\$ to \$19{\textbackslash}\%\$, and that of a tabular question answering model from \$33.5{\textbackslash}\%\$ to \$3.3{\textbackslash}\%\$. Additionally, we show how attributions can strengthen attacks proposed by Jia and Liang (2017) on paragraph comprehension models. Our results demonstrate that attributions can augment standard measures of accuracy and empower investigation of model performance. When a model is accurate but for the wrong reasons, attributions can surface erroneous logic in the model that indicates inadequacies in the test data.},
	urldate = {2022-11-28},
	publisher = {arXiv},
	author = {Mudrakarta, Pramod Kaushik and Taly, Ankur and Sundararajan, Mukund and Dhamdhere, Kedar},
	month = may,
	year = {2018},
	note = {arXiv:1805.05492 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/87TE479A/Mudrakarta et al. - 2018 - Did the Model Understand the Question.pdf:application/pdf},
}

@misc{smilkov_smoothgrad_2017,
	title = {{SmoothGrad}: removing noise by adding noise},
	shorttitle = {{SmoothGrad}},
	url = {http://arxiv.org/abs/1706.03825},
	doi = {10.48550/arXiv.1706.03825},
	abstract = {Explaining the output of a deep network remains a challenge. In the case of an image classifier, one type of explanation is to identify pixels that strongly influence the final decision. A starting point for this strategy is the gradient of the class score function with respect to the input image. This gradient can be interpreted as a sensitivity map, and there are several techniques that elaborate on this basic idea. This paper makes two contributions: it introduces SmoothGrad, a simple method that can help visually sharpen gradient-based sensitivity maps, and it discusses lessons in the visualization of these maps. We publish the code for our experiments and a website with our results.},
	urldate = {2022-11-28},
	publisher = {arXiv},
	author = {Smilkov, Daniel and Thorat, Nikhil and Kim, Been and Viégas, Fernanda and Wattenberg, Martin},
	month = jun,
	year = {2017},
	note = {arXiv:1706.03825 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/YCR6ABCK/Smilkov et al. - 2017 - SmoothGrad removing noise by adding noise.pdf:application/pdf},
}

@misc{adebayo_sanity_2020-1,
	title = {Sanity {Checks} for {Saliency} {Maps}},
	url = {http://arxiv.org/abs/1810.03292},
	doi = {10.48550/arXiv.1810.03292},
	abstract = {Saliency methods have emerged as a popular tool to highlight features in an input deemed relevant for the prediction of a learned model. Several saliency methods have been proposed, often guided by visual appeal on image data. In this work, we propose an actionable methodology to evaluate what kinds of explanations a given method can and cannot provide. We find that reliance, solely, on visual assessment can be misleading. Through extensive experiments we show that some existing saliency methods are independent both of the model and of the data generating process. Consequently, methods that fail the proposed tests are inadequate for tasks that are sensitive to either data or model, such as, finding outliers in the data, explaining the relationship between inputs and outputs that the model learned, and debugging the model. We interpret our findings through an analogy with edge detection in images, a technique that requires neither training data nor model. Theory in the case of a linear model and a single-layer convolutional neural network supports our experimental findings.},
	urldate = {2022-11-28},
	publisher = {arXiv},
	author = {Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian and Hardt, Moritz and Kim, Been},
	month = nov,
	year = {2020},
	note = {arXiv:1810.03292 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/DJDSS9U5/Adebayo et al. - 2020 - Sanity Checks for Saliency Maps.pdf:application/pdf},
}

@misc{dhamdhere_how_2018,
	title = {How {Important} {Is} a {Neuron}?},
	url = {http://arxiv.org/abs/1805.12233},
	doi = {10.48550/arXiv.1805.12233},
	abstract = {The problem of attributing a deep network's prediction to its {\textbackslash}emph\{input/base\} features is well-studied. We introduce the notion of {\textbackslash}emph\{conductance\} to extend the notion of attribution to the understanding the importance of {\textbackslash}emph\{hidden\} units. Informally, the conductance of a hidden unit of a deep network is the {\textbackslash}emph\{flow\} of attribution via this hidden unit. We use conductance to understand the importance of a hidden unit to the prediction for a specific input, or over a set of inputs. We evaluate the effectiveness of conductance in multiple ways, including theoretical properties, ablation studies, and a feature selection task. The empirical evaluations are done using the Inception network over ImageNet data, and a sentiment analysis network over reviews. In both cases, we demonstrate the effectiveness of conductance in identifying interesting insights about the internal workings of these networks.},
	urldate = {2022-11-28},
	publisher = {arXiv},
	author = {Dhamdhere, Kedar and Sundararajan, Mukund and Yan, Qiqi},
	month = may,
	year = {2018},
	note = {arXiv:1805.12233 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/RBRVP5KX/Dhamdhere et al. - 2018 - How Important Is a Neuron.pdf:application/pdf},
}

@misc{shrikumar_computationally_2018,
	title = {Computationally {Efficient} {Measures} of {Internal} {Neuron} {Importance}},
	url = {http://arxiv.org/abs/1807.09946},
	doi = {10.48550/arXiv.1807.09946},
	abstract = {The challenge of assigning importance to individual neurons in a network is of interest when interpreting deep learning models. In recent work, Dhamdhere et al. proposed Total Conductance, a "natural refinement of Integrated Gradients" for attributing importance to internal neurons. Unfortunately, the authors found that calculating conductance in tensorflow required the addition of several custom gradient operators and did not scale well. In this work, we show that the formula for Total Conductance is mathematically equivalent to Path Integrated Gradients computed on a hidden layer in the network. We provide a scalable implementation of Total Conductance using standard tensorflow gradient operators that we call Neuron Integrated Gradients. We compare Neuron Integrated Gradients to DeepLIFT, a pre-existing computationally efficient approach that is applicable to calculating internal neuron importance. We find that DeepLIFT produces strong empirical results and is faster to compute, but because it lacks the theoretical properties of Neuron Integrated Gradients, it may not always be preferred in practice. Colab notebook reproducing results: http://bit.ly/neuronintegratedgradients},
	urldate = {2022-11-28},
	publisher = {arXiv},
	author = {Shrikumar, Avanti and Su, Jocelin and Kundaje, Anshul},
	month = jul,
	year = {2018},
	note = {arXiv:1807.09946 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/MZVID3Y2/Shrikumar et al. - 2018 - Computationally Efficient Measures of Internal Neu.pdf:application/pdf},
}

@misc{shrikumar_learning_2019,
	title = {Learning {Important} {Features} {Through} {Propagating} {Activation} {Differences}},
	url = {http://arxiv.org/abs/1704.02685},
	doi = {10.48550/arXiv.1704.02685},
	abstract = {The purported "black box" nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. Video tutorial: http://goo.gl/qKb7pL, ICML slides: bit.ly/deeplifticmlslides, ICML talk: https://vimeo.com/238275076, code: http://goo.gl/RM8jvH.},
	urldate = {2022-11-28},
	publisher = {arXiv},
	author = {Shrikumar, Avanti and Greenside, Peyton and Kundaje, Anshul},
	month = oct,
	year = {2019},
	note = {arXiv:1704.02685 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/CSAZYN2B/Shrikumar et al. - 2019 - Learning Important Features Through Propagating Ac.pdf:application/pdf},
}

@article{ancona_towards_2018,
	title = {{TOWARDS} {BETTER} {UNDERSTANDING} {OF} {GRADIENT}-{BASED} {ATTRIBUTION} {METHODS} {FOR} {DEEP} {NEURAL} {NETWORKS}},
	abstract = {Understanding the ﬂow of information in Deep Neural Networks (DNNs) is a challenging problem that has gain increasing attention over the last few years. While several methods have been proposed to explain network predictions, there have been only a few attempts to compare them from a theoretical perspective. What is more, no exhaustive empirical comparison has been performed in the past. In this work, we analyze four gradient-based attribution methods and formally prove conditions of equivalence and approximation between them. By reformulating two of these methods, we construct a uniﬁed framework which enables a direct comparison, as well as an easier implementation. Finally, we propose a novel evaluation metric, called Sensitivity-n and test the gradient-based attribution methods alongside with a simple perturbation-based attribution method on several datasets in the domains of image and text classiﬁcation, using various network architectures.},
	language = {en},
	author = {Ancona, Marco and Öztireli, Cengiz and Ceolini, Enea and Gross, Markus},
	year = {2018},
	pages = {16},
	file = {Ancona et al. - 2018 - TOWARDS BETTER UNDERSTANDING OF GRADIENT-BASED ATT.pdf:/Users/eragon/Zotero/storage/FHL5SMNW/Ancona et al. - 2018 - TOWARDS BETTER UNDERSTANDING OF GRADIENT-BASED ATT.pdf:application/pdf},
}

@misc{shrikumar_computationally_2018-1,
	title = {Computationally {Efficient} {Measures} of {Internal} {Neuron} {Importance}},
	url = {http://arxiv.org/abs/1807.09946},
	doi = {10.48550/arXiv.1807.09946},
	abstract = {The challenge of assigning importance to individual neurons in a network is of interest when interpreting deep learning models. In recent work, Dhamdhere et al. proposed Total Conductance, a "natural refinement of Integrated Gradients" for attributing importance to internal neurons. Unfortunately, the authors found that calculating conductance in tensorflow required the addition of several custom gradient operators and did not scale well. In this work, we show that the formula for Total Conductance is mathematically equivalent to Path Integrated Gradients computed on a hidden layer in the network. We provide a scalable implementation of Total Conductance using standard tensorflow gradient operators that we call Neuron Integrated Gradients. We compare Neuron Integrated Gradients to DeepLIFT, a pre-existing computationally efficient approach that is applicable to calculating internal neuron importance. We find that DeepLIFT produces strong empirical results and is faster to compute, but because it lacks the theoretical properties of Neuron Integrated Gradients, it may not always be preferred in practice. Colab notebook reproducing results: http://bit.ly/neuronintegratedgradients},
	urldate = {2022-11-28},
	publisher = {arXiv},
	author = {Shrikumar, Avanti and Su, Jocelin and Kundaje, Anshul},
	month = jul,
	year = {2018},
	note = {arXiv:1807.09946 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/535IFUMD/Shrikumar et al. - 2018 - Computationally Efficient Measures of Internal Neu.pdf:application/pdf},
}

@inproceedings{lundberg_unified_2017,
	title = {A {Unified} {Approach} to {Interpreting} {Model} {Predictions}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html},
	abstract = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
	urldate = {2022-11-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Lundberg, Scott M and Lee, Su-In},
	year = {2017},
	file = {Full Text PDF:/Users/eragon/Zotero/storage/H2QC8KUK/Lundberg and Lee - 2017 - A Unified Approach to Interpreting Model Predictio.pdf:application/pdf},
}

@misc{leino_influence-directed_2018,
	title = {Influence-{Directed} {Explanations} for {Deep} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1802.03788},
	doi = {10.48550/arXiv.1802.03788},
	abstract = {We study the problem of explaining a rich class of behavioral properties of deep neural networks. Distinctively, our influence-directed explanations approach this problem by peering inside the network to identify neurons with high influence on a quantity and distribution of interest, using an axiomatically-justified influence measure, and then providing an interpretation for the concepts these neurons represent. We evaluate our approach by demonstrating a number of its unique capabilities on convolutional neural networks trained on ImageNet. Our evaluation demonstrates that influence-directed explanations (1) identify influential concepts that generalize across instances, (2) can be used to extract the "essence" of what the network learned about a class, and (3) isolate individual features the network uses to make decisions and distinguish related classes.},
	urldate = {2022-11-28},
	publisher = {arXiv},
	author = {Leino, Klas and Sen, Shayak and Datta, Anupam and Fredrikson, Matt and Li, Linyi},
	month = nov,
	year = {2018},
	note = {arXiv:1802.03788 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/KSZYTVZ2/Leino et al. - 2018 - Influence-Directed Explanations for Deep Convoluti.pdf:application/pdf},
}

@misc{simonyan_deep_2014-1,
	title = {Deep {Inside} {Convolutional} {Networks}: {Visualising} {Image} {Classification} {Models} and {Saliency} {Maps}},
	shorttitle = {Deep {Inside} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1312.6034},
	doi = {10.48550/arXiv.1312.6034},
	abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].},
	urldate = {2022-11-28},
	publisher = {arXiv},
	author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
	month = apr,
	year = {2014},
	note = {arXiv:1312.6034 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/PXVDTL4R/Simonyan et al. - 2014 - Deep Inside Convolutional Networks Visualising Im.pdf:application/pdf},
}

@misc{zeiler_visualizing_2013,
	title = {Visualizing and {Understanding} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1311.2901},
	doi = {10.48550/arXiv.1311.2901},
	abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky {\textbackslash}etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
	urldate = {2022-11-28},
	publisher = {arXiv},
	author = {Zeiler, Matthew D. and Fergus, Rob},
	month = nov,
	year = {2013},
	note = {arXiv:1311.2901 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/NUUQDELD/Zeiler and Fergus - 2013 - Visualizing and Understanding Convolutional Networ.pdf:application/pdf;Full Text:/Users/eragon/Zotero/storage/FJXDGAVP/Zeiler and Fergus - 2013 - Visualizing and Understanding Convolutional Networ.pdf:application/pdf},
}

@misc{zeiler_visualizing_2013-1,
	title = {Visualizing and {Understanding} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1311.2901},
	doi = {10.48550/arXiv.1311.2901},
	abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky {\textbackslash}etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
	urldate = {2022-11-28},
	publisher = {arXiv},
	author = {Zeiler, Matthew D. and Fergus, Rob},
	month = nov,
	year = {2013},
	note = {arXiv:1311.2901 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/TCYYQIJB/Zeiler and Fergus - 2013 - Visualizing and Understanding Convolutional Networ.pdf:application/pdf;Full Text:/Users/eragon/Zotero/storage/G7Z6NFNF/Zeiler and Fergus - 2013 - Visualizing and Understanding Convolutional Networ.pdf:application/pdf},
}

@misc{yeh_fidelity_2019,
	title = {On the ({In})fidelity and {Sensitivity} for {Explanations}},
	url = {http://arxiv.org/abs/1901.09392},
	doi = {10.48550/arXiv.1901.09392},
	abstract = {We consider objective evaluation measures of saliency explanations for complex black-box machine learning models. We propose simple robust variants of two notions that have been considered in recent literature: (in)fidelity, and sensitivity. We analyze optimal explanations with respect to both these measures, and while the optimal explanation for sensitivity is a vacuous constant explanation, the optimal explanation for infidelity is a novel combination of two popular explanation methods. By varying the perturbation distribution that defines infidelity, we obtain novel explanations by optimizing infidelity, which we show to out-perform existing explanations in both quantitative and qualitative measurements. Another salient question given these measures is how to modify any given explanation to have better values with respect to these measures. We propose a simple modification based on lowering sensitivity, and moreover show that when done appropriately, we could simultaneously improve both sensitivity as well as fidelity.},
	urldate = {2022-11-28},
	publisher = {arXiv},
	author = {Yeh, Chih-Kuan and Hsieh, Cheng-Yu and Suggala, Arun Sai and Inouye, David I. and Ravikumar, Pradeep},
	month = nov,
	year = {2019},
	note = {arXiv:1901.09392 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/PEVB52S7/Yeh et al. - 2019 - On the (In)fidelity and Sensitivity for Explanatio.pdf:application/pdf},
}

@article{eppel_classifying_nodate,
	title = {Classifying a specific image region using convolutional nets with an {ROI} mask as input},
	abstract = {Convolutional neural nets (CNN) are the leading computer vision method for classifying images. In some cases, it is desirable to classify only a specific region of the image that corresponds to a certain object. Hence, assuming that the region of the object in the image is known in advance and is given as a binary region of interest (ROI) mask, the goal is to classify the object in this region using a convolutional neural net. This goal is achieved using a standard image classification net with the addition of a side branch, which converts the ROI mask into an attention map. This map is then combined with the image classification net. This allows the net to focus the attention on the object region while still extracting contextual cues from the background. This approach was evaluated using the COCO object dataset and the OpenSurfaces materials dataset. In both cases, it gave superior results to methods that completely ignore the background region. In addition, it was found that combining the attention map at the first layer of the net gave better results than combining it at higher layers of the net. The advantages of this method are most apparent in the classification of small regions which demands a great deal of contextual information from the background.},
	language = {en},
	author = {Eppel, Sagi},
	keywords = {done},
	pages = {8},
}

@techreport{mitsuhara_embedding_2019,
	title = {Embedding {Human} {Knowledge} into {Deep} {Neural} {Network} via {Attention} {Map}},
	url = {http://arxiv.org/abs/1905.03540},
	abstract = {In this work, we aim to realize a method for embedding human knowledge into deep neural networks. While the conventional method to embed human knowledge has been applied for non-deep machine learning, it is challenging to apply it for deep learning models due to the enormous number of model parameters. To tackle this problem, we focus on the attention mechanism of an attention branch network (ABN). In this paper, we propose a fine-tuning method that utilizes a single-channel attention map which is manually edited by a human expert. Our fine-tuning method can train a network so that the output attention map corresponds to the edited ones. As a result, the fine-tuned network can output an attention map that takes into account human knowledge. Experimental results with ImageNet, CUB-200-2010, and IDRiD demonstrate that it is possible to obtain a clear attention map for a visual explanation and improve the classification performance. Our findings can be a novel framework for optimizing networks through human intuitive editing via a visual interface and suggest new possibilities for human-machine cooperation in addition to the improvement of visual explanations.},
	number = {arXiv:1905.03540},
	urldate = {2022-10-03},
	institution = {arXiv},
	author = {Mitsuhara, Masahiro and Fukui, Hiroshi and Sakashita, Yusuke and Ogata, Takanori and Hirakawa, Tsubasa and Yamashita, Takayoshi and Fujiyoshi, Hironobu},
	month = dec,
	year = {2019},
	note = {arXiv:1905.03540 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, done},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/2SVPKSTS/Mitsuhara et al. - 2019 - Embedding Human Knowledge into Deep Neural Network.pdf:application/pdf},
}

@techreport{lan_couplformerrethinking_2021,
	title = {Couplformer:{Rethinking} {Vision} {Transformer} with {Coupling} {Attention} {Map}},
	shorttitle = {Couplformer},
	url = {http://arxiv.org/abs/2112.05425},
	abstract = {With the development of the self-attention mechanism, the Transformer model has demonstrated its outstanding performance in the computer vision domain. However, the massive computation brought from the full attention mechanism became a heavy burden for memory consumption. Sequentially, the limitation of memory reduces the possibility of improving the Transformer model. To remedy this problem, we propose a novel memory economy attention mechanism named Couplformer, which decouples the attention map into two sub-matrices and generates the alignment scores from spatial information. A series of different scale image classification tasks are applied to evaluate the effectiveness of our model. The result of experiments shows that on the ImageNet-1k classification task, the Couplformer can significantly decrease 28\% memory consumption compared with regular Transformer while accessing sufficient accuracy requirements and outperforming 0.92\% on Top-1 accuracy while occupying the same memory footprint. As a result, the Couplformer can serve as an efficient backbone in visual tasks, and provide a novel perspective on the attention mechanism for researchers.},
	number = {arXiv:2112.05425},
	urldate = {2022-10-03},
	institution = {arXiv},
	author = {Lan, Hai and Wang, Xihao and Wei, Xian},
	month = dec,
	year = {2021},
	note = {arXiv:2112.05425 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/FB5UAR58/Lan et al. - 2021 - CouplformerRethinking Vision Transformer with Cou.pdf:application/pdf},
}

@inproceedings{bello_attention_2019,
	address = {Seoul, Korea (South)},
	title = {Attention {Augmented} {Convolutional} {Networks}},
	isbn = {978-1-72814-803-8},
	url = {https://ieeexplore.ieee.org/document/9010285/},
	doi = {10.1109/ICCV.2019.00338},
	abstract = {Convolutional networks have been the paradigm of choice in many computer vision applications. The convolution operation however has a signiﬁcant weakness in that it only operates on a local neighborhood, thus missing global information. Self-attention, on the other hand, has emerged as a recent advance to capture long range interactions, but has mostly been applied to sequence modeling and generative modeling tasks. In this paper, we consider the use of self-attention for discriminative visual tasks as an alternative to convolutions. We introduce a novel two-dimensional relative self-attention mechanism that proves competitive in replacing convolutions as a stand-alone computational primitive for image classiﬁcation. We ﬁnd in control experiments that the best results are obtained when combining both convolutions and self-attention. We therefore propose to augment convolutional operators with this self-attention mechanism by concatenating convolutional feature maps with a set of feature maps produced via self-attention. Extensive experiments show that Attention Augmentation leads to consistent improvements in image classiﬁcation on ImageNet and object detection on COCO across many different models and scales, including ResNets and a stateof-the art mobile constrained network, while keeping the number of parameters similar. In particular, our method achieves a 1.3\% top-1 accuracy improvement on ImageNet classiﬁcation over a ResNet50 baseline and outperforms other attention mechanisms for images such as Squeezeand-Excitation [17]. It also achieves an improvement of 1.4 mAP in COCO Object Detection on top of a RetinaNet baseline.},
	language = {en},
	urldate = {2022-10-03},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Bello, Irwan and Zoph, Barret and Le, Quoc and Vaswani, Ashish and Shlens, Jonathon},
	month = oct,
	year = {2019},
	pages = {3285--3294},
	file = {Bello et al. - 2019 - Attention Augmented Convolutional Networks.pdf:/Users/eragon/Zotero/storage/WENTJ9EM/Bello et al. - 2019 - Attention Augmented Convolutional Networks.pdf:application/pdf},
}

@article{oyama_influence_2018,
	title = {Influence of image classification accuracy on saliency map estimation},
	volume = {3},
	issn = {2468-2322},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/trit.2018.1012},
	doi = {10.1049/trit.2018.1012},
	abstract = {Saliency map estimation in computer vision aims to estimate the locations where people gaze in images. Since people tend to look at objects in images, the parameters of the model pre-trained on ImageNet for image classification are useful for the saliency map estimation. However, there is no research on the relationship between the image classification accuracy and the performance of the saliency map estimation. In this study, it is shown that there is a strong correlation between image classification accuracy and saliency map estimation accuracy. The authors also investigated the effective architecture based on multi-scale images and the up-sampling layers to refine the saliency-map resolution. The model achieved the state-of-the-art accuracy on the PASCAL-S, OSIE, and MIT1003 datasets. In the MIT saliency benchmark, the model achieved the best performance in some metrics and competitive results in the other metrics.},
	language = {en},
	number = {3},
	urldate = {2022-10-03},
	journal = {CAAI Transactions on Intelligence Technology},
	author = {Oyama, Taiki and Yamanaka, Takao},
	year = {2018},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1049/trit.2018.1012},
	keywords = {computer vision, (B6135) Optical, (C5260B) Computer vision and image processing techniques, image and video signal processing, image classification, ImageNet, MIT1003, multiscale images, OSIE, PASCAL-S, saliency-map resolution, up-sampling layer},
	pages = {140--152},
	file = {Full Text PDF:/Users/eragon/Zotero/storage/HHL5L9G9/Oyama and Yamanaka - 2018 - Influence of image classification accuracy on sali.pdf:application/pdf},
}

@article{selvaraju_grad-cam_nodate,
	title = {Grad-{CAM}: {Visual} {Explanations} {From} {Deep} {Networks} via {Gradient}-{Based} {Localization}},
	abstract = {We propose a technique for producing ‘visual explanations’ for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent. Our approach – Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say logits for ‘dog’ or even a caption), ﬂowing into the ﬁnal convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, GradCAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in tasks with multi-modal inputs (e.g. visual question answering) or reinforcement learning, without architectural changes or re-training. We combine Grad-CAM with existing ﬁne-grained visualizations to create a high-resolution class-discriminative visualization, Guided Grad-CAM, and apply it to image classiﬁcation, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classiﬁcation models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) outperform previous methods on the ILSVRC-15 weakly-supervised localization task, (c) are more faithful to the underlying model, and (d) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visualizations show even non-attention based models can localize inputs. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that Grad-CAM helps untrained users successfully discern a ‘stronger’ deep network from a ‘weaker’ one even when both make identical predictions. Our code is available at https: //github.com/ramprs/grad-cam/ along with a demo on CloudCV [2]1 and video at youtu.be/COjUB9Izk6E.},
	language = {en},
	author = {Selvaraju, Ramprasaath R and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
	pages = {9},
	file = {Selvaraju et al. - Grad-CAM Visual Explanations From Deep Networks v.pdf:/Users/eragon/Zotero/storage/EDVF5SE9/Selvaraju et al. - Grad-CAM Visual Explanations From Deep Networks v.pdf:application/pdf},
}

@inproceedings{chakraborty_generalizing_2022,
	address = {New Orleans, LA, USA},
	title = {Generalizing {Adversarial} {Explanations} with {Grad}-{CAM}},
	isbn = {978-1-66548-739-9},
	url = {https://ieeexplore.ieee.org/document/9857321/},
	doi = {10.1109/CVPRW56347.2022.00031},
	abstract = {Gradient-weighted Class Activation Mapping (GradCAM), is an example-based explanation method that provides a gradient activation heat map as an explanation for Convolution Neural Network (CNN) models. The drawback of this method is that it cannot be used to generalize CNN behaviour. In this paper, we present a novel method that extends Grad-CAM from example-based explanations to a method for explaining global model behaviour. This is achieved by introducing two new metrics, (i) Mean Observed Dissimilarity (MOD) and (ii) Variation in Dissimilarity (VID), for model generalization. These metrics are computed by comparing a Normalized Inverted Structural Similarity Index (NISSIM) metric of the Grad-CAM generated heatmap for samples from the original test set and samples from the adversarial test set. For our experiment, we study adversarial attacks on deep models such as VGG16, ResNet50, and ResNet101, and wide models such as InceptionNetv3 and XceptionNet using Fast Gradient Sign Method (FGSM). We then compute the metrics MOD and VID for the automatic face recognition (AFR) use case with the VGGFace2 dataset. We observe a consistent shift in the region highlighted in the Grad-CAM heatmap, reflecting its participation to the decision making, across all models under adversarial attacks. The proposed method can be used to understand adversarial attacks and explain the behaviour of black box CNN models for image analysis.},
	language = {en},
	urldate = {2022-10-03},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	publisher = {IEEE},
	author = {Chakraborty, Tanmay and Trehan, Utkarsh and Mallat, Khawla and Dugelay, Jean-Luc},
	month = jun,
	year = {2022},
	keywords = {read},
	pages = {186--192},
	file = {Chakraborty et al. - 2022 - Generalizing Adversarial Explanations with Grad-CA.pdf:/Users/eragon/Zotero/storage/6G98B28D/Chakraborty et al. - 2022 - Generalizing Adversarial Explanations with Grad-CA.pdf:application/pdf},
}

@inproceedings{chattopadhyay_grad-cam_2018,
	title = {Grad-{CAM}++: {Improved} {Visual} {Explanations} for {Deep} {Convolutional} {Networks}},
	shorttitle = {Grad-{CAM}++},
	url = {http://arxiv.org/abs/1710.11063},
	doi = {10.1109/WACV.2018.00097},
	abstract = {Over the last decade, Convolutional Neural Network (CNN) models have been highly successful in solving complex vision problems. However, these deep models are perceived as "black box" methods considering the lack of understanding of their internal functioning. There has been a significant recent interest in developing explainable deep learning models, and this paper is an effort in this direction. Building on a recently proposed method called Grad-CAM, we propose a generalized method called Grad-CAM++ that can provide better visual explanations of CNN model predictions, in terms of better object localization as well as explaining occurrences of multiple object instances in a single image, when compared to state-of-the-art. We provide a mathematical derivation for the proposed method, which uses a weighted combination of the positive partial derivatives of the last convolutional layer feature maps with respect to a specific class score as weights to generate a visual explanation for the corresponding class label. Our extensive experiments and evaluations, both subjective and objective, on standard datasets showed that Grad-CAM++ provides promising human-interpretable visual explanations for a given CNN architecture across multiple tasks including classification, image caption generation and 3D action recognition; as well as in new settings such as knowledge distillation.},
	urldate = {2022-10-03},
	booktitle = {2018 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	author = {Chattopadhyay, Aditya and Sarkar, Anirban and Howlader, Prantik and Balasubramanian, Vineeth N.},
	month = mar,
	year = {2018},
	note = {arXiv:1710.11063 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	pages = {839--847},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/V2RWR75J/Chattopadhyay et al. - 2018 - Grad-CAM++ Improved Visual Explanations for Deep .pdf:application/pdf},
}

@incollection{holzinger_overlap_2021,
	address = {Cham},
	title = {On the {Overlap} {Between} {Grad}-{CAM} {Saliency} {Maps} and {Explainable} {Visual} {Features} in {Skin} {Cancer} {Images}},
	volume = {12844},
	isbn = {978-3-030-84059-4 978-3-030-84060-0},
	url = {https://link.springer.com/10.1007/978-3-030-84060-0_16},
	abstract = {Dermatologists recognize melanomas by inspecting images in which they identify human-comprehensible visual features. In this paper, we investigate to what extent such features correspond to the saliency areas identiﬁed on CNNs trained for classiﬁcation. Our experiments, conducted on two neural architectures characterized by different depth and different resolution of the last convolutional layer, quantify to what extent thresholded Grad-CAM saliency maps can be used to identify visual features of skin cancer. We found that the best threshold value, i.e., the threshold at which we can measure the highest Jaccard index, varies signiﬁcantly among features; ranging from 0.3 to 0.7. In addition, we measured Jaccard indices as high as 0.143, which is almost 50\% of the performance of state-of-the-art architectures specialized in feature mask prediction at pixel-level, such as U-Net. Finally, a breakdown test between malignancy and classiﬁcation correctness shows that higher resolution saliency maps could help doctors in spotting wrong classiﬁcations.},
	language = {en},
	urldate = {2022-10-03},
	booktitle = {Machine {Learning} and {Knowledge} {Extraction}},
	publisher = {Springer International Publishing},
	author = {Nunnari, Fabrizio and Kadir, Md Abdul and Sonntag, Daniel},
	editor = {Holzinger, Andreas and Kieseberg, Peter and Tjoa, A Min and Weippl, Edgar},
	year = {2021},
	doi = {10.1007/978-3-030-84060-0_16},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {241--253},
	file = {Nunnari et al. - 2021 - On the Overlap Between Grad-CAM Saliency Maps and .pdf:/Users/eragon/Zotero/storage/33H2VKZ5/Nunnari et al. - 2021 - On the Overlap Between Grad-CAM Saliency Maps and .pdf:application/pdf},
}

@inproceedings{oquab_is_2015,
	address = {Boston, MA, USA},
	title = {Is object localization for free? - {Weakly}-supervised learning with convolutional neural networks},
	isbn = {978-1-4673-6964-0},
	shorttitle = {Is object localization for free?},
	url = {http://ieeexplore.ieee.org/document/7298668/},
	doi = {10.1109/CVPR.2015.7298668},
	abstract = {Successful methods for visual object recognition typically rely on training datasets containing lots of richly annotated images. Detailed image annotation, e.g. by object bounding boxes, however, is both expensive and often subjective. We describe a weakly supervised convolutional neural network (CNN) for object classiﬁcation that relies only on image-level labels, yet can learn from cluttered scenes containing multiple objects. We quantify its object classiﬁcation and object location prediction performance on the Pascal VOC 2012 (20 object classes) and the much larger Microsoft COCO (80 object classes) datasets. We ﬁnd that the network (i) outputs accurate image-level labels, (ii) predicts approximate locations (but not extents) of objects, and (iii) performs comparably to its fully-supervised counterparts using object bounding box annotation for training.},
	language = {en},
	urldate = {2022-10-21},
	booktitle = {2015 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Oquab, Maxime and Bottou, Leon and Laptev, Ivan and Sivic, Josef},
	month = jun,
	year = {2015},
	pages = {685--694},
	file = {Oquab et al. - 2015 - Is object localization for free - Weakly-supervis.pdf:/Users/eragon/Zotero/storage/G38P7ZRA/Oquab et al. - 2015 - Is object localization for free - Weakly-supervis.pdf:application/pdf},
}

@misc{lin_network_2014,
	title = {Network {In} {Network}},
	url = {http://arxiv.org/abs/1312.4400},
	abstract = {We propose a novel deep network structure called "Network In Network" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.},
	urldate = {2022-10-21},
	publisher = {arXiv},
	author = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
	month = mar,
	year = {2014},
	note = {arXiv:1312.4400 [cs]
version: 3},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/DNGUDZR4/Lin et al. - 2014 - Network In Network.pdf:application/pdf},
}

@article{zhong_random_2020,
	title = {Random {Erasing} {Data} {Augmentation}},
	volume = {34},
	issn = {2374-3468, 2159-5399},
	url = {https://aaai.org/ojs/index.php/AAAI/article/view/7000},
	doi = {10.1609/aaai.v34i07.7000},
	abstract = {In this paper, we introduce Random Erasing, a new data augmentation method for training the convolutional neural network (CNN). In training, Random Erasing randomly selects a rectangle region in an image and erases its pixels with random values. In this process, training images with various levels of occlusion are generated, which reduces the risk of over-ﬁtting and makes the model robust to occlusion. Random Erasing is parameter learning free, easy to implement, and can be integrated with most of the CNN-based recognition models. Albeit simple, Random Erasing is complementary to commonly used data augmentation techniques such as random cropping and ﬂipping, and yields consistent improvement over strong baselines in image classiﬁcation, object detection and person re-identiﬁcation. Code is available at: https://github.com/zhunzhong07/Random-Erasing.},
	language = {en},
	number = {07},
	urldate = {2022-10-21},
	journal = {AAAI},
	author = {Zhong, Zhun and Zheng, Liang and Kang, Guoliang and Li, Shaozi and Yang, Yi},
	month = apr,
	year = {2020},
	pages = {13001--13008},
	file = {Zhong et al. - 2020 - Random Erasing Data Augmentation.pdf:/Users/eragon/Zotero/storage/H2HNNUFX/Zhong et al. - 2020 - Random Erasing Data Augmentation.pdf:application/pdf},
}

@inproceedings{dvornik_modeling_2018,
	title = {Modeling {Visual} {Context} is {Key} to {Augmenting} {Object} {Detection} {Datasets}},
	url = {https://openaccess.thecvf.com/content_ECCV_2018/html/NIKITA_DVORNIK_Modeling_Visual_Context_ECCV_2018_paper.html},
	urldate = {2022-10-21},
	author = {Dvornik, Nikita and Mairal, Julien and Schmid, Cordelia},
	year = {2018},
	pages = {364--380},
	file = {Full Text PDF:/Users/eragon/Zotero/storage/FR4JCRWJ/Dvornik et al. - 2018 - Modeling Visual Context is Key to Augmenting Objec.pdf:application/pdf},
}

@misc{dvornik_importance_2019,
	title = {On the {Importance} of {Visual} {Context} for {Data} {Augmentation} in {Scene} {Understanding}},
	url = {http://arxiv.org/abs/1809.02492},
	abstract = {Performing data augmentation for learning deep neural networks is known to be important for training visual recognition systems. By artiﬁcially increasing the number of training examples, it helps reducing overﬁtting and improves generalization. While simple image transformations can already improve predictive performance in most vision tasks, larger gains can be obtained by leveraging task-speciﬁc prior knowledge. In this work, we consider object detection, semantic and instance segmentation and augment the training images by blending objects in existing scenes, using instance segmentation annotations. We observe that randomly pasting objects on images hurts the performance, unless the object is placed in the right context. To resolve this issue, we propose an explicit context model by using a convolutional neural network, which predicts whether an image region is suitable for placing a given object or not. In our experiments, we show that our approach is able to improve object detection, semantic and instance segmentation on the PASCAL VOC12 and COCO datasets, with signiﬁcant gains in a limited annotation scenario, i.e. when only one category is annotated. We also show that the method is not limited to datasets that come with expensive pixel-wise instance annotations and can be used when only bounding boxes are available, by employing weakly-supervised learning for instance masks approximation.},
	language = {en},
	urldate = {2022-10-21},
	publisher = {arXiv},
	author = {Dvornik, Nikita and Mairal, Julien and Schmid, Cordelia},
	month = sep,
	year = {2019},
	note = {arXiv:1809.02492 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Dvornik et al. - 2019 - On the Importance of Visual Context for Data Augme.pdf:/Users/eragon/Zotero/storage/F4LSAVHY/Dvornik et al. - 2019 - On the Importance of Visual Context for Data Augme.pdf:application/pdf},
}

@misc{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2022-10-21},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/WIMZKX6P/Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf},
}

@misc{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	doi = {10.48550/arXiv.1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2022-10-21},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv:1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/UKB798MK/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf},
}

@misc{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	doi = {10.48550/arXiv.2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2022-10-21},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv:2005.14165 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/LZZ4A45Z/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf},
}

@misc{dosovitskiy_image_2021,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	url = {http://arxiv.org/abs/2010.11929},
	doi = {10.48550/arXiv.2010.11929},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	urldate = {2022-10-21},
	publisher = {arXiv},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = jun,
	year = {2021},
	note = {arXiv:2010.11929 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/2VL6R42V/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Im.pdf:application/pdf},
}

@misc{liu_swin_2022,
	title = {Swin {Transformer} {V2}: {Scaling} {Up} {Capacity} and {Resolution}},
	shorttitle = {Swin {Transformer} {V2}},
	url = {http://arxiv.org/abs/2111.09883},
	abstract = {Large-scale NLP models have been shown to significantly improve the performance on language tasks with no signs of saturation. They also demonstrate amazing few-shot capabilities like that of human beings. This paper aims to explore large-scale models in computer vision. We tackle three major issues in training and application of large vision models, including training instability, resolution gaps between pre-training and fine-tuning, and hunger on labelled data. Three main techniques are proposed: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) A log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) A self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images. Through these techniques, this paper successfully trained a 3 billion-parameter Swin Transformer V2 model, which is the largest dense vision model to date, and makes it capable of training with images of up to 1,536\${\textbackslash}times\$1,536 resolution. It set new performance records on 4 representative vision tasks, including ImageNet-V2 image classification, COCO object detection, ADE20K semantic segmentation, and Kinetics-400 video action classification. Also note our training is much more efficient than that in Google's billion-level visual models, which consumes 40 times less labelled data and 40 times less training time. Code is available at {\textbackslash}url\{https://github.com/microsoft/Swin-Transformer\}.},
	urldate = {2022-10-21},
	publisher = {arXiv},
	author = {Liu, Ze and Hu, Han and Lin, Yutong and Yao, Zhuliang and Xie, Zhenda and Wei, Yixuan and Ning, Jia and Cao, Yue and Zhang, Zheng and Dong, Li and Wei, Furu and Guo, Baining},
	month = apr,
	year = {2022},
	note = {arXiv:2111.09883 [cs]
version: 2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/I53USPS2/Liu et al. - 2022 - Swin Transformer V2 Scaling Up Capacity and Resol.pdf:application/pdf},
}

@misc{kostrikov_image_2021,
	title = {Image {Augmentation} {Is} {All} {You} {Need}: {Regularizing} {Deep} {Reinforcement} {Learning} from {Pixels}},
	shorttitle = {Image {Augmentation} {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/2004.13649},
	abstract = {We propose a simple data augmentation technique that can be applied to standard model-free reinforcement learning algorithms, enabling robust learning directly from pixels without the need for auxiliary losses or pre-training. The approach leverages input perturbations commonly used in computer vision tasks to regularize the value function. Existing model-free approaches, such as Soft Actor-Critic (SAC), are not able to train deep networks effectively from image pixels. However, the addition of our augmentation method dramatically improves SAC's performance, enabling it to reach state-of-the-art performance on the DeepMind control suite, surpassing model-based (Dreamer, PlaNet, and SLAC) methods and recently proposed contrastive learning (CURL). Our approach can be combined with any model-free reinforcement learning algorithm, requiring only minor modifications. An implementation can be found at https://sites.google.com/view/data-regularized-q.},
	urldate = {2022-11-07},
	publisher = {arXiv},
	author = {Kostrikov, Ilya and Yarats, Denis and Fergus, Rob},
	month = mar,
	year = {2021},
	note = {arXiv:2004.13649 [cs, eess, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/FYYYKT7F/Kostrikov et al. - 2021 - Image Augmentation Is All You Need Regularizing D.pdf:application/pdf},
}

@misc{simonyan_deep_2014,
	title = {Deep {Inside} {Convolutional} {Networks}: {Visualising} {Image} {Classification} {Models} and {Saliency} {Maps}},
	shorttitle = {Deep {Inside} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1312.6034},
	abstract = {This paper addresses the visualisation of image classiﬁcation models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The ﬁrst one generates an image, which maximises the class score [5], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, speciﬁc to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classiﬁcation ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [13].},
	language = {en},
	urldate = {2022-11-18},
	publisher = {arXiv},
	author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
	month = apr,
	year = {2014},
	note = {arXiv:1312.6034 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Simonyan et al. - 2014 - Deep Inside Convolutional Networks Visualising Im.pdf:/Users/eragon/Zotero/storage/3TNFZE58/Simonyan et al. - 2014 - Deep Inside Convolutional Networks Visualising Im.pdf:application/pdf},
}

@misc{simonyan_deep_2014-1,
	title = {Deep {Inside} {Convolutional} {Networks}: {Visualising} {Image} {Classification} {Models} and {Saliency} {Maps}},
	shorttitle = {Deep {Inside} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1312.6034},
	abstract = {This paper addresses the visualisation of image classiﬁcation models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The ﬁrst one generates an image, which maximises the class score [5], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, speciﬁc to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classiﬁcation ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [13].},
	language = {en},
	urldate = {2022-11-18},
	publisher = {arXiv},
	author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
	month = apr,
	year = {2014},
	note = {arXiv:1312.6034 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Simonyan et al. - 2014 - Deep Inside Convolutional Networks Visualising Im.pdf:/Users/eragon/Zotero/storage/EXEY4ASY/Simonyan et al. - 2014 - Deep Inside Convolutional Networks Visualising Im.pdf:application/pdf},
}


@article{zhang_examining_2018,
	title = {Examining {CNN} {Representations} {With} {Respect} to {Dataset} {Bias}},
	volume = {32},
	copyright = {Copyright (c)},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/11833},
	doi = {10.1609/aaai.v32i1.11833},
	abstract = {Given a pre-trained CNN without any testing samples, this paper proposes a simple yet effective method to diagnose feature representations of the CNN. We aim to discover representation flaws caused by potential dataset bias. More specifically, when the CNN is trained to estimate image attributes, we mine latent relationships between representations of different attributes inside the CNN. Then, we compare the mined attribute relationships with ground-truth attribute relationships to discover the CNN's blind spots and failure modes due to dataset bias. In fact, representation flaws caused by dataset bias cannot be examined by conventional evaluation strategies based on testing images, because testing images may also have a similar bias. Experiments have demonstrated the effectiveness of our method.},
	language = {en},
	number = {1},
	urldate = {2022-09-06},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Zhang, Quanshi and Wang, Wenguan and Zhu, Song-Chun},
	month = apr,
	year = {2018},
	note = {Number: 1},
	keywords = {Knowledge representation},
	file = {Full Text PDF:/Users/eragon/Zotero/storage/6GCLCZLB/Zhang et al. - 2018 - Examining CNN Representations With Respect to Data.pdf:application/pdf},
}

@inproceedings{zhao_understanding_2021,
	title = {Understanding and {Evaluating} {Racial} {Biases} in {Image} {Captioning}},
	url = {https://openaccess.thecvf.com/content/ICCV2021/html/Zhao_Understanding_and_Evaluating_Racial_Biases_in_Image_Captioning_ICCV_2021_paper.html},
	language = {en},
	urldate = {2022-09-06},
	author = {Zhao, Dora and Wang, Angelina and Russakovsky, Olga},
	year = {2021},
	pages = {14830--14840},
	file = {Full Text PDF:/Users/eragon/Zotero/storage/8EQS5EK2/Zhao et al. - 2021 - Understanding and Evaluating Racial Biases in Imag.pdf:application/pdf},
}

@inproceedings{yang_towards_2020,
	address = {New York, NY, USA},
	series = {{FAT}* '20},
	title = {Towards fairer datasets: filtering and balancing the distribution of the people subtree in the {ImageNet} hierarchy},
	isbn = {978-1-4503-6936-7},
	shorttitle = {Towards fairer datasets},
	url = {https://doi.org/10.1145/3351095.3375709},
	doi = {10.1145/3351095.3375709},
	abstract = {Computer vision technology is being used by many but remains representative of only a few. People have reported misbehavior of computer vision models, including offensive prediction results and lower performance for underrepresented groups. Current computer vision models are typically developed using datasets consisting of manually annotated images or videos; the data and label distributions in these datasets are critical to the models' behavior. In this paper, we examine ImageNet, a large-scale ontology of images that has spurred the development of many modern computer vision methods. We consider three key factors within the person subtree of ImageNet that may lead to problematic behavior in downstream computer vision technology: (1) the stagnant concept vocabulary of WordNet, (2) the attempt at exhaustive illustration of all categories with images, and (3) the inequality of representation in the images within concepts. We seek to illuminate the root causes of these concerns and take the first steps to mitigate them constructively.},
	urldate = {2022-09-06},
	booktitle = {Proceedings of the 2020 {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Yang, Kaiyu and Qinami, Klint and Fei-Fei, Li and Deng, Jia and Russakovsky, Olga},
	month = jan,
	year = {2020},
	keywords = {computer vision, dataset construction, fairness, representative datasets},
	pages = {547--558},
	file = {Full Text:/Users/eragon/Zotero/storage/FVP8GKD7/Yang et al. - 2020 - Towards fairer datasets filtering and balancing t.pdf:application/pdf},
}

@techreport{venugopalan_its_2020,
	title = {It's easy to fool yourself: {Case} studies on identifying bias and confounding in bio-medical datasets},
	shorttitle = {It's easy to fool yourself},
	url = {http://arxiv.org/abs/1912.07661},
	abstract = {Confounding variables are a well known source of nuisance in biomedical studies. They present an even greater challenge when we combine them with black-box machine learning techniques that operate on raw data. This work presents two case studies. In one, we discovered biases arising from systematic errors in the data generation process. In the other, we found a spurious source of signal unrelated to the prediction task at hand. In both cases, our prediction models performed well but under careful examination hidden confounders and biases were revealed. These are cautionary tales on the limits of using machine learning techniques on raw data from scientific experiments.},
	number = {arXiv:1912.07661},
	urldate = {2022-09-06},
	institution = {arXiv},
	author = {Venugopalan, Subhashini and Narayanaswamy, Arunachalam and Yang, Samuel and Geraschenko, Anton and Lipnick, Scott and Makhortova, Nina and Hawrot, James and Marques, Christine and Pereira, Joao and Brenner, Michael and Rubin, Lee and Wainger, Brian and Berndl, Marc},
	month = apr,
	year = {2020},
	doi = {10.48550/arXiv.1912.07661},
	note = {arXiv:1912.07661 [cs, eess, q-bio, stat]
type: article},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Quantitative Biology - Quantitative Methods},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/JC4H8PF9/Venugopalan et al. - 2020 - It's easy to fool yourself Case studies on identi.pdf:application/pdf},
}

@article{mehrabi_survey_2021,
	title = {A {Survey} on {Bias} and {Fairness} in {Machine} {Learning}},
	volume = {54},
	issn = {0360-0300},
	url = {https://doi.org/10.1145/3457607},
	doi = {10.1145/3457607},
	abstract = {With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.},
	number = {6},
	urldate = {2022-09-06},
	journal = {ACM Comput. Surv.},
	author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
	month = jul,
	year = {2021},
	keywords = {deep learning, Fairness and bias in artificial intelligence, machine learning, natural language processing, representation learning},
	pages = {115:1--115:35},
	file = {Submitted Version:/Users/eragon/Zotero/storage/3X7JEMXP/Mehrabi et al. - 2021 - A Survey on Bias and Fairness in Machine Learning.pdf:application/pdf},
}

@inproceedings{steed_image_2021,
	address = {New York, NY, USA},
	series = {{FAccT} '21},
	title = {Image {Representations} {Learned} {With} {Unsupervised} {Pre}-{Training} {Contain} {Human}-like {Biases}},
	isbn = {978-1-4503-8309-7},
	url = {https://doi.org/10.1145/3442188.3445932},
	doi = {10.1145/3442188.3445932},
	abstract = {Recent advances in machine learning leverage massive datasets of unlabeled images from the web to learn general-purpose image representations for tasks from image classification to face recognition. But do unsupervised computer vision models automatically learn implicit patterns and embed social biases that could have harmful downstream effects? We develop a novel method for quantifying biased associations between representations of social concepts and attributes in images. We find that state-of-the-art unsupervised models trained on ImageNet, a popular benchmark image dataset curated from internet images, automatically learn racial, gender, and intersectional biases. We replicate 8 previously documented human biases from social psychology, from the innocuous, as with insects and flowers, to the potentially harmful, as with race and gender. Our results closely match three hypotheses about intersectional bias from social psychology. For the first time in unsupervised computer vision, we also quantify implicit human biases about weight, disabilities, and several ethnicities. When compared with statistical patterns in online image datasets, our findings suggest that machine learning models can automatically learn bias from the way people are stereotypically portrayed on the web.},
	urldate = {2022-09-06},
	booktitle = {Proceedings of the 2021 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Steed, Ryan and Caliskan, Aylin},
	month = mar,
	year = {2021},
	keywords = {computer vision, implicit bias, unsupervised learning},
	pages = {701--713},
	file = {Full Text PDF:/Users/eragon/Zotero/storage/8UGH43BN/Steed and Caliskan - 2021 - Image Representations Learned With Unsupervised Pr.pdf:application/pdf},
}

@techreport{geirhos_imagenet-trained_2019,
	title = {{ImageNet}-trained {CNNs} are biased towards texture; increasing shape bias improves accuracy and robustness},
	url = {http://arxiv.org/abs/1811.12231},
	abstract = {Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes. Some recent studies suggest a more important role of image textures. We here put these conflicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conflict. We show that ImageNet-trained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classification strategies. We then demonstrate that the same standard architecture (ResNet-50) that learns a texture-based representation on ImageNet is able to learn a shape-based representation instead when trained on "Stylized-ImageNet", a stylized version of ImageNet. This provides a much better fit for human behavioural performance in our well-controlled psychophysical lab setting (nine experiments totalling 48,560 psychophysical trials across 97 observers) and comes with a number of unexpected emergent benefits such as improved object detection performance and previously unseen robustness towards a wide range of image distortions, highlighting advantages of a shape-based representation.},
	number = {arXiv:1811.12231},
	urldate = {2022-09-06},
	institution = {arXiv},
	author = {Geirhos, Robert and Rubisch, Patricia and Michaelis, Claudio and Bethge, Matthias and Wichmann, Felix A. and Brendel, Wieland},
	month = jan,
	year = {2019},
	doi = {10.48550/arXiv.1811.12231},
	note = {arXiv:1811.12231 [cs, q-bio, stat]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, Quantitative Biology - Neurons and Cognition},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/7C8DHLXI/Geirhos et al. - 2019 - ImageNet-trained CNNs are biased towards texture\; .pdf:application/pdf},
}

@techreport{dulhanty_auditing_2019,
	title = {Auditing {ImageNet}: {Towards} a {Model}-driven {Framework} for {Annotating} {Demographic} {Attributes} of {Large}-{Scale} {Image} {Datasets}},
	shorttitle = {Auditing {ImageNet}},
	url = {http://arxiv.org/abs/1905.01347},
	abstract = {The ImageNet dataset ushered in a flood of academic and industry interest in deep learning for computer vision applications. Despite its significant impact, there has not been a comprehensive investigation into the demographic attributes of images contained within the dataset. Such a study could lead to new insights on inherent biases within ImageNet, particularly important given it is frequently used to pretrain models for a wide variety of computer vision tasks. In this work, we introduce a model-driven framework for the automatic annotation of apparent age and gender attributes in large-scale image datasets. Using this framework, we conduct the first demographic audit of the 2012 ImageNet Large Scale Visual Recognition Challenge (ILSVRC) subset of ImageNet and the "person" hierarchical category of ImageNet. We find that 41.62\% of faces in ILSVRC appear as female, 1.71\% appear as individuals above the age of 60, and males aged 15 to 29 account for the largest subgroup with 27.11\%. We note that the presented model-driven framework is not fair for all intersectional groups, so annotation are subject to bias. We present this work as the starting point for future development of unbiased annotation models and for the study of downstream effects of imbalances in the demographics of ImageNet. Code and annotations are available at: http://bit.ly/ImageNetDemoAudit},
	number = {arXiv:1905.01347},
	urldate = {2022-09-06},
	institution = {arXiv},
	author = {Dulhanty, Chris and Wong, Alexander},
	month = jun,
	year = {2019},
	doi = {10.48550/arXiv.1905.01347},
	note = {arXiv:1905.01347 [cs]
type: article},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/8GNWL9GM/Dulhanty and Wong - 2019 - Auditing ImageNet Towards a Model-driven Framewor.pdf:application/pdf},
}

@incollection{tommasi_deeper_2017,
	address = {Cham},
	series = {Advances in {Computer} {Vision} and {Pattern} {Recognition}},
	title = {A {Deeper} {Look} at {Dataset} {Bias}},
	isbn = {978-3-319-58347-1},
	url = {https://doi.org/10.1007/978-3-319-58347-1_2},
	abstract = {The presence of a bias in each image data collection has recently attracted a lot of attention in the computer vision community showing the limits in generalization of any learning method trained on a specific dataset. At the same time, with the rapid development of deep learning architectures, the activation values of Convolutional Neural Networks (CNN) are emerging as reliable and robust image descriptors. In this chapter we propose to verify the potential of the CNN features when facing the dataset biasDataset biasproblem. With this purpose we introduce a large testbed for cross-dataset analysis and we discuss the challenges faced to create two comprehensive experimental setups by aligning twelve existing image databases. We conduct a series of analyses looking at how the datasets differ among each other and verifying the performance of existing debiasing methods under different representations. We learn important lessons on which part of the dataset bias problem can be considered solved and which open questions still need to be tackled.},
	language = {en},
	urldate = {2022-09-06},
	booktitle = {Domain {Adaptation} in {Computer} {Vision} {Applications}},
	publisher = {Springer International Publishing},
	author = {Tommasi, Tatiana and Patricia, Novi and Caputo, Barbara and Tuytelaars, Tinne},
	editor = {Csurka, Gabriela},
	year = {2017},
	doi = {10.1007/978-3-319-58347-1_2},
	keywords = {Capture Bias, Cross-dataset Generalization, Dataset Bias, Robust Image Descriptors, Train Images},
	pages = {37--55},
}

@techreport{li_how_2021,
	title = {How {Does} a {Neural} {Network}'s {Architecture} {Impact} {Its} {Robustness} to {Noisy} {Labels}?},
	url = {http://arxiv.org/abs/2012.12896},
	abstract = {Noisy labels are inevitable in large real-world datasets. In this work, we explore an area understudied by previous works -- how the network's architecture impacts its robustness to noisy labels. We provide a formal framework connecting the robustness of a network to the alignments between its architecture and target/noise functions. Our framework measures a network's robustness via the predictive power in its representations -- the test performance of a linear model trained on the learned representations using a small set of clean labels. We hypothesize that a network is more robust to noisy labels if its architecture is more aligned with the target function than the noise. To support our hypothesis, we provide both theoretical and empirical evidence across various neural network architectures and different domains. We also find that when the network is well-aligned with the target function, its predictive power in representations could improve upon state-of-the-art (SOTA) noisy-label-training methods in terms of test accuracy and even outperform sophisticated methods that use clean labels.},
	number = {arXiv:2012.12896},
	urldate = {2022-09-06},
	institution = {arXiv},
	author = {Li, Jingling and Zhang, Mozhi and Xu, Keyulu and Dickerson, John P. and Ba, Jimmy},
	month = nov,
	year = {2021},
	doi = {10.48550/arXiv.2012.12896},
	note = {arXiv:2012.12896 [cs, stat]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/P4PY7AZY/Li et al. - 2021 - How Does a Neural Network's Architecture Impact It.pdf:application/pdf},
}

@inproceedings{ali_evaluation_2021,
	title = {Evaluation of {Latent} {Space} {Learning} {With} {Procedurally}-{Generated} {Datasets} of {Shapes}},
	url = {https://openaccess.thecvf.com/content/ICCV2021W/DLGC/html/Ali_Evaluation_of_Latent_Space_Learning_With_Procedurally-Generated_Datasets_of_Shapes_ICCVW_2021_paper.html},
	language = {en},
	urldate = {2022-09-06},
	author = {Ali, Sharjeel and van Kaick, Oliver},
	year = {2021},
	pages = {2086--2094},
	file = {Full Text PDF:/Users/eragon/Zotero/storage/SKFWLXEE/Ali and van Kaick - 2021 - Evaluation of Latent Space Learning With Procedura.pdf:application/pdf},
}

@techreport{choi_analyzing_2022,
	title = {Analyzing the {Latent} {Space} of {GAN} through {Local} {Dimension} {Estimation}},
	url = {http://arxiv.org/abs/2205.13182},
	abstract = {The impressive success of style-based GANs (StyleGANs) in high-fidelity image synthesis has motivated research to understand the semantic properties of their latent spaces. Recently, a close relationship was observed between the semantically disentangled local perturbations and the local PCA components in the learned latent space \${\textbackslash}mathcal\{W\}\$. However, understanding the number of disentangled perturbations remains challenging. Building upon this observation, we propose a local dimension estimation algorithm for an arbitrary intermediate layer in a pre-trained GAN model. The estimated intrinsic dimension corresponds to the number of disentangled local perturbations. In this perspective, we analyze the intermediate layers of the mapping network in StyleGANs. Our analysis clarifies the success of \${\textbackslash}mathcal\{W\}\$-space in StyleGAN and suggests an alternative. Moreover, the intrinsic dimension estimation opens the possibility of unsupervised evaluation of global-basis-compatibility and disentanglement for a latent space. Our proposed metric, called Distortion, measures an inconsistency of intrinsic tangent space on the learned latent space. The metric is purely geometric and does not require any additional attribute information. Nevertheless, the metric shows a high correlation with the global-basis-compatibility and supervised disentanglement score. Our findings pave the way towards an unsupervised selection of globally disentangled latent space among the intermediate latent spaces in a GAN.},
	number = {arXiv:2205.13182},
	urldate = {2022-09-06},
	institution = {arXiv},
	author = {Choi, Jaewoong and Hwang, Geonho and Cho, Hyunsoo and Kang, Myungjoo},
	month = may,
	year = {2022},
	doi = {10.48550/arXiv.2205.13182},
	note = {arXiv:2205.13182 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/XGUDBJX5/Choi et al. - 2022 - Analyzing the Latent Space of GAN through Local Di.pdf:application/pdf},
}

@inproceedings{birhane_large_2021,
	title = {Large image datasets: {A} pyrrhic win for computer vision?},
	shorttitle = {Large image datasets},
	doi = {10.1109/WACV48630.2021.00158},
	abstract = {In this paper we investigate problematic practices and consequences of large scale vision datasets (LSVDs). We examine broad issues such as the question of consent and justice as well as specific concerns such as the inclusion of verifiably pornographic images in datasets. Taking the ImageNet-ILSVRC-2012 dataset as an example, we perform a cross-sectional model-based quantitative census covering factors such as age, gender, NSFW content scoring, class- wise accuracy, human-cardinality-analysis, and the semanticity of the image class information in order to statistically investigate the extent and subtleties of ethical transgressions. We then use the census to help hand-curate a look-up-table of images in the ImageNet-ILSVRC-2012 dataset that fall into the categories of verifiably pornographic: shot in a non-consensual setting (up-skirt), beach voyeuristic, and exposed private parts. We survey the landscape of harm and threats both the society at large and individuals face due to uncritical and ill-considered dataset curation practices. We then propose possible courses of correction and critique their pros and cons. We have duly open-sourced all of the code and the census meta-datasets generated in this endeavor for the computer vision community to build on. By unveiling the severity of the threats, our hope is to motivate the constitution of mandatory Institutional Review Boards (IRB) for large scale dataset curation.},
	booktitle = {2021 {IEEE} {Winter} {Conference} on {Applications} of {Computer} {Vision} ({WACV})},
	author = {Birhane, Abeba and Prabhu, Vinay Uday},
	month = jan,
	year = {2021},
	note = {ISSN: 2642-9381},
	keywords = {Computer vision, Conferences, Faces, IEEE Constitution},
	pages = {1536--1546},
	file = {Submitted Version:/Users/eragon/Zotero/storage/NXB5CRPQ/Birhane and Prabhu - 2021 - Large image datasets A pyrrhic win for computer v.pdf:application/pdf},
}

@techreport{lang_explaining_2021,
	title = {Explaining in {Style}: {Training} a {GAN} to explain a classifier in {StyleSpace}},
	shorttitle = {Explaining in {Style}},
	url = {http://arxiv.org/abs/2104.13369},
	abstract = {Image classification models can depend on multiple different semantic attributes of the image. An explanation of the decision of the classifier needs to both discover and visualize these properties. Here we present StylEx, a method for doing this, by training a generative model to specifically explain multiple attributes that underlie classifier decisions. A natural source for such attributes is the StyleSpace of StyleGAN, which is known to generate semantically meaningful dimensions in the image. However, because standard GAN training is not dependent on the classifier, it may not represent these attributes which are important for the classifier decision, and the dimensions of StyleSpace may represent irrelevant attributes. To overcome this, we propose a training procedure for a StyleGAN, which incorporates the classifier model, in order to learn a classifier-specific StyleSpace. Explanatory attributes are then selected from this space. These can be used to visualize the effect of changing multiple attributes per image, thus providing image-specific explanations. We apply StylEx to multiple domains, including animals, leaves, faces and retinal images. For these, we show how an image can be modified in different ways to change its classifier output. Our results show that the method finds attributes that align well with semantic ones, generate meaningful image-specific explanations, and are human-interpretable as measured in user-studies.},
	number = {arXiv:2104.13369},
	urldate = {2022-09-06},
	institution = {arXiv},
	author = {Lang, Oran and Gandelsman, Yossi and Yarom, Michal and Wald, Yoav and Elidan, Gal and Hassidim, Avinatan and Freeman, William T. and Isola, Phillip and Globerson, Amir and Irani, Michal and Mosseri, Inbar},
	month = sep,
	year = {2021},
	doi = {10.48550/arXiv.2104.13369},
	note = {arXiv:2104.13369 [cs, eess, stat]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing, Electrical Engineering and Systems Science - Image and Video Processing},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/9ULRZW8X/Lang et al. - 2021 - Explaining in Style Training a GAN to explain a c.pdf:application/pdf},
}

@inproceedings{gong_reshaping_2013,
	title = {Reshaping {Visual} {Datasets} for {Domain} {Adaptation}},
	volume = {26},
	url = {https://proceedings.neurips.cc/paper/2013/hash/2291d2ec3b3048d1a6f86c2c4591b7e0-Abstract.html},
	abstract = {In visual recognition problems, the common data distribution mismatches between training and testing make domain adaptation essential. However, image data is difficult to manually divide into the discrete domains required by adaptation algorithms, and the standard practice of equating datasets with domains is a weak proxy for all the real conditions that alter the statistics in complex ways (lighting, pose, background, resolution, etc.) We propose an approach to automatically discover latent domains in image or video datasets. Our formulation imposes two key properties on domains: maximum distinctiveness and maximum learnability. By maximum distinctiveness, we require the underlying distributions of the identified domains to be different from each other; by maximum learnability, we ensure that a strong discriminative model can be learned from the domain. We devise a nonparametric representation and efficient optimization procedure for distinctiveness, which, when coupled with our learnability constraint, can successfully discover domains among both training and test data. We extensively evaluate our approach on object recognition and human activity recognition tasks.},
	urldate = {2022-09-06},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Gong, Boqing and Grauman, Kristen and Sha, Fei},
	year = {2013},
	file = {Full Text PDF:/Users/eragon/Zotero/storage/2STZZQCC/Gong et al. - 2013 - Reshaping Visual Datasets for Domain Adaptation.pdf:application/pdf},
}

@techreport{kang_interpreting_2019,
	title = {Interpreting {Undesirable} {Pixels} for {Image} {Classification} on {Black}-{Box} {Models}},
	url = {http://arxiv.org/abs/1909.12446},
	abstract = {In an effort to interpret black-box models, researches for developing explanation methods have proceeded in recent years. Most studies have tried to identify input pixels that are crucial to the prediction of a classifier. While this approach is meaningful to analyse the characteristic of blackbox models, it is also important to investigate pixels that interfere with the prediction. To tackle this issue, in this paper, we propose an explanation method that visualizes undesirable regions to classify an image as a target class. To be specific, we divide the concept of undesirable regions into two terms: (1) factors for a target class, which hinder that black-box models identify intrinsic characteristics of a target class and (2) factors for non-target classes that are important regions for an image to be classified as other classes. We visualize such undesirable regions on heatmaps to qualitatively validate the proposed method. Furthermore, we present an evaluation metric to provide quantitative results on ImageNet.},
	number = {arXiv:1909.12446},
	urldate = {2022-09-26},
	institution = {arXiv},
	author = {Kang, Sin-Han and Jung, Hong-Gyu and Lee, Seong-Whan},
	month = dec,
	year = {2019},
	note = {arXiv:1909.12446 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/4MXQT9TW/Kang et al. - 2019 - Interpreting Undesirable Pixels for Image Classifi.pdf:application/pdf},
}

@article{wickramanayake_explanation-based_nodate,
	title = {Explanation-based {Data} {Augmentation} for {Image} {Classification}},
	abstract = {Existing works have generated explanations for deep neural network decisions to provide insights into model behavior. We observe that these explanations can also be used to identify concepts that caused misclassiﬁcations. This allows us to understand the possible limitations of the dataset used to train the model, particularly the under-represented regions in the dataset. This work proposes a framework that utilizes concept-based explanations to automatically augment the dataset with new images that can cover these under-represented regions to improve the model performance. The framework is able to use the explanations generated by both interpretable classiﬁers and post-hoc explanations from black-box classiﬁers. Experiment results demonstrate that the proposed approach improves the accuracy of classiﬁers compared to state-of-the-art augmentation strategies.},
	language = {en},
	author = {Wickramanayake, Sandareka and Lee, Mong Li and Hsu, Wynne},
	pages = {12},
	file = {Wickramanayake et al. - Explanation-based Data Augmentation for Image Clas.pdf:/Users/eragon/Zotero/storage/9H2FTFQ7/Wickramanayake et al. - Explanation-based Data Augmentation for Image Clas.pdf:application/pdf},
}

@techreport{radhakrishnan_patchnet_2018,
	title = {Patchnet: {Interpretable} {Neural} {Networks} for {Image} {Classification}},
	shorttitle = {Patchnet},
	url = {http://arxiv.org/abs/1705.08078},
	abstract = {Understanding how a complex machine learning model makes a classification decision is essential for its acceptance in sensitive areas such as health care. Towards this end, we present PatchNet, a method that provides the features indicative of each class in an image using a tradeoff between restricting global image context and classification error. We mathematically analyze this tradeoff, demonstrate Patchnet's ability to construct sharp visual heatmap representations of the learned features, and quantitatively compare these features with features selected by domain experts by applying PatchNet to the classification of benign/malignant skin lesions from the ISBI-ISIC 2017 melanoma classification challenge.},
	number = {arXiv:1705.08078},
	urldate = {2022-09-26},
	institution = {arXiv},
	author = {Radhakrishnan, Adityanarayanan and Durham, Charles and Soylemezoglu, Ali and Uhler, Caroline},
	month = nov,
	year = {2018},
	note = {arXiv:1705.08078 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/FM4FT9BL/Radhakrishnan et al. - 2018 - Patchnet Interpretable Neural Networks for Image .pdf:application/pdf},
}

@article{ye_dufenet_2022,
	title = {{DuFeNet}: {Improve} the {Accuracy} and {Increase} {Shape} {Bias} of {Neural} {Network} {Models}},
	volume = {16},
	issn = {1863-1703, 1863-1711},
	shorttitle = {{DuFeNet}},
	url = {https://link.springer.com/10.1007/s11760-021-02065-3},
	doi = {10.1007/s11760-021-02065-3},
	abstract = {In image classiﬁcation ﬁeld, existing work tends to modify the network structure to obtain higher accuracy or faster speed. However, some studies have found that the neural network usually has texture bias effect, which means that the neural network is more sensitive to the texture information than the shape information. Based on such phenomenon, we propose a new way to improve network performance by making full use of gradient information. The dual features network (DuFeNet) is proposed in this paper. In DuFeNet, one sub-network is used to learn the information of gradient features, and the other is a traditional neural network with texture bias. The structure of DuFeNet is easy to implement in the original neural network structure. The experimental results clearly show that DuFeNet can achieve better accuracy in image classiﬁcation and detection. It can increase the shape bias of the network adapted to human visual perception. Besides, DuFeNet can be used without modifying the structure of the original network at lower additional parameters cost.},
	language = {en},
	number = {5},
	urldate = {2022-09-26},
	journal = {SIViP},
	author = {Ye, Zecong and Gao, Zhiqiang and Cui, Xiaolong and Wang, Yaojie and Shan, Nanliang},
	month = jul,
	year = {2022},
	pages = {1153--1160},
	file = {Ye et al. - 2022 - DuFeNet Improve the Accuracy and Increase Shape B.pdf:/Users/eragon/Zotero/storage/D2Y5RLEB/Ye et al. - 2022 - DuFeNet Improve the Accuracy and Increase Shape B.pdf:application/pdf},
}

@article{cheng_improve_2022,
	title = {Improve the {Deep} {Learning} {Models} in {Forestry} {Based} on {Explanations} and {Expertise}},
	volume = {13},
	issn = {1664-462X},
	url = {https://www.frontiersin.org/articles/10.3389/fpls.2022.902105/full},
	doi = {10.3389/fpls.2022.902105},
	abstract = {In forestry studies, deep learning models have achieved excellent performance in many application scenarios (e.g., detecting forest damage). However, the unclear model decisions (i.e., black-box) undermine the credibility of the results and hinder their practicality. This study intends to obtain explanations of such models through the use of explainable artificial intelligence methods, and then use feature unlearning methods to improve their performance, which is the first such attempt in the field of forestry. Results of three experiments show that the model training can be guided by expertise to gain specific knowledge, which is reflected by explanations. For all three experiments based on synthetic and real leaf images, the improvement of models is quantified in the classification accuracy (up to 4.6\%) and three indicators of explanation assessment (i.e., root-mean-square error, cosine similarity, and the proportion of important pixels). Besides, the introduced expertise in annotation matrix form was automatically created in all experiments. This study emphasizes that studies of deep learning in forestry should not only pursue model performance (e.g., higher classification accuracy) but also focus on the explanations and try to improve models according to the expertise.},
	language = {en},
	urldate = {2022-09-26},
	journal = {Front. Plant Sci.},
	author = {Cheng, Ximeng and Doosthosseini, Ali and Kunkel, Julian},
	month = may,
	year = {2022},
	pages = {902105},
	file = {Cheng et al. - 2022 - Improve the Deep Learning Models in Forestry Based.pdf:/Users/eragon/Zotero/storage/WDJK8WMR/Cheng et al. - 2022 - Improve the Deep Learning Models in Forestry Based.pdf:application/pdf},
}

@techreport{rombach_high-resolution_2022,
	title = {High-{Resolution} {Image} {Synthesis} with {Latent} {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2112.10752},
	abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .},
	number = {arXiv:2112.10752},
	urldate = {2022-09-26},
	institution = {arXiv},
	author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
	month = apr,
	year = {2022},
	note = {arXiv:2112.10752 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/eragon/Zotero/storage/TU5QQTQU/Rombach et al. - 2022 - High-Resolution Image Synthesis with Latent Diffus.pdf:application/pdf},
}

