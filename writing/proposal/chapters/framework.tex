
\section{Theoretical Framework} \label{section:thero}
The literature that this thesis builds upon is explained below. 
\subsection*{Explainibility}
% Gradcam etc
It is commonly noted that neural networks are black boxes. Although in the past decade, these NNs have performed vastly better than a lot of computer vision algorithms, their results are still hard to explain. Due to the high dimensionality of the feature space representation that they create, it becomes impossible to directly visualize the reasons for the decisions made by these networks. The field of Explainable AI (XAI) rose to deal with these challenges. One of the most notable algorithms in this domain for image classification is that of CAM \cite{oquab_is_2015} (eventually succeeded by Grad-CAM \cite{selvaraju_grad-cam_nodate} and then by Grad-CAM++ \cite{chattopadhyay_grad-cam_2018}). The main idea is to visualize the gradients of the final convolutional layer in a trained network to create a sort of "activation map". This map could essentially be used to visualize which parts of the image the network used to make its decision.\\
In CAM \cite{oquab_is_2015}, the final layer would have to be replaced by a Global Average Pooling \cite{lin_network_2014} followed by a Softmax. This would then require retraining the network. Since this was not very feasible, further research came up with Grad-CAM \cite{selvaraju_grad-cam_nodate}. In this paper, the authors proposed using backpropagation over the image and using class information to create the attention map. This makes it able to be directly applied to any network meant for vision classification. The drawback of this method though, was that it sometimes failed when there was more than one instance of the object in the image. It also sometimes failed to fully cover the object in the attention map. Grad-CAM++ \cite{chattopadhyay_grad-cam_2018} solves this issue by modifying the backpropagation algorithm which now scales the map by considering the size of the response.\\
In this thesis, we want to examine using images weighted by the outputs of Grad-CAM and Grad-CAM++ as augmentation methods during training. 
\subsection*{Augmentation}
% Random Erasing, Saliency Mix
It is not always possible to have a huge amount of data when it comes to training a network. To maximize the performance that can be achieved with existing data, performing transformations to the images either before or during training has become a norm. This is called Augmentation and in a lot of cases, it helps in improving the performance \cite{perez2017effectiveness}. There are numerous variants of this, ranging from flipping the image on either axis, random cropping, randomly erasing parts of the image, isolating color channels, etc. In this paper, we build on two such techniques.\\
Random Erasing \cite{zhong2020random} is an algorithm in a random region from which the image is deleted. The region size is randomly defined at every instance of running the algorithm. Combined with other such Augmentation, this improves performance by a significant amount. The second algorithm is called Visual Context Augmentation \cite{dvornik2018modeling} in which a network for object detection learns about context by being given data where the object to be detected is blacked out and other images are around it. Many such images with different contexts for the same object are generated and then the network is trained. This teaches the network to predict the object in time.\\
In this thesis, we combine Augmentation and Explainability to create the proposed "Proxy Attention".
\subsection*{Attention}
One of the challenges with any neural network is deciding what parts of an image or a sentence are important for the final decision.
In Natural Language Processing, Sequence2Sequence models (eg: RNN, LSTM, etc) had been used for a long time, but they had the critical flaw of not being able to work over very long sentences. In time, Vaswani et al. \cite{vaswani_attention_2017}, in their seminal paper "Attention Is All You Need", proposed a type of attention called Scaled Dot Product Attention as part of a new architecture they titled the "Transformer". This modification allowed the network to not only learn how to classify but also learn which parts are important. Transformer based models such as BERT \cite{devlin_bert_2019} and GPT-3 \cite{brown_language_2020} then suddenly overtook RNNs, LSTMs etc.
\subsection*{ViT}
% Good/Bad/Moderates
The Transformer used to be generally only used in NLP, but recently \cite{dosovitskiy2020image} converted the Transformer pipeline to work with Images, and the Vision Transformer (ViT) was created. This of course led to a boom in using Transformers for Vision but these networks require even more massive amounts of data compared to CNNs. Transfer learning alleviates this issue to an extent, but not fully. This is a huge problem.

To examine the effects of Proxy Attention, we compare results to the ViT as well. The aim is to combine them both if this combination ends up being significant.